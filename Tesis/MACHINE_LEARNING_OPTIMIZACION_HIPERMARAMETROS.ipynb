{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports we know we'll need\n",
    "import skopt\n",
    "# !pip install scikit-optimize if  necessary\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow\n",
    "from tensorflow.python.keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
    "\n",
    "dim_num_dense_nodes1 = Integer(low=1, high=300, name='num_dense_nodes1')\n",
    "dim_num_dense_nodes2 = Integer(low=1, high=300, name='num_dense_nodes2')\n",
    "dim_num_dense_nodes3 = Integer(low=1, high=300, name='num_dense_nodes3')\n",
    "\n",
    "dim_activation = Categorical(categories=['relu', 'tanh'],\n",
    "                             name='activation')\n",
    "dim_batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "dim_adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "\n",
    "              dim_num_dense_nodes1,\n",
    "              dim_num_dense_nodes2,\n",
    "              dim_num_dense_nodes3,\n",
    "              \n",
    "              dim_activation,\n",
    "              dim_batch_size,\n",
    "              dim_adam_decay\n",
    "             ]\n",
    "default_parameters = [1e-3, 1, 13,13,13 ,'relu',64, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes1, num_dense_nodes2,num_dense_nodes3, activation, adam_decay):\n",
    "    #start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(99, input_shape= input_shape, activation=activation\n",
    "                   ))\n",
    "    #create a loop making a new dense layer for the amount passed to this model.\n",
    "    #naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_dense_layers):\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "        if i==0:\n",
    "            model.add(Dense(num_dense_nodes1,\n",
    "                     activation=activation,\n",
    "                            name=name\n",
    "                     ))\n",
    "        if i==1:\n",
    "            model.add(Dense(num_dense_nodes2,\n",
    "                     activation=activation,\n",
    "                            name=name\n",
    "                     ))\n",
    "        if i==2:\n",
    "            model.add(Dense(num_dense_nodes3,\n",
    "                     activation=activation,\n",
    "                            name=name\n",
    "                     ))\n",
    "    #add our classification layer.\n",
    "    model.add(Dense(33,activation='sigmoid'))\n",
    "    \n",
    "    #setup our optimizer and compile\n",
    "    adam = Adam(lr=learning_rate, decay= adam_decay)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers, \n",
    "            num_dense_nodes1,num_dense_nodes2,num_dense_nodes3,activation, batch_size,adam_decay):\n",
    "\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         \n",
    "                         num_dense_nodes1=num_dense_nodes1,\n",
    "                         num_dense_nodes2=num_dense_nodes2,\n",
    "                         num_dense_nodes3=num_dense_nodes3,\n",
    "                         \n",
    "                         activation=activation,\n",
    "                         adam_decay=adam_decay\n",
    "                        )\n",
    "    \n",
    "\n",
    "    #named blackbox becuase it represents the structure\n",
    "    blackbox = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=90,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.15,\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = blackbox.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    del blackbox\n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    tensorflow.reset_default_graph()\n",
    "    gc.collect()\n",
    "    reset_keras()\n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "LUNES=pd.read_csv(\"AC_PRIN_LUNES.csv\",index_col = 0)\n",
    "MARTES=pd.read_csv(\"AC_PRIN_MARTES.csv\",index_col = 0)\n",
    "MIERCOLES=pd.read_csv(\"AC_PRIN_MIERCOLES.csv\",index_col = 0)\n",
    "JUEVES=pd.read_csv(\"AC_PRIN_JUEVES.csv\",index_col = 0)\n",
    "VIERNES=pd.read_csv(\"AC_PRIN_VIERNES.csv\",index_col = 0)\n",
    "SABADO=pd.read_csv(\"AC_PRIN_SABADO.csv\",index_col = 0)\n",
    "DOMINGO=pd.read_csv(\"AC_PRIN_DOMINGO.csv\",index_col = 0)\n",
    "###UNIVERSIDAD POLITECNICA DE VALENCIA\n",
    "LUNESEV=pd.read_csv(\"5EV5_LUNES.csv\",index_col = 0)\n",
    "MARTESEV=pd.read_csv(\"5EV5_MARTES.csv\",index_col = 0)\n",
    "MIERCOLESEV=pd.read_csv(\"5EV5_MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESEV=pd.read_csv(\"5EV5_JUEVES.csv\",index_col = 0)\n",
    "VIERNESEV=pd.read_csv(\"5EV5_VIERNES.csv\",index_col = 0)\n",
    "SABADOEV=pd.read_csv(\"5EV5_SABADO.csv\",index_col = 0)\n",
    "DOMINGOEV=pd.read_csv(\"5EV5_DOMINGO.csv\",index_col = 0)\n",
    "#### DATOS KANGLE\n",
    "LUNESKG=pd.read_csv(\"KANGLE________LUNES.csv\",index_col = 0)\n",
    "MARTESKG=pd.read_csv(\"KANGLE________MARTES.csv\",index_col = 0)\n",
    "MIERCOLESKG=pd.read_csv(\"KANGLE________MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESKG=pd.read_csv(\"KANGLE________JUEVES.csv\",index_col = 0)\n",
    "VIERNESKG=pd.read_csv(\"KANGLE________VIERNES.csv\",index_col = 0)\n",
    "SABADOKG=pd.read_csv(\"KANGLE________SABADO.csv\",index_col = 0)\n",
    "DOMINGOKG=pd.read_csv(\"KANGLE________DOMINGO.csv\",index_col = 0)\n",
    "##DATOS FRANCIA\n",
    "LUNESFR=pd.read_csv(\"FRANCIA_______LUNES.csv\",index_col = 0)\n",
    "MARTESFR=pd.read_csv(\"FRANCIA_______MARTES.csv\",index_col = 0)\n",
    "MIERCOLESFR=pd.read_csv(\"FRANCIA_______MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESFR=pd.read_csv(\"FRANCIA_______JUEVES.csv\",index_col = 0)\n",
    "VIERNESFR=pd.read_csv(\"FRANCIA_______VIERNES.csv\",index_col = 0)\n",
    "SABADOFR=pd.read_csv(\"FRANCIA_______SABADO.csv\",index_col = 0)\n",
    "DOMINGOFR=pd.read_csv(\"FRANCIA_______DOMINGO.csv\",index_col = 0)\n",
    "##DATOS RESIDENCIA 1\n",
    "LUNESR1=pd.read_csv(\"RESIDENCIA1______LUNES.csv\",index_col = 0)\n",
    "MARTESR1=pd.read_csv(\"RESIDENCIA1______MARTES.csv\",index_col = 0)\n",
    "MIERCOLESR1=pd.read_csv(\"RESIDENCIA1______MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESR1=pd.read_csv(\"RESIDENCIA1______JUEVES.csv\",index_col = 0)\n",
    "VIERNESR1=pd.read_csv(\"RESIDENCIA1______VIERNES.csv\",index_col = 0)\n",
    "SABADOR1=pd.read_csv(\"RESIDENCIA1______SABADO.csv\",index_col = 0)\n",
    "DOMINGOR1=pd.read_csv(\"RESIDENCIA1______DOMINGO.csv\",index_col = 0)\n",
    "##DATOS RESIDENCIA 10\n",
    "LUNESR10=pd.read_csv(\"RESIDENCIA10______LUNES.csv\",index_col = 0)\n",
    "MARTESR10=pd.read_csv(\"RESIDENCIA10______MARTES.csv\",index_col = 0)\n",
    "MIERCOLESR10=pd.read_csv(\"RESIDENCIA10______MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESR10=pd.read_csv(\"RESIDENCIA10______JUEVES.csv\",index_col = 0)\n",
    "VIERNESR10=pd.read_csv(\"RESIDENCIA10______VIERNES.csv\",index_col = 0)\n",
    "SABADOR10=pd.read_csv(\"RESIDENCIA10______SABADO.csv\",index_col = 0)\n",
    "DOMINGOR10=pd.read_csv(\"RESIDENCIA10______DOMINGO.csv\",index_col = 0)\n",
    "##DATOS RESIDENCIA 11\n",
    "LUNESR11=pd.read_csv(\"RESIDENCIA11______LUNES.csv\",index_col = 0)\n",
    "MARTESR11=pd.read_csv(\"RESIDENCIA11______MARTES.csv\",index_col = 0)\n",
    "MIERCOLESR11=pd.read_csv(\"RESIDENCIA11______MIERCOLES.csv\",index_col = 0)\n",
    "JUEVESR11=pd.read_csv(\"RESIDENCIA11______JUEVES.csv\",index_col = 0)\n",
    "VIERNESR11=pd.read_csv(\"RESIDENCIA11______VIERNES.csv\",index_col = 0)\n",
    "SABADOR11=pd.read_csv(\"RESIDENCIA11______SABADO.csv\",index_col = 0)\n",
    "DOMINGOR11=pd.read_csv(\"RESIDENCIA11______DOMINGO.csv\",index_col = 0)\n",
    "\n",
    "total=pd.concat([LUNES,MARTES,MIERCOLES,JUEVES,VIERNES,SABADO,DOMINGO,LUNESEV,MARTESEV,MIERCOLESEV,JUEVESEV,VIERNESEV,SABADOEV,DOMINGOEV,\n",
    "                LUNESKG,MARTESKG,MIERCOLESKG,JUEVESKG,VIERNESKG,SABADOKG,DOMINGOKG,\n",
    "                LUNESFR,MARTESFR,MIERCOLESFR,JUEVESFR,VIERNESFR,SABADOFR,DOMINGOFR,\n",
    "                LUNESR1,MARTESR1,MIERCOLESR1,JUEVESR1,VIERNESR1,SABADOR1,DOMINGOR1,\n",
    "                LUNESR10,MARTESR10,MIERCOLESR10,JUEVESR10,VIERNESR10,SABADOR10,DOMINGOR10,\n",
    "                LUNESR11,MARTESR11,MIERCOLESR11,JUEVESR11,VIERNESR11,SABADOR11,DOMINGOR11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total2=total.sample(frac=1, replace=True, random_state=1)\n",
    "No_dias=total.shape[0]\n",
    "dataset = np.zeros((No_dias,99))\n",
    "dataset=total2.iloc[:,0:99]\n",
    "salida=total2.iloc[:,99:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156 789 3945\n",
      "(3156, 99) (789, 99) (3156, 33) (789, 33)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size,test_size,len(dataset))\n",
    "Xtrain=np.array(dataset.iloc[0:train_size,:])\n",
    "Xtest=np.array(dataset.iloc[train_size:,:])\n",
    "Ytrain=np.array(salida.iloc[0:train_size,:])\n",
    "Ytest=np.array(salida.iloc[train_size:,:])\n",
    "print(Xtrain.shape,Xtest.shape,Ytrain.shape,Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['escalamieto_OP.pkl']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'escalamieto_OP.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODIGO DE OPTIMIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xtrain\n",
    "y_train = Ytrain\n",
    "input_shape= X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.6614 - acc: 0.6222 - val_loss: 0.5806 - val_acc: 0.7562\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.4584 - acc: 0.8343 - val_loss: 0.3462 - val_acc: 0.8870\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.2862 - acc: 0.9020 - val_loss: 0.2333 - val_acc: 0.9383\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.2117 - acc: 0.9474 - val_loss: 0.1909 - val_acc: 0.9498\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1867 - acc: 0.9491 - val_loss: 0.1759 - val_acc: 0.9510\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1737 - acc: 0.9499 - val_loss: 0.1670 - val_acc: 0.9513\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1649 - acc: 0.9506 - val_loss: 0.1605 - val_acc: 0.9510\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1583 - acc: 0.9508 - val_loss: 0.1560 - val_acc: 0.9506\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1525 - acc: 0.9514 - val_loss: 0.1520 - val_acc: 0.9507\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1474 - acc: 0.9520 - val_loss: 0.1487 - val_acc: 0.9510\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1431 - acc: 0.9523 - val_loss: 0.1456 - val_acc: 0.9515\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1391 - acc: 0.9528 - val_loss: 0.1429 - val_acc: 0.9527\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1356 - acc: 0.9533 - val_loss: 0.1408 - val_acc: 0.9524\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1326 - acc: 0.9537 - val_loss: 0.1387 - val_acc: 0.9528\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1300 - acc: 0.9539 - val_loss: 0.1371 - val_acc: 0.9537\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1276 - acc: 0.9543 - val_loss: 0.1355 - val_acc: 0.9538\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1255 - acc: 0.9547 - val_loss: 0.1342 - val_acc: 0.9540\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1237 - acc: 0.9554 - val_loss: 0.1334 - val_acc: 0.9537\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1220 - acc: 0.9554 - val_loss: 0.1321 - val_acc: 0.9538\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.1207 - acc: 0.9559 - val_loss: 0.1312 - val_acc: 0.9544\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1193 - acc: 0.9561 - val_loss: 0.1305 - val_acc: 0.9546\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1179 - acc: 0.9569 - val_loss: 0.1296 - val_acc: 0.9546\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1168 - acc: 0.9571 - val_loss: 0.1287 - val_acc: 0.9547\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1157 - acc: 0.9575 - val_loss: 0.1280 - val_acc: 0.9545\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.1147 - acc: 0.9580 - val_loss: 0.1275 - val_acc: 0.9546\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1136 - acc: 0.9582 - val_loss: 0.1269 - val_acc: 0.9547\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1126 - acc: 0.9584 - val_loss: 0.1265 - val_acc: 0.9549\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1117 - acc: 0.9586 - val_loss: 0.1259 - val_acc: 0.9551\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.1108 - acc: 0.9590 - val_loss: 0.1252 - val_acc: 0.9551\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1099 - acc: 0.9593 - val_loss: 0.1248 - val_acc: 0.9552\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1092 - acc: 0.9592 - val_loss: 0.1243 - val_acc: 0.9549\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.1084 - acc: 0.9596 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.1077 - acc: 0.9598 - val_loss: 0.1235 - val_acc: 0.9561\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1071 - acc: 0.9600 - val_loss: 0.1230 - val_acc: 0.9556\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1063 - acc: 0.9604 - val_loss: 0.1227 - val_acc: 0.9557\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 0.1057 - acc: 0.9603 - val_loss: 0.1224 - val_acc: 0.9561\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1052 - acc: 0.9606 - val_loss: 0.1222 - val_acc: 0.9555\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1045 - acc: 0.9609 - val_loss: 0.1218 - val_acc: 0.9563\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1041 - acc: 0.9609 - val_loss: 0.1215 - val_acc: 0.9560\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1036 - acc: 0.9610 - val_loss: 0.1213 - val_acc: 0.9565\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1032 - acc: 0.9614 - val_loss: 0.1212 - val_acc: 0.9557\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1025 - acc: 0.9616 - val_loss: 0.1208 - val_acc: 0.9565\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1021 - acc: 0.9618 - val_loss: 0.1203 - val_acc: 0.9569\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.1016 - acc: 0.9620 - val_loss: 0.1203 - val_acc: 0.9568\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1011 - acc: 0.9623 - val_loss: 0.1200 - val_acc: 0.9568\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1007 - acc: 0.9624 - val_loss: 0.1197 - val_acc: 0.9574\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1003 - acc: 0.9627 - val_loss: 0.1195 - val_acc: 0.9570\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0999 - acc: 0.9630 - val_loss: 0.1194 - val_acc: 0.9574\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0994 - acc: 0.9633 - val_loss: 0.1195 - val_acc: 0.9570\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0991 - acc: 0.9633 - val_loss: 0.1192 - val_acc: 0.9574\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0988 - acc: 0.9632 - val_loss: 0.1188 - val_acc: 0.9567\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0983 - acc: 0.9637 - val_loss: 0.1188 - val_acc: 0.9571\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0980 - acc: 0.9636 - val_loss: 0.1186 - val_acc: 0.9571\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0976 - acc: 0.9639 - val_loss: 0.1183 - val_acc: 0.9579\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0973 - acc: 0.9639 - val_loss: 0.1181 - val_acc: 0.9576\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0969 - acc: 0.9641 - val_loss: 0.1180 - val_acc: 0.9581\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0966 - acc: 0.9644 - val_loss: 0.1178 - val_acc: 0.9576\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0963 - acc: 0.9643 - val_loss: 0.1177 - val_acc: 0.9575\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0960 - acc: 0.9646 - val_loss: 0.1177 - val_acc: 0.9579\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0957 - acc: 0.9645 - val_loss: 0.1173 - val_acc: 0.9584\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0953 - acc: 0.9645 - val_loss: 0.1173 - val_acc: 0.9580\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0951 - acc: 0.9650 - val_loss: 0.1173 - val_acc: 0.9582\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0947 - acc: 0.9651 - val_loss: 0.1172 - val_acc: 0.9580\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0944 - acc: 0.9653 - val_loss: 0.1171 - val_acc: 0.9582\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0942 - acc: 0.9652 - val_loss: 0.1169 - val_acc: 0.9583\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0939 - acc: 0.9653 - val_loss: 0.1168 - val_acc: 0.9586\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0936 - acc: 0.9655 - val_loss: 0.1166 - val_acc: 0.9584\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0934 - acc: 0.9656 - val_loss: 0.1165 - val_acc: 0.9586\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.0931 - acc: 0.9655 - val_loss: 0.1165 - val_acc: 0.9583\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.0929 - acc: 0.9656 - val_loss: 0.1165 - val_acc: 0.9588\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0926 - acc: 0.9659 - val_loss: 0.1162 - val_acc: 0.9590\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0924 - acc: 0.9658 - val_loss: 0.1161 - val_acc: 0.9590\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0922 - acc: 0.9660 - val_loss: 0.1162 - val_acc: 0.9583\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0919 - acc: 0.9659 - val_loss: 0.1162 - val_acc: 0.9586\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0917 - acc: 0.9662 - val_loss: 0.1160 - val_acc: 0.9591\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0914 - acc: 0.9664 - val_loss: 0.1159 - val_acc: 0.9589\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0912 - acc: 0.9663 - val_loss: 0.1160 - val_acc: 0.9585\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0910 - acc: 0.9665 - val_loss: 0.1158 - val_acc: 0.9587\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0907 - acc: 0.9666 - val_loss: 0.1156 - val_acc: 0.9593\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0905 - acc: 0.9667 - val_loss: 0.1158 - val_acc: 0.9590\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0903 - acc: 0.9669 - val_loss: 0.1157 - val_acc: 0.9591\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0901 - acc: 0.9669 - val_loss: 0.1155 - val_acc: 0.9587\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0899 - acc: 0.9669 - val_loss: 0.1155 - val_acc: 0.9591\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0897 - acc: 0.9671 - val_loss: 0.1153 - val_acc: 0.9592\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0895 - acc: 0.9671 - val_loss: 0.1153 - val_acc: 0.9588\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0893 - acc: 0.9672 - val_loss: 0.1153 - val_acc: 0.9590\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0891 - acc: 0.9674 - val_loss: 0.1152 - val_acc: 0.9597\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0889 - acc: 0.9672 - val_loss: 0.1152 - val_acc: 0.9597\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0888 - acc: 0.9673 - val_loss: 0.1151 - val_acc: 0.9596\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0885 - acc: 0.9675 - val_loss: 0.1152 - val_acc: 0.9591\n",
      "\n",
      "Accuracy: 95.91%\n",
      "\n",
      "0\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 325us/step - loss: 0.5681 - acc: 0.7418 - val_loss: 0.4316 - val_acc: 0.8781\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.3322 - acc: 0.9251 - val_loss: 0.2663 - val_acc: 0.9486\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.2370 - acc: 0.9488 - val_loss: 0.2172 - val_acc: 0.9505\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.2067 - acc: 0.9496 - val_loss: 0.1976 - val_acc: 0.9506\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.1927 - acc: 0.9503 - val_loss: 0.1874 - val_acc: 0.9513\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.1848 - acc: 0.9510 - val_loss: 0.1813 - val_acc: 0.9517\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1795 - acc: 0.9514 - val_loss: 0.1768 - val_acc: 0.9520\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.1755 - acc: 0.9516 - val_loss: 0.1735 - val_acc: 0.9519\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.1722 - acc: 0.9518 - val_loss: 0.1708 - val_acc: 0.9519\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1695 - acc: 0.9521 - val_loss: 0.1685 - val_acc: 0.9518\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.1672 - acc: 0.9521 - val_loss: 0.1666 - val_acc: 0.9521\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1652 - acc: 0.9523 - val_loss: 0.1649 - val_acc: 0.9520\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.1634 - acc: 0.9523 - val_loss: 0.1636 - val_acc: 0.9522\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.1619 - acc: 0.9525 - val_loss: 0.1623 - val_acc: 0.9522\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 490us/step - loss: 0.1605 - acc: 0.9525 - val_loss: 0.1612 - val_acc: 0.9525\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.1593 - acc: 0.9527 - val_loss: 0.1602 - val_acc: 0.9522\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.1581 - acc: 0.9526 - val_loss: 0.1593 - val_acc: 0.9523\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.1570 - acc: 0.9528 - val_loss: 0.1584 - val_acc: 0.9524\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.1559 - acc: 0.9527 - val_loss: 0.1577 - val_acc: 0.9526\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.1550 - acc: 0.9529 - val_loss: 0.1571 - val_acc: 0.9528\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.1541 - acc: 0.9529 - val_loss: 0.1564 - val_acc: 0.9528\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.1533 - acc: 0.9530 - val_loss: 0.1559 - val_acc: 0.9527\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1526 - acc: 0.9529 - val_loss: 0.1553 - val_acc: 0.9527\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.1518 - acc: 0.9532 - val_loss: 0.1549 - val_acc: 0.9528\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.1511 - acc: 0.9532 - val_loss: 0.1543 - val_acc: 0.9528\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1505 - acc: 0.9533 - val_loss: 0.1540 - val_acc: 0.9528\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.1499 - acc: 0.9534 - val_loss: 0.1535 - val_acc: 0.9529\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1493 - acc: 0.9534 - val_loss: 0.1531 - val_acc: 0.9530\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.1487 - acc: 0.9534 - val_loss: 0.1527 - val_acc: 0.9530\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.1482 - acc: 0.9535 - val_loss: 0.1523 - val_acc: 0.9530\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1477 - acc: 0.9536 - val_loss: 0.1520 - val_acc: 0.9529\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1472 - acc: 0.9537 - val_loss: 0.1517 - val_acc: 0.9529\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1467 - acc: 0.9537 - val_loss: 0.1514 - val_acc: 0.9528\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 118us/step - loss: 0.1463 - acc: 0.9537 - val_loss: 0.1511 - val_acc: 0.9528\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.1458 - acc: 0.9536 - val_loss: 0.1508 - val_acc: 0.9527\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.1454 - acc: 0.9538 - val_loss: 0.1505 - val_acc: 0.9528\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.1450 - acc: 0.9538 - val_loss: 0.1502 - val_acc: 0.9527\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 0.1447 - acc: 0.9539 - val_loss: 0.1500 - val_acc: 0.9529\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1443 - acc: 0.9538 - val_loss: 0.1498 - val_acc: 0.9527\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1439 - acc: 0.9539 - val_loss: 0.1494 - val_acc: 0.9529\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 0.1435 - acc: 0.9539 - val_loss: 0.1492 - val_acc: 0.9529\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1432 - acc: 0.9538 - val_loss: 0.1490 - val_acc: 0.9529\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.1428 - acc: 0.9539 - val_loss: 0.1487 - val_acc: 0.9530\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.1425 - acc: 0.9540 - val_loss: 0.1484 - val_acc: 0.9531\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.1422 - acc: 0.9540 - val_loss: 0.1482 - val_acc: 0.9531\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1419 - acc: 0.9539 - val_loss: 0.1479 - val_acc: 0.9531\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.1415 - acc: 0.9540 - val_loss: 0.1477 - val_acc: 0.9532\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1412 - acc: 0.9541 - val_loss: 0.1475 - val_acc: 0.9533\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1409 - acc: 0.9541 - val_loss: 0.1473 - val_acc: 0.9533\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.1407 - acc: 0.9540 - val_loss: 0.1471 - val_acc: 0.9533\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.1404 - acc: 0.9541 - val_loss: 0.1469 - val_acc: 0.9533\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1401 - acc: 0.9541 - val_loss: 0.1466 - val_acc: 0.9533\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.1399 - acc: 0.9542 - val_loss: 0.1465 - val_acc: 0.9532\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.1396 - acc: 0.9542 - val_loss: 0.1463 - val_acc: 0.9532\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 247us/step - loss: 0.1394 - acc: 0.9542 - val_loss: 0.1461 - val_acc: 0.9531\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 1s 254us/step - loss: 0.1391 - acc: 0.9542 - val_loss: 0.1459 - val_acc: 0.9532\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 1s 351us/step - loss: 0.1388 - acc: 0.9542 - val_loss: 0.1457 - val_acc: 0.9531\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1386 - acc: 0.9543 - val_loss: 0.1455 - val_acc: 0.9531\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 2s 587us/step - loss: 0.1383 - acc: 0.9543 - val_loss: 0.1453 - val_acc: 0.9531\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 186us/step - loss: 0.1381 - acc: 0.9543 - val_loss: 0.1451 - val_acc: 0.9531\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 124us/step - loss: 0.1378 - acc: 0.9543 - val_loss: 0.1449 - val_acc: 0.9532\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 194us/step - loss: 0.1376 - acc: 0.9545 - val_loss: 0.1447 - val_acc: 0.9532\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.1373 - acc: 0.9545 - val_loss: 0.1445 - val_acc: 0.9533\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.1371 - acc: 0.9545 - val_loss: 0.1444 - val_acc: 0.9532\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 169us/step - loss: 0.1369 - acc: 0.9546 - val_loss: 0.1442 - val_acc: 0.9532\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.1367 - acc: 0.9546 - val_loss: 0.1441 - val_acc: 0.9533\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.1365 - acc: 0.9547 - val_loss: 0.1439 - val_acc: 0.9533\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.1363 - acc: 0.9547 - val_loss: 0.1437 - val_acc: 0.9533\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.1360 - acc: 0.9547 - val_loss: 0.1436 - val_acc: 0.9535\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.1358 - acc: 0.9547 - val_loss: 0.1435 - val_acc: 0.9535\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.1356 - acc: 0.9549 - val_loss: 0.1433 - val_acc: 0.9535\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.1355 - acc: 0.9547 - val_loss: 0.1432 - val_acc: 0.9536\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1353 - acc: 0.9548 - val_loss: 0.1430 - val_acc: 0.9536\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.1351 - acc: 0.9549 - val_loss: 0.1429 - val_acc: 0.9535\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1349 - acc: 0.9548 - val_loss: 0.1427 - val_acc: 0.9534\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1347 - acc: 0.9550 - val_loss: 0.1426 - val_acc: 0.9535\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.1345 - acc: 0.9549 - val_loss: 0.1425 - val_acc: 0.9535\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1344 - acc: 0.9549 - val_loss: 0.1424 - val_acc: 0.9535\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.1342 - acc: 0.9549 - val_loss: 0.1423 - val_acc: 0.9533\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 1s 191us/step - loss: 0.1340 - acc: 0.9549 - val_loss: 0.1421 - val_acc: 0.9533\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 1s 242us/step - loss: 0.1339 - acc: 0.9549 - val_loss: 0.1420 - val_acc: 0.9533\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.1337 - acc: 0.9550 - val_loss: 0.1419 - val_acc: 0.9533\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 1s 255us/step - loss: 0.1336 - acc: 0.9550 - val_loss: 0.1418 - val_acc: 0.9533\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.1334 - acc: 0.9551 - val_loss: 0.1417 - val_acc: 0.9535\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.1332 - acc: 0.9552 - val_loss: 0.1416 - val_acc: 0.9535\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.1331 - acc: 0.9551 - val_loss: 0.1415 - val_acc: 0.9535\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1330 - acc: 0.9551 - val_loss: 0.1414 - val_acc: 0.9535\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 176us/step - loss: 0.1328 - acc: 0.9552 - val_loss: 0.1413 - val_acc: 0.9535\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 160us/step - loss: 0.1327 - acc: 0.9552 - val_loss: 0.1412 - val_acc: 0.9536\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.1325 - acc: 0.9553 - val_loss: 0.1411 - val_acc: 0.9537\n",
      "\n",
      "Accuracy: 95.37%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 337us/step - loss: 0.4800 - acc: 0.7746 - val_loss: 0.1919 - val_acc: 0.9376\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1511 - acc: 0.9485 - val_loss: 0.1408 - val_acc: 0.9510\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1315 - acc: 0.9526 - val_loss: 0.1301 - val_acc: 0.9535\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1235 - acc: 0.9545 - val_loss: 0.1266 - val_acc: 0.9538\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1186 - acc: 0.9553 - val_loss: 0.1240 - val_acc: 0.9544\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1144 - acc: 0.9565 - val_loss: 0.1212 - val_acc: 0.9544\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1109 - acc: 0.9578 - val_loss: 0.1185 - val_acc: 0.9552\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1077 - acc: 0.9593 - val_loss: 0.1178 - val_acc: 0.9542\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1048 - acc: 0.9601 - val_loss: 0.1165 - val_acc: 0.9557\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1026 - acc: 0.9611 - val_loss: 0.1145 - val_acc: 0.9560\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0998 - acc: 0.9624 - val_loss: 0.1126 - val_acc: 0.9564\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 253us/step - loss: 0.0975 - acc: 0.9633 - val_loss: 0.1119 - val_acc: 0.9563\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 189us/step - loss: 0.0954 - acc: 0.9643 - val_loss: 0.1101 - val_acc: 0.9565\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.0934 - acc: 0.9646 - val_loss: 0.1096 - val_acc: 0.9571\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0916 - acc: 0.9657 - val_loss: 0.1093 - val_acc: 0.9567\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0897 - acc: 0.9665 - val_loss: 0.1080 - val_acc: 0.9579\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 175us/step - loss: 0.0882 - acc: 0.9670 - val_loss: 0.1067 - val_acc: 0.9577\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 364us/step - loss: 0.0865 - acc: 0.9675 - val_loss: 0.1064 - val_acc: 0.9582\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 1s 229us/step - loss: 0.0851 - acc: 0.9683 - val_loss: 0.1061 - val_acc: 0.9591\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 243us/step - loss: 0.0836 - acc: 0.9694 - val_loss: 0.1049 - val_acc: 0.9592\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 161us/step - loss: 0.0822 - acc: 0.9698 - val_loss: 0.1045 - val_acc: 0.9591\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0809 - acc: 0.9704 - val_loss: 0.1036 - val_acc: 0.9600\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0794 - acc: 0.9710 - val_loss: 0.1026 - val_acc: 0.9604\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0783 - acc: 0.9718 - val_loss: 0.1016 - val_acc: 0.9611\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0773 - acc: 0.9715 - val_loss: 0.1018 - val_acc: 0.9612\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0759 - acc: 0.9729 - val_loss: 0.1013 - val_acc: 0.9602\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.0750 - acc: 0.9727 - val_loss: 0.1010 - val_acc: 0.9613\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0740 - acc: 0.9734 - val_loss: 0.1001 - val_acc: 0.9616\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0728 - acc: 0.9737 - val_loss: 0.0996 - val_acc: 0.9617\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.0718 - acc: 0.9744 - val_loss: 0.0993 - val_acc: 0.9620\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0710 - acc: 0.9745 - val_loss: 0.0994 - val_acc: 0.9617\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0700 - acc: 0.9751 - val_loss: 0.0987 - val_acc: 0.9621\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0690 - acc: 0.9753 - val_loss: 0.0977 - val_acc: 0.9627\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0683 - acc: 0.9758 - val_loss: 0.0979 - val_acc: 0.9630\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0675 - acc: 0.9760 - val_loss: 0.0975 - val_acc: 0.9630\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0667 - acc: 0.9763 - val_loss: 0.0967 - val_acc: 0.9630\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0658 - acc: 0.9769 - val_loss: 0.0974 - val_acc: 0.9632\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0651 - acc: 0.9769 - val_loss: 0.0967 - val_acc: 0.9636\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0643 - acc: 0.9772 - val_loss: 0.0965 - val_acc: 0.9639\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0637 - acc: 0.9778 - val_loss: 0.0959 - val_acc: 0.9636\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0628 - acc: 0.9779 - val_loss: 0.0959 - val_acc: 0.9638\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0621 - acc: 0.9781 - val_loss: 0.0966 - val_acc: 0.9638\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0615 - acc: 0.9785 - val_loss: 0.0955 - val_acc: 0.9643\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0608 - acc: 0.9787 - val_loss: 0.0951 - val_acc: 0.9646\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0602 - acc: 0.9791 - val_loss: 0.0948 - val_acc: 0.9650\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0597 - acc: 0.9793 - val_loss: 0.0951 - val_acc: 0.9662\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0591 - acc: 0.9795 - val_loss: 0.0945 - val_acc: 0.9650\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0946 - val_acc: 0.9654\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0579 - acc: 0.9801 - val_loss: 0.0941 - val_acc: 0.9654\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0574 - acc: 0.9802 - val_loss: 0.0939 - val_acc: 0.9652\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0567 - acc: 0.9807 - val_loss: 0.0936 - val_acc: 0.9662\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0562 - acc: 0.9810 - val_loss: 0.0938 - val_acc: 0.9661\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0557 - acc: 0.9808 - val_loss: 0.0935 - val_acc: 0.9660\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0551 - acc: 0.9814 - val_loss: 0.0933 - val_acc: 0.9662\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0546 - acc: 0.9815 - val_loss: 0.0933 - val_acc: 0.9667\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0542 - acc: 0.9817 - val_loss: 0.0931 - val_acc: 0.9662\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0536 - acc: 0.9821 - val_loss: 0.0930 - val_acc: 0.9669\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0533 - acc: 0.9822 - val_loss: 0.0929 - val_acc: 0.9664\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0527 - acc: 0.9826 - val_loss: 0.0926 - val_acc: 0.9668\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0523 - acc: 0.9826 - val_loss: 0.0923 - val_acc: 0.9673\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0519 - acc: 0.9828 - val_loss: 0.0920 - val_acc: 0.9677\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0514 - acc: 0.9831 - val_loss: 0.0921 - val_acc: 0.9676\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0510 - acc: 0.9835 - val_loss: 0.0919 - val_acc: 0.9675\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0506 - acc: 0.9834 - val_loss: 0.0917 - val_acc: 0.9684\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0501 - acc: 0.9839 - val_loss: 0.0919 - val_acc: 0.9674\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0497 - acc: 0.9840 - val_loss: 0.0916 - val_acc: 0.9683\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0494 - acc: 0.9841 - val_loss: 0.0915 - val_acc: 0.9680\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0490 - acc: 0.9844 - val_loss: 0.0914 - val_acc: 0.9677\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0486 - acc: 0.9847 - val_loss: 0.0912 - val_acc: 0.9687\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0482 - acc: 0.9845 - val_loss: 0.0913 - val_acc: 0.9682\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0478 - acc: 0.9851 - val_loss: 0.0912 - val_acc: 0.9682\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0474 - acc: 0.9852 - val_loss: 0.0908 - val_acc: 0.9687\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0470 - acc: 0.9851 - val_loss: 0.0910 - val_acc: 0.9685\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0467 - acc: 0.9855 - val_loss: 0.0909 - val_acc: 0.9683\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0463 - acc: 0.9854 - val_loss: 0.0907 - val_acc: 0.9684\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0460 - acc: 0.9858 - val_loss: 0.0906 - val_acc: 0.9688\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0457 - acc: 0.9858 - val_loss: 0.0904 - val_acc: 0.9692\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0453 - acc: 0.9862 - val_loss: 0.0903 - val_acc: 0.9692\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0450 - acc: 0.9863 - val_loss: 0.0903 - val_acc: 0.9692\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0447 - acc: 0.9866 - val_loss: 0.0901 - val_acc: 0.9692\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0445 - acc: 0.9865 - val_loss: 0.0901 - val_acc: 0.9689\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0441 - acc: 0.9866 - val_loss: 0.0900 - val_acc: 0.9685\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0438 - acc: 0.9870 - val_loss: 0.0901 - val_acc: 0.9686\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0435 - acc: 0.9870 - val_loss: 0.0899 - val_acc: 0.9689\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0431 - acc: 0.9872 - val_loss: 0.0896 - val_acc: 0.9695\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0429 - acc: 0.9873 - val_loss: 0.0898 - val_acc: 0.9689\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 361us/step - loss: 0.0427 - acc: 0.9876 - val_loss: 0.0900 - val_acc: 0.9689\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0424 - acc: 0.9876 - val_loss: 0.0897 - val_acc: 0.9692\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0420 - acc: 0.9877 - val_loss: 0.0895 - val_acc: 0.9693\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0417 - acc: 0.9878 - val_loss: 0.0896 - val_acc: 0.9690\n",
      "\n",
      "Accuracy: 96.90%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 221us/step - loss: 0.7045 - acc: 0.5272 - val_loss: 0.6875 - val_acc: 0.5683\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.6782 - acc: 0.5931 - val_loss: 0.6673 - val_acc: 0.6151\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.6618 - acc: 0.6261 - val_loss: 0.6530 - val_acc: 0.6410\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.6489 - acc: 0.6458 - val_loss: 0.6407 - val_acc: 0.6577\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.6368 - acc: 0.6646 - val_loss: 0.6295 - val_acc: 0.6780\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.6244 - acc: 0.6819 - val_loss: 0.6174 - val_acc: 0.6969\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.6117 - acc: 0.6987 - val_loss: 0.6046 - val_acc: 0.7174\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.5983 - acc: 0.7154 - val_loss: 0.5915 - val_acc: 0.7337\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.5840 - acc: 0.7330 - val_loss: 0.5775 - val_acc: 0.7514\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.5690 - acc: 0.7543 - val_loss: 0.5629 - val_acc: 0.7718\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.5533 - acc: 0.7697 - val_loss: 0.5474 - val_acc: 0.7862\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.5369 - acc: 0.7853 - val_loss: 0.5319 - val_acc: 0.8025\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.5202 - acc: 0.8050 - val_loss: 0.5157 - val_acc: 0.8191\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.5033 - acc: 0.8246 - val_loss: 0.4996 - val_acc: 0.8315\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.4860 - acc: 0.8353 - val_loss: 0.4823 - val_acc: 0.8408\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.4689 - acc: 0.8495 - val_loss: 0.4662 - val_acc: 0.8540\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.4522 - acc: 0.8633 - val_loss: 0.4501 - val_acc: 0.8640\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.4358 - acc: 0.8706 - val_loss: 0.4336 - val_acc: 0.8717\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.4198 - acc: 0.8806 - val_loss: 0.4189 - val_acc: 0.8795\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.4045 - acc: 0.8890 - val_loss: 0.4038 - val_acc: 0.8854\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.3897 - acc: 0.8961 - val_loss: 0.3898 - val_acc: 0.8915\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.3758 - acc: 0.9026 - val_loss: 0.3764 - val_acc: 0.8950\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.3623 - acc: 0.9069 - val_loss: 0.3633 - val_acc: 0.8996\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.3498 - acc: 0.9107 - val_loss: 0.3514 - val_acc: 0.9037\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.3379 - acc: 0.9149 - val_loss: 0.3398 - val_acc: 0.9071\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.3266 - acc: 0.9182 - val_loss: 0.3289 - val_acc: 0.9096\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.3162 - acc: 0.9205 - val_loss: 0.3188 - val_acc: 0.9123\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.3063 - acc: 0.9239 - val_loss: 0.3094 - val_acc: 0.9155\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.2971 - acc: 0.9264 - val_loss: 0.3006 - val_acc: 0.9182\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2885 - acc: 0.9290 - val_loss: 0.2922 - val_acc: 0.9205\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2806 - acc: 0.9310 - val_loss: 0.2841 - val_acc: 0.9225\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2730 - acc: 0.9325 - val_loss: 0.2771 - val_acc: 0.9233\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.2661 - acc: 0.9342 - val_loss: 0.2704 - val_acc: 0.9250\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2596 - acc: 0.9358 - val_loss: 0.2641 - val_acc: 0.9257\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2535 - acc: 0.9369 - val_loss: 0.2580 - val_acc: 0.9271\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.2479 - acc: 0.9378 - val_loss: 0.2525 - val_acc: 0.9294\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.2425 - acc: 0.9389 - val_loss: 0.2472 - val_acc: 0.9308\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2377 - acc: 0.9395 - val_loss: 0.2426 - val_acc: 0.9317\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.2330 - acc: 0.9406 - val_loss: 0.2381 - val_acc: 0.9324\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2287 - acc: 0.9413 - val_loss: 0.2338 - val_acc: 0.9337\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.2247 - acc: 0.9419 - val_loss: 0.2298 - val_acc: 0.9350\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2210 - acc: 0.9426 - val_loss: 0.2260 - val_acc: 0.9360\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.2174 - acc: 0.9431 - val_loss: 0.2226 - val_acc: 0.9368\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2141 - acc: 0.9434 - val_loss: 0.2193 - val_acc: 0.9379\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.2110 - acc: 0.9439 - val_loss: 0.2162 - val_acc: 0.9385\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2080 - acc: 0.9442 - val_loss: 0.2133 - val_acc: 0.9388\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2053 - acc: 0.9446 - val_loss: 0.2105 - val_acc: 0.9404\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.2027 - acc: 0.9449 - val_loss: 0.2079 - val_acc: 0.9414\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.2003 - acc: 0.9452 - val_loss: 0.2055 - val_acc: 0.9417\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1979 - acc: 0.9454 - val_loss: 0.2032 - val_acc: 0.9425\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1957 - acc: 0.9456 - val_loss: 0.2009 - val_acc: 0.9429\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1937 - acc: 0.9457 - val_loss: 0.1988 - val_acc: 0.9433\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1917 - acc: 0.9458 - val_loss: 0.1969 - val_acc: 0.9437\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1899 - acc: 0.9461 - val_loss: 0.1950 - val_acc: 0.9438\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1881 - acc: 0.9463 - val_loss: 0.1933 - val_acc: 0.9442\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1864 - acc: 0.9466 - val_loss: 0.1915 - val_acc: 0.9448\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1848 - acc: 0.9468 - val_loss: 0.1899 - val_acc: 0.9451\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1834 - acc: 0.9472 - val_loss: 0.1883 - val_acc: 0.9455\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1819 - acc: 0.9472 - val_loss: 0.1868 - val_acc: 0.9457\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1805 - acc: 0.9473 - val_loss: 0.1855 - val_acc: 0.9459\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1793 - acc: 0.9473 - val_loss: 0.1841 - val_acc: 0.9459\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1780 - acc: 0.9475 - val_loss: 0.1829 - val_acc: 0.9460\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1769 - acc: 0.9475 - val_loss: 0.1816 - val_acc: 0.9462\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1757 - acc: 0.9476 - val_loss: 0.1805 - val_acc: 0.9462\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1746 - acc: 0.9476 - val_loss: 0.1793 - val_acc: 0.9464\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1736 - acc: 0.9478 - val_loss: 0.1783 - val_acc: 0.9464\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1726 - acc: 0.9478 - val_loss: 0.1772 - val_acc: 0.9467\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1716 - acc: 0.9481 - val_loss: 0.1762 - val_acc: 0.9468\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1707 - acc: 0.9482 - val_loss: 0.1752 - val_acc: 0.9469\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1698 - acc: 0.9483 - val_loss: 0.1743 - val_acc: 0.9471\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1690 - acc: 0.9484 - val_loss: 0.1734 - val_acc: 0.9472\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1682 - acc: 0.9484 - val_loss: 0.1726 - val_acc: 0.9471\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1674 - acc: 0.9485 - val_loss: 0.1717 - val_acc: 0.9471\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1666 - acc: 0.9485 - val_loss: 0.1710 - val_acc: 0.9473\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1659 - acc: 0.9486 - val_loss: 0.1702 - val_acc: 0.9475\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1652 - acc: 0.9487 - val_loss: 0.1695 - val_acc: 0.9474\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1646 - acc: 0.9488 - val_loss: 0.1688 - val_acc: 0.9474\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1639 - acc: 0.9488 - val_loss: 0.1681 - val_acc: 0.9476\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1633 - acc: 0.9490 - val_loss: 0.1674 - val_acc: 0.9476\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1627 - acc: 0.9489 - val_loss: 0.1668 - val_acc: 0.9475\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1621 - acc: 0.9489 - val_loss: 0.1662 - val_acc: 0.9476\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1615 - acc: 0.9490 - val_loss: 0.1656 - val_acc: 0.9476\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1610 - acc: 0.9491 - val_loss: 0.1650 - val_acc: 0.9476\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1605 - acc: 0.9491 - val_loss: 0.1644 - val_acc: 0.9478\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1599 - acc: 0.9491 - val_loss: 0.1638 - val_acc: 0.9478\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1594 - acc: 0.9491 - val_loss: 0.1633 - val_acc: 0.9478\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1589 - acc: 0.9491 - val_loss: 0.1628 - val_acc: 0.9479\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1585 - acc: 0.9491 - val_loss: 0.1623 - val_acc: 0.9480\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1580 - acc: 0.9492 - val_loss: 0.1618 - val_acc: 0.9480\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1576 - acc: 0.9493 - val_loss: 0.1613 - val_acc: 0.9483\n",
      "\n",
      "Accuracy: 94.83%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 330us/step - loss: 0.5276 - acc: 0.7433 - val_loss: 0.2293 - val_acc: 0.9379\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.1647 - acc: 0.9470 - val_loss: 0.1459 - val_acc: 0.9482\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.1407 - acc: 0.9509 - val_loss: 0.1382 - val_acc: 0.9506\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.1338 - acc: 0.9524 - val_loss: 0.1342 - val_acc: 0.9517\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.1290 - acc: 0.9532 - val_loss: 0.1314 - val_acc: 0.9519\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.1252 - acc: 0.9539 - val_loss: 0.1284 - val_acc: 0.9525\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1218 - acc: 0.9545 - val_loss: 0.1262 - val_acc: 0.9535\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.1190 - acc: 0.9550 - val_loss: 0.1247 - val_acc: 0.9533\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.1165 - acc: 0.9557 - val_loss: 0.1231 - val_acc: 0.9537\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 222us/step - loss: 0.1145 - acc: 0.9567 - val_loss: 0.1220 - val_acc: 0.9538\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 1s 353us/step - loss: 0.1123 - acc: 0.9573 - val_loss: 0.1218 - val_acc: 0.9539\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.1105 - acc: 0.9583 - val_loss: 0.1198 - val_acc: 0.9549\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1087 - acc: 0.9588 - val_loss: 0.1187 - val_acc: 0.9553\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.1071 - acc: 0.9596 - val_loss: 0.1175 - val_acc: 0.9552\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1054 - acc: 0.9599 - val_loss: 0.1163 - val_acc: 0.9554\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1039 - acc: 0.9607 - val_loss: 0.1157 - val_acc: 0.9557\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1025 - acc: 0.9615 - val_loss: 0.1148 - val_acc: 0.9553\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 161us/step - loss: 0.1012 - acc: 0.9620 - val_loss: 0.1142 - val_acc: 0.9567\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.1000 - acc: 0.9626 - val_loss: 0.1134 - val_acc: 0.9566\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.0988 - acc: 0.9629 - val_loss: 0.1129 - val_acc: 0.9568\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0975 - acc: 0.9632 - val_loss: 0.1114 - val_acc: 0.9568\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0963 - acc: 0.9638 - val_loss: 0.1117 - val_acc: 0.9570\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0951 - acc: 0.9643 - val_loss: 0.1109 - val_acc: 0.9572\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0940 - acc: 0.9649 - val_loss: 0.1101 - val_acc: 0.9576\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0930 - acc: 0.9653 - val_loss: 0.1098 - val_acc: 0.9575\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 1s 197us/step - loss: 0.0920 - acc: 0.9656 - val_loss: 0.1096 - val_acc: 0.9583\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 1s 207us/step - loss: 0.0911 - acc: 0.9661 - val_loss: 0.1082 - val_acc: 0.9585\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.0900 - acc: 0.9664 - val_loss: 0.1081 - val_acc: 0.9584\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0892 - acc: 0.9668 - val_loss: 0.1078 - val_acc: 0.9584\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 161us/step - loss: 0.0883 - acc: 0.9673 - val_loss: 0.1071 - val_acc: 0.9590\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.0875 - acc: 0.9674 - val_loss: 0.1067 - val_acc: 0.9591\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0866 - acc: 0.9678 - val_loss: 0.1063 - val_acc: 0.9594\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.0857 - acc: 0.9679 - val_loss: 0.1057 - val_acc: 0.9591\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0850 - acc: 0.9681 - val_loss: 0.1058 - val_acc: 0.9595\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0844 - acc: 0.9684 - val_loss: 0.1051 - val_acc: 0.9594\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0835 - acc: 0.9689 - val_loss: 0.1045 - val_acc: 0.9593\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0827 - acc: 0.9689 - val_loss: 0.1042 - val_acc: 0.9596\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0821 - acc: 0.9693 - val_loss: 0.1044 - val_acc: 0.9600\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0813 - acc: 0.9697 - val_loss: 0.1035 - val_acc: 0.9598\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0807 - acc: 0.9698 - val_loss: 0.1033 - val_acc: 0.9598\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0800 - acc: 0.9700 - val_loss: 0.1031 - val_acc: 0.9600\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0794 - acc: 0.9704 - val_loss: 0.1030 - val_acc: 0.9597\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0788 - acc: 0.9708 - val_loss: 0.1027 - val_acc: 0.9604\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0782 - acc: 0.9713 - val_loss: 0.1022 - val_acc: 0.9604\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0776 - acc: 0.9713 - val_loss: 0.1023 - val_acc: 0.9606\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0770 - acc: 0.9715 - val_loss: 0.1021 - val_acc: 0.9600\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0764 - acc: 0.9719 - val_loss: 0.1017 - val_acc: 0.9607\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0759 - acc: 0.9720 - val_loss: 0.1018 - val_acc: 0.9610\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0753 - acc: 0.9725 - val_loss: 0.1012 - val_acc: 0.9610\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0747 - acc: 0.9723 - val_loss: 0.1011 - val_acc: 0.9610\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0742 - acc: 0.9729 - val_loss: 0.1006 - val_acc: 0.9618\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0737 - acc: 0.9731 - val_loss: 0.1004 - val_acc: 0.9615\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0732 - acc: 0.9733 - val_loss: 0.1006 - val_acc: 0.9613\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0728 - acc: 0.9735 - val_loss: 0.1001 - val_acc: 0.9614\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0722 - acc: 0.9736 - val_loss: 0.1001 - val_acc: 0.9615\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0718 - acc: 0.9739 - val_loss: 0.1001 - val_acc: 0.9619\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0713 - acc: 0.9741 - val_loss: 0.0995 - val_acc: 0.9615\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0709 - acc: 0.9744 - val_loss: 0.0992 - val_acc: 0.9618\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0704 - acc: 0.9744 - val_loss: 0.0993 - val_acc: 0.9619\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0700 - acc: 0.9747 - val_loss: 0.0991 - val_acc: 0.9626\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.0695 - acc: 0.9749 - val_loss: 0.0991 - val_acc: 0.9625\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 382us/step - loss: 0.0691 - acc: 0.9753 - val_loss: 0.0986 - val_acc: 0.9628\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 163us/step - loss: 0.0688 - acc: 0.9748 - val_loss: 0.0986 - val_acc: 0.9625\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0683 - acc: 0.9751 - val_loss: 0.0986 - val_acc: 0.9630\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 182us/step - loss: 0.0679 - acc: 0.9758 - val_loss: 0.0985 - val_acc: 0.9623\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0675 - acc: 0.9755 - val_loss: 0.0982 - val_acc: 0.9629\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0672 - acc: 0.9761 - val_loss: 0.0983 - val_acc: 0.9625\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0667 - acc: 0.9764 - val_loss: 0.0980 - val_acc: 0.9630\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 212us/step - loss: 0.0665 - acc: 0.9760 - val_loss: 0.0979 - val_acc: 0.9632\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0661 - acc: 0.9765 - val_loss: 0.0979 - val_acc: 0.9641\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0656 - acc: 0.9765 - val_loss: 0.0976 - val_acc: 0.9638\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0653 - acc: 0.9768 - val_loss: 0.0975 - val_acc: 0.9638\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0650 - acc: 0.9771 - val_loss: 0.0975 - val_acc: 0.9637\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0646 - acc: 0.9771 - val_loss: 0.0973 - val_acc: 0.9639\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0643 - acc: 0.9774 - val_loss: 0.0971 - val_acc: 0.9637\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0639 - acc: 0.9776 - val_loss: 0.0971 - val_acc: 0.9638\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0637 - acc: 0.9777 - val_loss: 0.0971 - val_acc: 0.9638\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0633 - acc: 0.9781 - val_loss: 0.0974 - val_acc: 0.9645\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0630 - acc: 0.9779 - val_loss: 0.0968 - val_acc: 0.9641\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0627 - acc: 0.9781 - val_loss: 0.0969 - val_acc: 0.9641\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0624 - acc: 0.9780 - val_loss: 0.0969 - val_acc: 0.9639\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0621 - acc: 0.9784 - val_loss: 0.0965 - val_acc: 0.9639\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0618 - acc: 0.9785 - val_loss: 0.0967 - val_acc: 0.9641\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0615 - acc: 0.9786 - val_loss: 0.0962 - val_acc: 0.9646\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0612 - acc: 0.9786 - val_loss: 0.0962 - val_acc: 0.9646\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0609 - acc: 0.9789 - val_loss: 0.0964 - val_acc: 0.9649\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0606 - acc: 0.9789 - val_loss: 0.0961 - val_acc: 0.9645\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0604 - acc: 0.9790 - val_loss: 0.0965 - val_acc: 0.9641\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0601 - acc: 0.9793 - val_loss: 0.0962 - val_acc: 0.9649\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0598 - acc: 0.9795 - val_loss: 0.0961 - val_acc: 0.9650\n",
      "\n",
      "Accuracy: 96.50%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 255us/step - loss: 0.5374 - acc: 0.8010 - val_loss: 0.3674 - val_acc: 0.9459\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.2791 - acc: 0.9487 - val_loss: 0.2211 - val_acc: 0.9506\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.2044 - acc: 0.9499 - val_loss: 0.1906 - val_acc: 0.9518\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1853 - acc: 0.9509 - val_loss: 0.1791 - val_acc: 0.9522\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1752 - acc: 0.9513 - val_loss: 0.1720 - val_acc: 0.9526\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1680 - acc: 0.9515 - val_loss: 0.1665 - val_acc: 0.9529\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1622 - acc: 0.9517 - val_loss: 0.1627 - val_acc: 0.9527\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1576 - acc: 0.9521 - val_loss: 0.1590 - val_acc: 0.9526\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1536 - acc: 0.9523 - val_loss: 0.1561 - val_acc: 0.9533\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1504 - acc: 0.9526 - val_loss: 0.1537 - val_acc: 0.9533\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1473 - acc: 0.9528 - val_loss: 0.1515 - val_acc: 0.9531\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1448 - acc: 0.9532 - val_loss: 0.1495 - val_acc: 0.9535\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1423 - acc: 0.9535 - val_loss: 0.1475 - val_acc: 0.9535\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1401 - acc: 0.9536 - val_loss: 0.1459 - val_acc: 0.9535\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1381 - acc: 0.9539 - val_loss: 0.1446 - val_acc: 0.9534\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1362 - acc: 0.9543 - val_loss: 0.1431 - val_acc: 0.9534\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1345 - acc: 0.9544 - val_loss: 0.1417 - val_acc: 0.9535\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1329 - acc: 0.9546 - val_loss: 0.1405 - val_acc: 0.9539\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1314 - acc: 0.9550 - val_loss: 0.1394 - val_acc: 0.9539\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1300 - acc: 0.9552 - val_loss: 0.1382 - val_acc: 0.9542\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1287 - acc: 0.9555 - val_loss: 0.1374 - val_acc: 0.9545\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.1275 - acc: 0.9559 - val_loss: 0.1365 - val_acc: 0.9542\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 169us/step - loss: 0.1263 - acc: 0.9560 - val_loss: 0.1357 - val_acc: 0.9547\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.1252 - acc: 0.9562 - val_loss: 0.1350 - val_acc: 0.9547\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.1242 - acc: 0.9563 - val_loss: 0.1342 - val_acc: 0.9549\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1233 - acc: 0.9567 - val_loss: 0.1334 - val_acc: 0.9552\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1224 - acc: 0.9571 - val_loss: 0.1327 - val_acc: 0.9551\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1214 - acc: 0.9573 - val_loss: 0.1321 - val_acc: 0.9552\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1206 - acc: 0.9574 - val_loss: 0.1314 - val_acc: 0.9553\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1198 - acc: 0.9577 - val_loss: 0.1309 - val_acc: 0.9555\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1191 - acc: 0.9578 - val_loss: 0.1304 - val_acc: 0.9559\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1183 - acc: 0.9581 - val_loss: 0.1299 - val_acc: 0.9561\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1176 - acc: 0.9581 - val_loss: 0.1294 - val_acc: 0.9559\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1169 - acc: 0.9585 - val_loss: 0.1288 - val_acc: 0.9560\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1163 - acc: 0.9585 - val_loss: 0.1284 - val_acc: 0.9560\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1157 - acc: 0.9587 - val_loss: 0.1278 - val_acc: 0.9560\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1150 - acc: 0.9588 - val_loss: 0.1275 - val_acc: 0.9560\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.1145 - acc: 0.9590 - val_loss: 0.1271 - val_acc: 0.9560\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.1140 - acc: 0.9592 - val_loss: 0.1267 - val_acc: 0.9563\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.1133 - acc: 0.9593 - val_loss: 0.1263 - val_acc: 0.9562\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1129 - acc: 0.9593 - val_loss: 0.1260 - val_acc: 0.9565\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 1s 326us/step - loss: 0.1124 - acc: 0.9597 - val_loss: 0.1256 - val_acc: 0.9562\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1119 - acc: 0.9597 - val_loss: 0.1252 - val_acc: 0.9564\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.1115 - acc: 0.9600 - val_loss: 0.1249 - val_acc: 0.9566\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1110 - acc: 0.9600 - val_loss: 0.1246 - val_acc: 0.9564\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1105 - acc: 0.9602 - val_loss: 0.1242 - val_acc: 0.9565\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1101 - acc: 0.9601 - val_loss: 0.1239 - val_acc: 0.9570\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.1097 - acc: 0.9606 - val_loss: 0.1236 - val_acc: 0.9569\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.1092 - acc: 0.9605 - val_loss: 0.1233 - val_acc: 0.9566\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1089 - acc: 0.9607 - val_loss: 0.1231 - val_acc: 0.9568\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1085 - acc: 0.9609 - val_loss: 0.1229 - val_acc: 0.9564\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1081 - acc: 0.9609 - val_loss: 0.1226 - val_acc: 0.9566\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1078 - acc: 0.9609 - val_loss: 0.1223 - val_acc: 0.9571\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1074 - acc: 0.9612 - val_loss: 0.1220 - val_acc: 0.9572\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1070 - acc: 0.9612 - val_loss: 0.1217 - val_acc: 0.9571\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1067 - acc: 0.9611 - val_loss: 0.1215 - val_acc: 0.9572\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.1063 - acc: 0.9615 - val_loss: 0.1212 - val_acc: 0.9573\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1060 - acc: 0.9615 - val_loss: 0.1211 - val_acc: 0.9572\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1057 - acc: 0.9616 - val_loss: 0.1209 - val_acc: 0.9574\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1054 - acc: 0.9618 - val_loss: 0.1207 - val_acc: 0.9573\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1051 - acc: 0.9618 - val_loss: 0.1204 - val_acc: 0.9577\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1048 - acc: 0.9619 - val_loss: 0.1202 - val_acc: 0.9574\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.1045 - acc: 0.9620 - val_loss: 0.1200 - val_acc: 0.9579\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1042 - acc: 0.9621 - val_loss: 0.1199 - val_acc: 0.9578\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1040 - acc: 0.9624 - val_loss: 0.1197 - val_acc: 0.9579\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1037 - acc: 0.9622 - val_loss: 0.1195 - val_acc: 0.9578\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1035 - acc: 0.9623 - val_loss: 0.1193 - val_acc: 0.9575\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1031 - acc: 0.9624 - val_loss: 0.1191 - val_acc: 0.9578\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1029 - acc: 0.9625 - val_loss: 0.1190 - val_acc: 0.9581\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1027 - acc: 0.9627 - val_loss: 0.1188 - val_acc: 0.9580\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1024 - acc: 0.9626 - val_loss: 0.1186 - val_acc: 0.9578\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1022 - acc: 0.9628 - val_loss: 0.1185 - val_acc: 0.9583\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1019 - acc: 0.9627 - val_loss: 0.1184 - val_acc: 0.9580\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1017 - acc: 0.9629 - val_loss: 0.1183 - val_acc: 0.9581\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1014 - acc: 0.9629 - val_loss: 0.1181 - val_acc: 0.9581\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1012 - acc: 0.9631 - val_loss: 0.1180 - val_acc: 0.9582\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1010 - acc: 0.9632 - val_loss: 0.1178 - val_acc: 0.9581\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.1008 - acc: 0.9633 - val_loss: 0.1177 - val_acc: 0.9582\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1006 - acc: 0.9633 - val_loss: 0.1175 - val_acc: 0.9581\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1004 - acc: 0.9634 - val_loss: 0.1175 - val_acc: 0.9582\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1002 - acc: 0.9635 - val_loss: 0.1173 - val_acc: 0.9583\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.1000 - acc: 0.9636 - val_loss: 0.1172 - val_acc: 0.9580\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0998 - acc: 0.9635 - val_loss: 0.1170 - val_acc: 0.9581\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0996 - acc: 0.9636 - val_loss: 0.1169 - val_acc: 0.9584\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0994 - acc: 0.9637 - val_loss: 0.1168 - val_acc: 0.9584\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0992 - acc: 0.9638 - val_loss: 0.1167 - val_acc: 0.9584\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0990 - acc: 0.9637 - val_loss: 0.1166 - val_acc: 0.9584\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0988 - acc: 0.9637 - val_loss: 0.1164 - val_acc: 0.9586\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0986 - acc: 0.9639 - val_loss: 0.1163 - val_acc: 0.9584\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0985 - acc: 0.9640 - val_loss: 0.1163 - val_acc: 0.9583\n",
      "\n",
      "Accuracy: 95.83%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 464us/step - loss: 0.6507 - acc: 0.6856 - val_loss: 0.6104 - val_acc: 0.7666\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.5677 - acc: 0.7830 - val_loss: 0.5290 - val_acc: 0.7980\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.4979 - acc: 0.8073 - val_loss: 0.4710 - val_acc: 0.8169\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.4501 - acc: 0.8305 - val_loss: 0.4305 - val_acc: 0.8487\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 174us/step - loss: 0.4157 - acc: 0.8676 - val_loss: 0.4000 - val_acc: 0.8868\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.3895 - acc: 0.8997 - val_loss: 0.3766 - val_acc: 0.9112\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.3690 - acc: 0.9146 - val_loss: 0.3579 - val_acc: 0.9183\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.3526 - acc: 0.9182 - val_loss: 0.3427 - val_acc: 0.9194\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 178us/step - loss: 0.3390 - acc: 0.9189 - val_loss: 0.3300 - val_acc: 0.9196\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.3275 - acc: 0.9191 - val_loss: 0.3190 - val_acc: 0.9199\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.3175 - acc: 0.9192 - val_loss: 0.3094 - val_acc: 0.9198\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.3088 - acc: 0.9192 - val_loss: 0.3011 - val_acc: 0.9198\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.3010 - acc: 0.9192 - val_loss: 0.2936 - val_acc: 0.9198\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.2942 - acc: 0.9192 - val_loss: 0.2871 - val_acc: 0.9198\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.2881 - acc: 0.9192 - val_loss: 0.2811 - val_acc: 0.9198\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.2826 - acc: 0.9194 - val_loss: 0.2758 - val_acc: 0.9201\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.2777 - acc: 0.9201 - val_loss: 0.2710 - val_acc: 0.9210\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2732 - acc: 0.9215 - val_loss: 0.2667 - val_acc: 0.9231\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.2692 - acc: 0.9241 - val_loss: 0.2627 - val_acc: 0.9262\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.2655 - acc: 0.9276 - val_loss: 0.2591 - val_acc: 0.9306\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.2621 - acc: 0.9322 - val_loss: 0.2558 - val_acc: 0.9361\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 393us/step - loss: 0.2590 - acc: 0.9381 - val_loss: 0.2527 - val_acc: 0.9409\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.2561 - acc: 0.9421 - val_loss: 0.2499 - val_acc: 0.9441\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.2534 - acc: 0.9455 - val_loss: 0.2473 - val_acc: 0.9467\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.2509 - acc: 0.9469 - val_loss: 0.2449 - val_acc: 0.9483\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.2486 - acc: 0.9479 - val_loss: 0.2426 - val_acc: 0.9491\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.2465 - acc: 0.9483 - val_loss: 0.2405 - val_acc: 0.9494\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.2444 - acc: 0.9487 - val_loss: 0.2385 - val_acc: 0.9496\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.2426 - acc: 0.9488 - val_loss: 0.2367 - val_acc: 0.9499\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 1s 228us/step - loss: 0.2407 - acc: 0.9488 - val_loss: 0.2349 - val_acc: 0.9499\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.2391 - acc: 0.9489 - val_loss: 0.2332 - val_acc: 0.9499\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 1s 240us/step - loss: 0.2375 - acc: 0.9489 - val_loss: 0.2317 - val_acc: 0.9499\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.2359 - acc: 0.9489 - val_loss: 0.2302 - val_acc: 0.9499\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.2345 - acc: 0.9489 - val_loss: 0.2288 - val_acc: 0.9499\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.2331 - acc: 0.9489 - val_loss: 0.2275 - val_acc: 0.9499\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.2318 - acc: 0.9489 - val_loss: 0.2262 - val_acc: 0.9499\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.2306 - acc: 0.9489 - val_loss: 0.2250 - val_acc: 0.9499\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.2294 - acc: 0.9489 - val_loss: 0.2238 - val_acc: 0.9499\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.2283 - acc: 0.9489 - val_loss: 0.2227 - val_acc: 0.9499\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.2272 - acc: 0.9489 - val_loss: 0.2216 - val_acc: 0.9499\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2262 - acc: 0.9489 - val_loss: 0.2207 - val_acc: 0.9499\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2252 - acc: 0.9489 - val_loss: 0.2197 - val_acc: 0.9499\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2243 - acc: 0.9489 - val_loss: 0.2188 - val_acc: 0.9499\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2234 - acc: 0.9489 - val_loss: 0.2178 - val_acc: 0.9499\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.2225 - acc: 0.9489 - val_loss: 0.2170 - val_acc: 0.9499\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.2216 - acc: 0.9489 - val_loss: 0.2162 - val_acc: 0.9499\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2208 - acc: 0.9489 - val_loss: 0.2154 - val_acc: 0.9499\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.2200 - acc: 0.9489 - val_loss: 0.2146 - val_acc: 0.9499\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2193 - acc: 0.9489 - val_loss: 0.2139 - val_acc: 0.9499\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2185 - acc: 0.9489 - val_loss: 0.2132 - val_acc: 0.9499\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.2178 - acc: 0.9489 - val_loss: 0.2125 - val_acc: 0.9499\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2171 - acc: 0.9489 - val_loss: 0.2118 - val_acc: 0.9499\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.2165 - acc: 0.9489 - val_loss: 0.2111 - val_acc: 0.9499\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2158 - acc: 0.9489 - val_loss: 0.2105 - val_acc: 0.9499\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2152 - acc: 0.9489 - val_loss: 0.2099 - val_acc: 0.9499\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2146 - acc: 0.9489 - val_loss: 0.2093 - val_acc: 0.9499\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.2140 - acc: 0.9489 - val_loss: 0.2087 - val_acc: 0.9499\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.2134 - acc: 0.9489 - val_loss: 0.2082 - val_acc: 0.9499\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.2129 - acc: 0.9489 - val_loss: 0.2076 - val_acc: 0.9499\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.2124 - acc: 0.9489 - val_loss: 0.2071 - val_acc: 0.9499\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2118 - acc: 0.9489 - val_loss: 0.2066 - val_acc: 0.9499\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2113 - acc: 0.9489 - val_loss: 0.2061 - val_acc: 0.9499\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2108 - acc: 0.9490 - val_loss: 0.2056 - val_acc: 0.9499\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.2103 - acc: 0.9490 - val_loss: 0.2051 - val_acc: 0.9499\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.2099 - acc: 0.9490 - val_loss: 0.2047 - val_acc: 0.9499\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2094 - acc: 0.9490 - val_loss: 0.2042 - val_acc: 0.9499\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.2090 - acc: 0.9490 - val_loss: 0.2038 - val_acc: 0.9499\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.2085 - acc: 0.9490 - val_loss: 0.2034 - val_acc: 0.9499\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.2081 - acc: 0.9490 - val_loss: 0.2029 - val_acc: 0.9499\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.2077 - acc: 0.9490 - val_loss: 0.2026 - val_acc: 0.9499\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2073 - acc: 0.9490 - val_loss: 0.2021 - val_acc: 0.9499\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2069 - acc: 0.9490 - val_loss: 0.2017 - val_acc: 0.9499\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2065 - acc: 0.9490 - val_loss: 0.2014 - val_acc: 0.9499\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2061 - acc: 0.9490 - val_loss: 0.2010 - val_acc: 0.9499\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.2057 - acc: 0.9490 - val_loss: 0.2006 - val_acc: 0.9499\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2054 - acc: 0.9490 - val_loss: 0.2003 - val_acc: 0.9499\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2050 - acc: 0.9490 - val_loss: 0.1999 - val_acc: 0.9499\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2047 - acc: 0.9490 - val_loss: 0.1996 - val_acc: 0.9499\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.2043 - acc: 0.9490 - val_loss: 0.1993 - val_acc: 0.9499\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2040 - acc: 0.9490 - val_loss: 0.1989 - val_acc: 0.9499\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2037 - acc: 0.9490 - val_loss: 0.1986 - val_acc: 0.9499\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.2033 - acc: 0.9490 - val_loss: 0.1983 - val_acc: 0.9499\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.2030 - acc: 0.9490 - val_loss: 0.1980 - val_acc: 0.9499\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2027 - acc: 0.9490 - val_loss: 0.1977 - val_acc: 0.9499\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.2024 - acc: 0.9490 - val_loss: 0.1974 - val_acc: 0.9499\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.2021 - acc: 0.9490 - val_loss: 0.1971 - val_acc: 0.9499\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.2018 - acc: 0.9490 - val_loss: 0.1968 - val_acc: 0.9499\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.2015 - acc: 0.9490 - val_loss: 0.1966 - val_acc: 0.9499\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.2012 - acc: 0.9490 - val_loss: 0.1963 - val_acc: 0.9499\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.2010 - acc: 0.9490 - val_loss: 0.1960 - val_acc: 0.9499\n",
      "\n",
      "Accuracy: 94.99%\n",
      "\n",
      "50381\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 371us/step - loss: 0.4064 - acc: 0.8831 - val_loss: 0.2072 - val_acc: 0.9499\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1963 - acc: 0.9491 - val_loss: 0.1810 - val_acc: 0.9505\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.1770 - acc: 0.9508 - val_loss: 0.1688 - val_acc: 0.9519\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.1658 - acc: 0.9515 - val_loss: 0.1606 - val_acc: 0.9526\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 121us/step - loss: 0.1570 - acc: 0.9521 - val_loss: 0.1549 - val_acc: 0.9538\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1499 - acc: 0.9529 - val_loss: 0.1497 - val_acc: 0.9542\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1445 - acc: 0.9537 - val_loss: 0.1460 - val_acc: 0.9540\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1401 - acc: 0.9540 - val_loss: 0.1429 - val_acc: 0.9542\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1360 - acc: 0.9552 - val_loss: 0.1403 - val_acc: 0.9550\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.1327 - acc: 0.9555 - val_loss: 0.1381 - val_acc: 0.9544\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.1297 - acc: 0.9560 - val_loss: 0.1363 - val_acc: 0.9548\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.1272 - acc: 0.9566 - val_loss: 0.1340 - val_acc: 0.9554\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1249 - acc: 0.9571 - val_loss: 0.1326 - val_acc: 0.9558\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1228 - acc: 0.9578 - val_loss: 0.1310 - val_acc: 0.9556\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1208 - acc: 0.9581 - val_loss: 0.1297 - val_acc: 0.9560\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1191 - acc: 0.9587 - val_loss: 0.1287 - val_acc: 0.9559\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1178 - acc: 0.9586 - val_loss: 0.1276 - val_acc: 0.9565\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.1163 - acc: 0.9594 - val_loss: 0.1261 - val_acc: 0.9564\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.1150 - acc: 0.9597 - val_loss: 0.1256 - val_acc: 0.9563\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1137 - acc: 0.9601 - val_loss: 0.1244 - val_acc: 0.9567\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1126 - acc: 0.9603 - val_loss: 0.1239 - val_acc: 0.9563\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1116 - acc: 0.9605 - val_loss: 0.1232 - val_acc: 0.9565\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1106 - acc: 0.9606 - val_loss: 0.1223 - val_acc: 0.9565\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1097 - acc: 0.9609 - val_loss: 0.1215 - val_acc: 0.9568\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1088 - acc: 0.9610 - val_loss: 0.1208 - val_acc: 0.9569\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1080 - acc: 0.9614 - val_loss: 0.1209 - val_acc: 0.9567\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1072 - acc: 0.9616 - val_loss: 0.1197 - val_acc: 0.9570\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1064 - acc: 0.9615 - val_loss: 0.1195 - val_acc: 0.9570\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.1058 - acc: 0.9619 - val_loss: 0.1190 - val_acc: 0.9571\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.1051 - acc: 0.9619 - val_loss: 0.1184 - val_acc: 0.9570\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1045 - acc: 0.9621 - val_loss: 0.1181 - val_acc: 0.9572\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1038 - acc: 0.9623 - val_loss: 0.1174 - val_acc: 0.9576\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1033 - acc: 0.9624 - val_loss: 0.1169 - val_acc: 0.9574\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1027 - acc: 0.9625 - val_loss: 0.1169 - val_acc: 0.9572\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 189us/step - loss: 0.1021 - acc: 0.9629 - val_loss: 0.1165 - val_acc: 0.9576\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.1016 - acc: 0.9627 - val_loss: 0.1161 - val_acc: 0.9574\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.1012 - acc: 0.9629 - val_loss: 0.1156 - val_acc: 0.9577\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1006 - acc: 0.9633 - val_loss: 0.1154 - val_acc: 0.9579\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1002 - acc: 0.9635 - val_loss: 0.1150 - val_acc: 0.9575\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0998 - acc: 0.9636 - val_loss: 0.1151 - val_acc: 0.9578\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0993 - acc: 0.9636 - val_loss: 0.1145 - val_acc: 0.9576\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0989 - acc: 0.9636 - val_loss: 0.1145 - val_acc: 0.9579\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0985 - acc: 0.9639 - val_loss: 0.1140 - val_acc: 0.9581\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0981 - acc: 0.9641 - val_loss: 0.1137 - val_acc: 0.9582\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0977 - acc: 0.9642 - val_loss: 0.1136 - val_acc: 0.9586\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0974 - acc: 0.9643 - val_loss: 0.1134 - val_acc: 0.9584\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.0970 - acc: 0.964 - 0s 67us/step - loss: 0.0970 - acc: 0.9646 - val_loss: 0.1131 - val_acc: 0.9584\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0966 - acc: 0.9646 - val_loss: 0.1128 - val_acc: 0.9588\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0962 - acc: 0.9647 - val_loss: 0.1128 - val_acc: 0.9588\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0959 - acc: 0.9647 - val_loss: 0.1125 - val_acc: 0.9586\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0956 - acc: 0.9649 - val_loss: 0.1124 - val_acc: 0.9583\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0953 - acc: 0.9650 - val_loss: 0.1124 - val_acc: 0.9587\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0950 - acc: 0.9652 - val_loss: 0.1120 - val_acc: 0.9584\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0947 - acc: 0.9650 - val_loss: 0.1119 - val_acc: 0.9591\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0944 - acc: 0.9653 - val_loss: 0.1117 - val_acc: 0.9588\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0942 - acc: 0.9654 - val_loss: 0.1115 - val_acc: 0.9591\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0938 - acc: 0.9654 - val_loss: 0.1113 - val_acc: 0.9588\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0936 - acc: 0.9653 - val_loss: 0.1112 - val_acc: 0.9590\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0933 - acc: 0.9658 - val_loss: 0.1111 - val_acc: 0.9591\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0930 - acc: 0.9658 - val_loss: 0.1110 - val_acc: 0.9591\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0928 - acc: 0.9658 - val_loss: 0.1107 - val_acc: 0.9592\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0926 - acc: 0.9659 - val_loss: 0.1107 - val_acc: 0.9595\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0923 - acc: 0.9661 - val_loss: 0.1105 - val_acc: 0.9592\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0920 - acc: 0.9662 - val_loss: 0.1103 - val_acc: 0.9593\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0918 - acc: 0.9663 - val_loss: 0.1101 - val_acc: 0.9593\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0916 - acc: 0.9663 - val_loss: 0.1102 - val_acc: 0.9594\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0913 - acc: 0.9664 - val_loss: 0.1098 - val_acc: 0.9592\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0912 - acc: 0.9663 - val_loss: 0.1097 - val_acc: 0.9595\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0909 - acc: 0.9666 - val_loss: 0.1097 - val_acc: 0.9594\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 1s 375us/step - loss: 0.0907 - acc: 0.9666 - val_loss: 0.1094 - val_acc: 0.9595\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0905 - acc: 0.9667 - val_loss: 0.1095 - val_acc: 0.9595\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0903 - acc: 0.9668 - val_loss: 0.1092 - val_acc: 0.9593\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0901 - acc: 0.9668 - val_loss: 0.1092 - val_acc: 0.9593\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0899 - acc: 0.9667 - val_loss: 0.1090 - val_acc: 0.9595\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0897 - acc: 0.9670 - val_loss: 0.1090 - val_acc: 0.9595\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0895 - acc: 0.9668 - val_loss: 0.1088 - val_acc: 0.9594\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 168us/step - loss: 0.0893 - acc: 0.9670 - val_loss: 0.1087 - val_acc: 0.9599\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 1s 192us/step - loss: 0.0892 - acc: 0.9672 - val_loss: 0.1087 - val_acc: 0.9594\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0890 - acc: 0.9672 - val_loss: 0.1085 - val_acc: 0.9596\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0888 - acc: 0.9673 - val_loss: 0.1085 - val_acc: 0.9595\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0886 - acc: 0.9671 - val_loss: 0.1082 - val_acc: 0.9597\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.0885 - acc: 0.9674 - val_loss: 0.1084 - val_acc: 0.9595\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0883 - acc: 0.9673 - val_loss: 0.1081 - val_acc: 0.9597\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 121us/step - loss: 0.0881 - acc: 0.9674 - val_loss: 0.1080 - val_acc: 0.9599\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0879 - acc: 0.9674 - val_loss: 0.1078 - val_acc: 0.9601\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0878 - acc: 0.9676 - val_loss: 0.1078 - val_acc: 0.9600\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0876 - acc: 0.9676 - val_loss: 0.1076 - val_acc: 0.9601\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0874 - acc: 0.9677 - val_loss: 0.1077 - val_acc: 0.9600\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0873 - acc: 0.9679 - val_loss: 0.1077 - val_acc: 0.9599\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0872 - acc: 0.9679 - val_loss: 0.1075 - val_acc: 0.9601\n",
      "\n",
      "Accuracy: 96.01%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 230us/step - loss: 0.4341 - acc: 0.7950 - val_loss: 0.1643 - val_acc: 0.9463\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1507 - acc: 0.9479 - val_loss: 0.1373 - val_acc: 0.9502\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1280 - acc: 0.9526 - val_loss: 0.1273 - val_acc: 0.9529\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1179 - acc: 0.9559 - val_loss: 0.1196 - val_acc: 0.9542\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1098 - acc: 0.9586 - val_loss: 0.1166 - val_acc: 0.9545\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1030 - acc: 0.9607 - val_loss: 0.1127 - val_acc: 0.9560\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0971 - acc: 0.9630 - val_loss: 0.1086 - val_acc: 0.9573\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0913 - acc: 0.9655 - val_loss: 0.1066 - val_acc: 0.9581\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0865 - acc: 0.9672 - val_loss: 0.1032 - val_acc: 0.9590\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0814 - acc: 0.9695 - val_loss: 0.1024 - val_acc: 0.9593\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0773 - acc: 0.9713 - val_loss: 0.0996 - val_acc: 0.9602\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0725 - acc: 0.9733 - val_loss: 0.0987 - val_acc: 0.9620\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0685 - acc: 0.9752 - val_loss: 0.0969 - val_acc: 0.9634\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0655 - acc: 0.9764 - val_loss: 0.0947 - val_acc: 0.9636\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0616 - acc: 0.9779 - val_loss: 0.0941 - val_acc: 0.9646\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0577 - acc: 0.9796 - val_loss: 0.0934 - val_acc: 0.9647\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0545 - acc: 0.9813 - val_loss: 0.0916 - val_acc: 0.9653\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0516 - acc: 0.9826 - val_loss: 0.0914 - val_acc: 0.9671\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0485 - acc: 0.9839 - val_loss: 0.0900 - val_acc: 0.9679\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0458 - acc: 0.9846 - val_loss: 0.0898 - val_acc: 0.9673\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0429 - acc: 0.9862 - val_loss: 0.0915 - val_acc: 0.9678\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0411 - acc: 0.9868 - val_loss: 0.0890 - val_acc: 0.9691\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0388 - acc: 0.9878 - val_loss: 0.0898 - val_acc: 0.9701\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0890 - val_acc: 0.9709\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0343 - acc: 0.9893 - val_loss: 0.0895 - val_acc: 0.9697\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0321 - acc: 0.9905 - val_loss: 0.0907 - val_acc: 0.9710\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0309 - acc: 0.9911 - val_loss: 0.0886 - val_acc: 0.9712\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0291 - acc: 0.9918 - val_loss: 0.0894 - val_acc: 0.9712\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 0.0877 - val_acc: 0.9733\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0255 - acc: 0.9930 - val_loss: 0.0895 - val_acc: 0.9725\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0246 - acc: 0.9933 - val_loss: 0.0908 - val_acc: 0.9730\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0232 - acc: 0.9942 - val_loss: 0.0897 - val_acc: 0.9740\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0218 - acc: 0.9944 - val_loss: 0.0917 - val_acc: 0.9729\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0205 - acc: 0.9952 - val_loss: 0.0894 - val_acc: 0.9740\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0201 - acc: 0.9952 - val_loss: 0.0906 - val_acc: 0.9730\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0920 - val_acc: 0.9735\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0181 - acc: 0.9961 - val_loss: 0.0922 - val_acc: 0.9744\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0170 - acc: 0.9964 - val_loss: 0.0935 - val_acc: 0.9740\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0947 - val_acc: 0.9736\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0154 - acc: 0.9970 - val_loss: 0.0931 - val_acc: 0.9741\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0144 - acc: 0.9973 - val_loss: 0.0953 - val_acc: 0.9746\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0142 - acc: 0.9973 - val_loss: 0.0945 - val_acc: 0.9749\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0136 - acc: 0.9975 - val_loss: 0.0951 - val_acc: 0.9748\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0127 - acc: 0.9977 - val_loss: 0.0960 - val_acc: 0.9744\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0124 - acc: 0.9979 - val_loss: 0.0973 - val_acc: 0.9753\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0116 - acc: 0.9982 - val_loss: 0.0982 - val_acc: 0.9751\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0115 - acc: 0.9982 - val_loss: 0.0956 - val_acc: 0.9756\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0108 - acc: 0.9981 - val_loss: 0.0976 - val_acc: 0.9749\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0104 - acc: 0.9983 - val_loss: 0.0969 - val_acc: 0.9749\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0100 - acc: 0.9985 - val_loss: 0.0998 - val_acc: 0.9747\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0098 - acc: 0.9985 - val_loss: 0.0990 - val_acc: 0.9746\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0097 - acc: 0.9985 - val_loss: 0.1006 - val_acc: 0.9746\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0098 - acc: 0.9983 - val_loss: 0.0998 - val_acc: 0.9744\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0087 - acc: 0.9987 - val_loss: 0.1006 - val_acc: 0.9753\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0082 - acc: 0.9988 - val_loss: 0.1007 - val_acc: 0.9751\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0078 - acc: 0.9989 - val_loss: 0.1019 - val_acc: 0.9747\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0078 - acc: 0.9989 - val_loss: 0.1019 - val_acc: 0.9747\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 0.9989 - val_loss: 0.1040 - val_acc: 0.9750\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0071 - acc: 0.9991 - val_loss: 0.1026 - val_acc: 0.9752\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.1024 - val_acc: 0.9762\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.1051 - val_acc: 0.9753\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.1062 - val_acc: 0.9753\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0059 - acc: 0.9994 - val_loss: 0.1056 - val_acc: 0.9753\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0061 - acc: 0.9993 - val_loss: 0.1054 - val_acc: 0.9757\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0057 - acc: 0.9994 - val_loss: 0.1078 - val_acc: 0.9749\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.1067 - val_acc: 0.9746\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0054 - acc: 0.9994 - val_loss: 0.1069 - val_acc: 0.9753\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0062 - acc: 0.9990 - val_loss: 0.1116 - val_acc: 0.9747\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.1079 - val_acc: 0.9752\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0050 - acc: 0.9995 - val_loss: 0.1088 - val_acc: 0.9749\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 0.1072 - val_acc: 0.9754\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.1084 - val_acc: 0.9753\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.1097 - val_acc: 0.9754\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0043 - acc: 0.9997 - val_loss: 0.1082 - val_acc: 0.9751\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.1107 - val_acc: 0.9744\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.1133 - val_acc: 0.9756\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.1123 - val_acc: 0.9750\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.1114 - val_acc: 0.9748\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.1123 - val_acc: 0.9744\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0042 - acc: 0.9996 - val_loss: 0.1113 - val_acc: 0.9751\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0037 - acc: 0.9997 - val_loss: 0.1125 - val_acc: 0.9755\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 0.1130 - val_acc: 0.9753\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 0.1134 - val_acc: 0.9746\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 0.1136 - val_acc: 0.9753\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.1120 - val_acc: 0.9753\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1139 - val_acc: 0.9751\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0030 - acc: 0.9999 - val_loss: 0.1139 - val_acc: 0.9752\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.1141 - val_acc: 0.9752\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.1147 - val_acc: 0.9751\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1162 - val_acc: 0.9753\n",
      "\n",
      "Accuracy: 97.53%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 425us/step - loss: 0.2506 - acc: 0.9302 - val_loss: 0.1616 - val_acc: 0.9521\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 389us/step - loss: 0.1510 - acc: 0.9517 - val_loss: 0.1424 - val_acc: 0.9523\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1312 - acc: 0.9544 - val_loss: 0.1304 - val_acc: 0.9535\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1191 - acc: 0.9570 - val_loss: 0.1234 - val_acc: 0.9542\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.1118 - acc: 0.9586 - val_loss: 0.1191 - val_acc: 0.9551\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 174us/step - loss: 0.1049 - acc: 0.9611 - val_loss: 0.1178 - val_acc: 0.9568\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0995 - acc: 0.9625 - val_loss: 0.1113 - val_acc: 0.9581\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 0.0951 - acc: 0.9645 - val_loss: 0.1090 - val_acc: 0.9583\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.0906 - acc: 0.9660 - val_loss: 0.1087 - val_acc: 0.9600\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0875 - acc: 0.9670 - val_loss: 0.1063 - val_acc: 0.9592\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0844 - acc: 0.9680 - val_loss: 0.1039 - val_acc: 0.9607\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.0818 - acc: 0.9688 - val_loss: 0.1035 - val_acc: 0.9605\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0794 - acc: 0.9698 - val_loss: 0.1024 - val_acc: 0.9604\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0771 - acc: 0.9704 - val_loss: 0.1012 - val_acc: 0.9614\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0752 - acc: 0.9720 - val_loss: 0.1007 - val_acc: 0.9622\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0733 - acc: 0.9725 - val_loss: 0.1007 - val_acc: 0.9627\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0715 - acc: 0.9736 - val_loss: 0.0994 - val_acc: 0.9635\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0699 - acc: 0.9742 - val_loss: 0.0987 - val_acc: 0.9643\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.0685 - acc: 0.9744 - val_loss: 0.0982 - val_acc: 0.9633\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0673 - acc: 0.9750 - val_loss: 0.0981 - val_acc: 0.9639\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0659 - acc: 0.9755 - val_loss: 0.0986 - val_acc: 0.9640\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0649 - acc: 0.9761 - val_loss: 0.0974 - val_acc: 0.9646\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0637 - acc: 0.9766 - val_loss: 0.0976 - val_acc: 0.9652\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.0625 - acc: 0.9769 - val_loss: 0.0972 - val_acc: 0.9649\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.0617 - acc: 0.9771 - val_loss: 0.0967 - val_acc: 0.9646\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0607 - acc: 0.9776 - val_loss: 0.0966 - val_acc: 0.9659\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0598 - acc: 0.9778 - val_loss: 0.0970 - val_acc: 0.9656\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0589 - acc: 0.9784 - val_loss: 0.0975 - val_acc: 0.9652\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0581 - acc: 0.9789 - val_loss: 0.0968 - val_acc: 0.9652\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0576 - acc: 0.9788 - val_loss: 0.0969 - val_acc: 0.9658\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0565 - acc: 0.9796 - val_loss: 0.0961 - val_acc: 0.9653\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.0559 - acc: 0.9796 - val_loss: 0.0955 - val_acc: 0.9664\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0552 - acc: 0.9801 - val_loss: 0.0955 - val_acc: 0.9664\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0544 - acc: 0.9806 - val_loss: 0.0957 - val_acc: 0.9659\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0540 - acc: 0.9805 - val_loss: 0.0960 - val_acc: 0.9662\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.0533 - acc: 0.9807 - val_loss: 0.0965 - val_acc: 0.9666\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.0527 - acc: 0.9813 - val_loss: 0.0963 - val_acc: 0.9657\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0523 - acc: 0.9812 - val_loss: 0.0958 - val_acc: 0.9670\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.0517 - acc: 0.9816 - val_loss: 0.0963 - val_acc: 0.9665\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.0511 - acc: 0.9819 - val_loss: 0.0956 - val_acc: 0.9669\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 121us/step - loss: 0.0506 - acc: 0.9819 - val_loss: 0.0958 - val_acc: 0.9665\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.0502 - acc: 0.9821 - val_loss: 0.0951 - val_acc: 0.9673\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0496 - acc: 0.9827 - val_loss: 0.0960 - val_acc: 0.9669\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0492 - acc: 0.9828 - val_loss: 0.0962 - val_acc: 0.9669\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.0488 - acc: 0.9829 - val_loss: 0.0958 - val_acc: 0.9666\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0482 - acc: 0.9828 - val_loss: 0.0969 - val_acc: 0.9666\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0480 - acc: 0.9830 - val_loss: 0.0957 - val_acc: 0.9677\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0476 - acc: 0.9831 - val_loss: 0.0956 - val_acc: 0.9675\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.0471 - acc: 0.9835 - val_loss: 0.0958 - val_acc: 0.9675\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.0466 - acc: 0.9837 - val_loss: 0.0963 - val_acc: 0.9676\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0462 - acc: 0.9837 - val_loss: 0.0965 - val_acc: 0.9674\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0460 - acc: 0.9842 - val_loss: 0.0966 - val_acc: 0.9674\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 1s 375us/step - loss: 0.0457 - acc: 0.9840 - val_loss: 0.0957 - val_acc: 0.9681\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.0451 - acc: 0.9845 - val_loss: 0.0954 - val_acc: 0.9678\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0448 - acc: 0.9846 - val_loss: 0.0963 - val_acc: 0.9680\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.0445 - acc: 0.9848 - val_loss: 0.0967 - val_acc: 0.9677\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0442 - acc: 0.9848 - val_loss: 0.0958 - val_acc: 0.9679\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.0438 - acc: 0.9850 - val_loss: 0.0959 - val_acc: 0.9684\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 173us/step - loss: 0.0436 - acc: 0.9849 - val_loss: 0.0953 - val_acc: 0.9684\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0433 - acc: 0.9853 - val_loss: 0.0956 - val_acc: 0.9685\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0430 - acc: 0.9853 - val_loss: 0.0958 - val_acc: 0.9686\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.0427 - acc: 0.9854 - val_loss: 0.0955 - val_acc: 0.9688\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0423 - acc: 0.9858 - val_loss: 0.0963 - val_acc: 0.9683\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.0420 - acc: 0.9860 - val_loss: 0.0963 - val_acc: 0.9687\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.0418 - acc: 0.9861 - val_loss: 0.0965 - val_acc: 0.9684\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0415 - acc: 0.9861 - val_loss: 0.0959 - val_acc: 0.9689\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0413 - acc: 0.9860 - val_loss: 0.0960 - val_acc: 0.9684\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.0410 - acc: 0.9865 - val_loss: 0.0959 - val_acc: 0.9686\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0407 - acc: 0.9865 - val_loss: 0.0961 - val_acc: 0.9686\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0404 - acc: 0.9866 - val_loss: 0.0960 - val_acc: 0.9685\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.0402 - acc: 0.9868 - val_loss: 0.0968 - val_acc: 0.9678\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0399 - acc: 0.9867 - val_loss: 0.0959 - val_acc: 0.9686\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0397 - acc: 0.9869 - val_loss: 0.0971 - val_acc: 0.9685\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0395 - acc: 0.9869 - val_loss: 0.0967 - val_acc: 0.9688\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0394 - acc: 0.9870 - val_loss: 0.0962 - val_acc: 0.9685\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0391 - acc: 0.9873 - val_loss: 0.0965 - val_acc: 0.9688\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0387 - acc: 0.9874 - val_loss: 0.0969 - val_acc: 0.9683\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0386 - acc: 0.9875 - val_loss: 0.0964 - val_acc: 0.9685\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.0383 - acc: 0.9875 - val_loss: 0.0968 - val_acc: 0.9689\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.0381 - acc: 0.9876 - val_loss: 0.0969 - val_acc: 0.9689\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0380 - acc: 0.9876 - val_loss: 0.0968 - val_acc: 0.9687\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0377 - acc: 0.9879 - val_loss: 0.0965 - val_acc: 0.9691\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0375 - acc: 0.9880 - val_loss: 0.0967 - val_acc: 0.9691\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0373 - acc: 0.9879 - val_loss: 0.0971 - val_acc: 0.9683\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.0963 - val_acc: 0.9692\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0369 - acc: 0.9880 - val_loss: 0.0968 - val_acc: 0.9691\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 135us/step - loss: 0.0367 - acc: 0.9884 - val_loss: 0.0975 - val_acc: 0.9687\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.0365 - acc: 0.9884 - val_loss: 0.0973 - val_acc: 0.9687\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0363 - acc: 0.9884 - val_loss: 0.0974 - val_acc: 0.9693\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0361 - acc: 0.9885 - val_loss: 0.0970 - val_acc: 0.9692\n",
      "\n",
      "Accuracy: 96.92%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 340us/step - loss: 0.3933 - acc: 0.8618 - val_loss: 0.2115 - val_acc: 0.9495\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1946 - acc: 0.9494 - val_loss: 0.1751 - val_acc: 0.9522\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1694 - acc: 0.9505 - val_loss: 0.1609 - val_acc: 0.9515\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1555 - acc: 0.9516 - val_loss: 0.1511 - val_acc: 0.9519\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1450 - acc: 0.9527 - val_loss: 0.1444 - val_acc: 0.9522\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1366 - acc: 0.9537 - val_loss: 0.1378 - val_acc: 0.9524\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1297 - acc: 0.9550 - val_loss: 0.1332 - val_acc: 0.9532\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1245 - acc: 0.9562 - val_loss: 0.1309 - val_acc: 0.9538\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1200 - acc: 0.9574 - val_loss: 0.1275 - val_acc: 0.9544\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 334us/step - loss: 0.1161 - acc: 0.9585 - val_loss: 0.1259 - val_acc: 0.9550\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.1130 - acc: 0.9592 - val_loss: 0.1242 - val_acc: 0.9545\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.1100 - acc: 0.9603 - val_loss: 0.1230 - val_acc: 0.9554\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1071 - acc: 0.9609 - val_loss: 0.1215 - val_acc: 0.9557\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1050 - acc: 0.9616 - val_loss: 0.1200 - val_acc: 0.9564\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1028 - acc: 0.9625 - val_loss: 0.1188 - val_acc: 0.9567\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.1006 - acc: 0.9627 - val_loss: 0.1181 - val_acc: 0.9576\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0990 - acc: 0.9634 - val_loss: 0.1167 - val_acc: 0.9579\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0970 - acc: 0.9640 - val_loss: 0.1161 - val_acc: 0.9581\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0956 - acc: 0.9646 - val_loss: 0.1148 - val_acc: 0.9579\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0940 - acc: 0.9651 - val_loss: 0.1149 - val_acc: 0.9582\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0925 - acc: 0.9656 - val_loss: 0.1138 - val_acc: 0.9583\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0911 - acc: 0.9663 - val_loss: 0.1135 - val_acc: 0.9577\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0899 - acc: 0.9666 - val_loss: 0.1129 - val_acc: 0.9593\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0886 - acc: 0.9670 - val_loss: 0.1126 - val_acc: 0.9590\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0874 - acc: 0.9679 - val_loss: 0.1122 - val_acc: 0.9591\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0863 - acc: 0.9678 - val_loss: 0.1118 - val_acc: 0.9593\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0854 - acc: 0.9685 - val_loss: 0.1111 - val_acc: 0.9597\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0842 - acc: 0.9690 - val_loss: 0.1110 - val_acc: 0.9606\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0833 - acc: 0.9693 - val_loss: 0.1104 - val_acc: 0.9605\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0825 - acc: 0.9694 - val_loss: 0.1106 - val_acc: 0.9604\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0816 - acc: 0.9699 - val_loss: 0.1103 - val_acc: 0.9606\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0809 - acc: 0.9702 - val_loss: 0.1093 - val_acc: 0.9602\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0801 - acc: 0.9707 - val_loss: 0.1096 - val_acc: 0.9606\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0792 - acc: 0.9711 - val_loss: 0.1091 - val_acc: 0.9606\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0785 - acc: 0.9710 - val_loss: 0.1088 - val_acc: 0.9613\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0778 - acc: 0.9718 - val_loss: 0.1084 - val_acc: 0.9611\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0771 - acc: 0.9718 - val_loss: 0.1082 - val_acc: 0.9610\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0764 - acc: 0.9720 - val_loss: 0.1077 - val_acc: 0.9611\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0757 - acc: 0.9724 - val_loss: 0.1075 - val_acc: 0.9620\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0751 - acc: 0.9725 - val_loss: 0.1079 - val_acc: 0.9622\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0745 - acc: 0.9729 - val_loss: 0.1072 - val_acc: 0.9622\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0740 - acc: 0.9730 - val_loss: 0.1068 - val_acc: 0.9625\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0733 - acc: 0.9732 - val_loss: 0.1065 - val_acc: 0.9622\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0728 - acc: 0.9737 - val_loss: 0.1064 - val_acc: 0.9629\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0722 - acc: 0.9738 - val_loss: 0.1063 - val_acc: 0.9627\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0718 - acc: 0.9739 - val_loss: 0.1066 - val_acc: 0.9633\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0712 - acc: 0.9743 - val_loss: 0.1063 - val_acc: 0.9632\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0707 - acc: 0.9743 - val_loss: 0.1058 - val_acc: 0.9635\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0703 - acc: 0.9746 - val_loss: 0.1056 - val_acc: 0.9643\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0699 - acc: 0.9748 - val_loss: 0.1058 - val_acc: 0.9639\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0694 - acc: 0.9749 - val_loss: 0.1055 - val_acc: 0.9631\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0689 - acc: 0.9750 - val_loss: 0.1055 - val_acc: 0.9637\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0684 - acc: 0.9753 - val_loss: 0.1053 - val_acc: 0.9640\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0681 - acc: 0.9754 - val_loss: 0.1054 - val_acc: 0.9644\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 139us/step - loss: 0.0677 - acc: 0.9755 - val_loss: 0.1052 - val_acc: 0.9632\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0673 - acc: 0.9757 - val_loss: 0.1045 - val_acc: 0.9645\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.0669 - acc: 0.9759 - val_loss: 0.1046 - val_acc: 0.9639\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 143us/step - loss: 0.0665 - acc: 0.9762 - val_loss: 0.1052 - val_acc: 0.9638\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0662 - acc: 0.9759 - val_loss: 0.1044 - val_acc: 0.9642\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.0658 - acc: 0.9762 - val_loss: 0.1045 - val_acc: 0.9640\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0654 - acc: 0.9763 - val_loss: 0.1043 - val_acc: 0.9642\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0651 - acc: 0.9767 - val_loss: 0.1043 - val_acc: 0.9649\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0647 - acc: 0.9766 - val_loss: 0.1041 - val_acc: 0.9650\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0644 - acc: 0.9768 - val_loss: 0.1039 - val_acc: 0.9643\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0640 - acc: 0.9769 - val_loss: 0.1038 - val_acc: 0.9648\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0638 - acc: 0.9772 - val_loss: 0.1040 - val_acc: 0.9647\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0634 - acc: 0.9771 - val_loss: 0.1035 - val_acc: 0.9650\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0632 - acc: 0.9772 - val_loss: 0.1037 - val_acc: 0.9653\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0627 - acc: 0.9775 - val_loss: 0.1035 - val_acc: 0.9651\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0624 - acc: 0.9776 - val_loss: 0.1035 - val_acc: 0.9655\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0622 - acc: 0.9777 - val_loss: 0.1035 - val_acc: 0.9652\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0619 - acc: 0.9780 - val_loss: 0.1034 - val_acc: 0.9652\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0616 - acc: 0.9779 - val_loss: 0.1028 - val_acc: 0.9651\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0614 - acc: 0.9779 - val_loss: 0.1035 - val_acc: 0.9657\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0610 - acc: 0.9781 - val_loss: 0.1028 - val_acc: 0.9657\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0608 - acc: 0.9784 - val_loss: 0.1034 - val_acc: 0.9655\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0605 - acc: 0.9783 - val_loss: 0.1029 - val_acc: 0.9659\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0602 - acc: 0.9786 - val_loss: 0.1028 - val_acc: 0.9657\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0600 - acc: 0.9787 - val_loss: 0.1024 - val_acc: 0.9655\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0597 - acc: 0.9787 - val_loss: 0.1023 - val_acc: 0.9661\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0595 - acc: 0.9789 - val_loss: 0.1025 - val_acc: 0.9656\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0593 - acc: 0.9789 - val_loss: 0.1025 - val_acc: 0.9656\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.1024 - val_acc: 0.9661\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0588 - acc: 0.9792 - val_loss: 0.1026 - val_acc: 0.9662\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0586 - acc: 0.9790 - val_loss: 0.1023 - val_acc: 0.9657\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0584 - acc: 0.9793 - val_loss: 0.1026 - val_acc: 0.9655\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0581 - acc: 0.9793 - val_loss: 0.1023 - val_acc: 0.9662\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0579 - acc: 0.9793 - val_loss: 0.1024 - val_acc: 0.9658\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0577 - acc: 0.9795 - val_loss: 0.1018 - val_acc: 0.9663\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0574 - acc: 0.9796 - val_loss: 0.1022 - val_acc: 0.9662\n",
      "\n",
      "Accuracy: 96.62%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 347us/step - loss: 0.4079 - acc: 0.8023 - val_loss: 0.1543 - val_acc: 0.9490\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1455 - acc: 0.9500 - val_loss: 0.1332 - val_acc: 0.9517\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1282 - acc: 0.9531 - val_loss: 0.1263 - val_acc: 0.9535\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1206 - acc: 0.9549 - val_loss: 0.1215 - val_acc: 0.9545\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1136 - acc: 0.9571 - val_loss: 0.1186 - val_acc: 0.9559\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1077 - acc: 0.9598 - val_loss: 0.1143 - val_acc: 0.9566\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.1010 - acc: 0.9623 - val_loss: 0.1101 - val_acc: 0.9580\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0949 - acc: 0.9649 - val_loss: 0.1096 - val_acc: 0.9574\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0906 - acc: 0.9661 - val_loss: 0.1064 - val_acc: 0.9592\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0845 - acc: 0.9682 - val_loss: 0.1066 - val_acc: 0.9584\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0794 - acc: 0.9705 - val_loss: 0.1007 - val_acc: 0.9625\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0739 - acc: 0.9735 - val_loss: 0.0993 - val_acc: 0.9630\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0682 - acc: 0.9756 - val_loss: 0.0966 - val_acc: 0.9635\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0636 - acc: 0.9771 - val_loss: 0.0957 - val_acc: 0.9645\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0597 - acc: 0.9793 - val_loss: 0.0952 - val_acc: 0.9646\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0543 - acc: 0.9811 - val_loss: 0.0940 - val_acc: 0.9665\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0505 - acc: 0.9827 - val_loss: 0.0926 - val_acc: 0.9678\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0450 - acc: 0.9852 - val_loss: 0.0922 - val_acc: 0.9673\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0424 - acc: 0.9861 - val_loss: 0.0931 - val_acc: 0.9680\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0385 - acc: 0.9878 - val_loss: 0.0933 - val_acc: 0.9700\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0359 - acc: 0.9886 - val_loss: 0.0945 - val_acc: 0.9696\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0328 - acc: 0.9900 - val_loss: 0.0915 - val_acc: 0.9717\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0297 - acc: 0.9910 - val_loss: 0.0944 - val_acc: 0.9708\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0271 - acc: 0.9921 - val_loss: 0.0948 - val_acc: 0.9719\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.0946 - val_acc: 0.9737\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0217 - acc: 0.9941 - val_loss: 0.0947 - val_acc: 0.9734\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0192 - acc: 0.9954 - val_loss: 0.0974 - val_acc: 0.9732\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0171 - acc: 0.9961 - val_loss: 0.1005 - val_acc: 0.9737\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0992 - val_acc: 0.9738\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0137 - acc: 0.9973 - val_loss: 0.0992 - val_acc: 0.9755\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.1002 - val_acc: 0.9751\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0109 - acc: 0.9982 - val_loss: 0.1047 - val_acc: 0.9755\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0096 - acc: 0.9984 - val_loss: 0.1050 - val_acc: 0.9748\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0086 - acc: 0.9987 - val_loss: 0.1063 - val_acc: 0.9751\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0081 - acc: 0.9987 - val_loss: 0.1058 - val_acc: 0.9749\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.1097 - val_acc: 0.9750\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0064 - acc: 0.9990 - val_loss: 0.1088 - val_acc: 0.9753\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.1108 - val_acc: 0.9749\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0057 - acc: 0.9993 - val_loss: 0.1131 - val_acc: 0.9751\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.1139 - val_acc: 0.9756\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0044 - acc: 0.9996 - val_loss: 0.1198 - val_acc: 0.9756\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.1166 - val_acc: 0.9753\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0044 - acc: 0.9996 - val_loss: 0.1167 - val_acc: 0.9749\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0038 - acc: 0.9996 - val_loss: 0.1217 - val_acc: 0.9747\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.1215 - val_acc: 0.9751\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 0.1200 - val_acc: 0.9750\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.0035 - acc: 0.9995 - val_loss: 0.1215 - val_acc: 0.9756\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.1259 - val_acc: 0.9753\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0088 - acc: 0.9978 - val_loss: 0.1284 - val_acc: 0.9727\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1502 - val_acc: 0.9674\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.0374 - acc: 0.9868 - val_loss: 0.1309 - val_acc: 0.9665\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0325 - acc: 0.9880 - val_loss: 0.1263 - val_acc: 0.9704\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0214 - acc: 0.9924 - val_loss: 0.1225 - val_acc: 0.9722\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1181 - val_acc: 0.9735\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.1240 - val_acc: 0.9747\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0054 - acc: 0.9991 - val_loss: 0.1220 - val_acc: 0.9747\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.1254 - val_acc: 0.9758\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0027 - acc: 0.9998 - val_loss: 0.1233 - val_acc: 0.9763\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1241 - val_acc: 0.9760\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1256 - val_acc: 0.9758\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1252 - val_acc: 0.9763\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.1256 - val_acc: 0.9761\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.1267 - val_acc: 0.9760\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1276 - val_acc: 0.9761\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1282 - val_acc: 0.9753\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1301 - val_acc: 0.9763\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1311 - val_acc: 0.9758\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1309 - val_acc: 0.9760\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1329 - val_acc: 0.9752\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1307 - val_acc: 0.9763\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1319 - val_acc: 0.9762\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1316 - val_acc: 0.9754\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1327 - val_acc: 0.9762\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1333 - val_acc: 0.9765\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1312 - val_acc: 0.9753\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1327 - val_acc: 0.9762\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1335 - val_acc: 0.9766\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 9.3418e-04 - acc: 0.9999 - val_loss: 0.1355 - val_acc: 0.9759\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 7.4752e-04 - acc: 1.0000 - val_loss: 0.1369 - val_acc: 0.9758\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 9.0305e-04 - acc: 0.9999 - val_loss: 0.1369 - val_acc: 0.9756\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 8.2799e-04 - acc: 0.9999 - val_loss: 0.1366 - val_acc: 0.9762\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 7.4199e-04 - acc: 1.0000 - val_loss: 0.1373 - val_acc: 0.9763\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 8.0750e-04 - acc: 0.9999 - val_loss: 0.1385 - val_acc: 0.9760\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 9.1500e-04 - acc: 0.9999 - val_loss: 0.1400 - val_acc: 0.9760\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1373 - val_acc: 0.9759\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1398 - val_acc: 0.9763\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1392 - val_acc: 0.9760\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1416 - val_acc: 0.9751\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 1s 332us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1364 - val_acc: 0.9751\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1481 - val_acc: 0.9726\n",
      "\n",
      "Accuracy: 97.26%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 353us/step - loss: 0.3527 - acc: 0.8323 - val_loss: 0.1526 - val_acc: 0.9444\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1387 - acc: 0.9505 - val_loss: 0.1259 - val_acc: 0.9535\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.1195 - acc: 0.9553 - val_loss: 0.1211 - val_acc: 0.9534\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1109 - acc: 0.9581 - val_loss: 0.1150 - val_acc: 0.9544\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1025 - acc: 0.9604 - val_loss: 0.1135 - val_acc: 0.9566\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0962 - acc: 0.9628 - val_loss: 0.1047 - val_acc: 0.9594\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0845 - acc: 0.9679 - val_loss: 0.1024 - val_acc: 0.9610\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0773 - acc: 0.9704 - val_loss: 0.0982 - val_acc: 0.9630\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0734 - acc: 0.9719 - val_loss: 0.1023 - val_acc: 0.9611\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0669 - acc: 0.9752 - val_loss: 0.0956 - val_acc: 0.9639\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0607 - acc: 0.9784 - val_loss: 0.0978 - val_acc: 0.9632\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0575 - acc: 0.9791 - val_loss: 0.0948 - val_acc: 0.9662\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0547 - acc: 0.9805 - val_loss: 0.0929 - val_acc: 0.9664\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0488 - acc: 0.9826 - val_loss: 0.0904 - val_acc: 0.9694\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0436 - acc: 0.9853 - val_loss: 0.0893 - val_acc: 0.9705\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.0880 - val_acc: 0.9701\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0348 - acc: 0.9889 - val_loss: 0.0849 - val_acc: 0.9718\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0299 - acc: 0.9911 - val_loss: 0.0846 - val_acc: 0.9739\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0264 - acc: 0.9924 - val_loss: 0.0855 - val_acc: 0.9731\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0837 - val_acc: 0.9738\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0842 - val_acc: 0.9750\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.0881 - val_acc: 0.9742\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9942 - val_loss: 0.0873 - val_acc: 0.9743\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0201 - acc: 0.9946 - val_loss: 0.0862 - val_acc: 0.9760\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 0.0852 - val_acc: 0.9771\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9771\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0869 - val_acc: 0.9762\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0097 - acc: 0.9982 - val_loss: 0.0887 - val_acc: 0.9770\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0089 - acc: 0.9984 - val_loss: 0.0902 - val_acc: 0.9760\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0085 - acc: 0.9986 - val_loss: 0.0929 - val_acc: 0.9763\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0069 - acc: 0.9991 - val_loss: 0.0917 - val_acc: 0.9769\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0055 - acc: 0.9993 - val_loss: 0.0902 - val_acc: 0.9778\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0049 - acc: 0.9995 - val_loss: 0.0927 - val_acc: 0.9772\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0042 - acc: 0.9997 - val_loss: 0.0975 - val_acc: 0.9774\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0101 - acc: 0.9975 - val_loss: 0.0980 - val_acc: 0.9761\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0963 - val_acc: 0.9757\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0116 - acc: 0.9972 - val_loss: 0.1032 - val_acc: 0.9740\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0093 - acc: 0.9979 - val_loss: 0.1004 - val_acc: 0.9767\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.1022 - val_acc: 0.9758\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0071 - acc: 0.9986 - val_loss: 0.1004 - val_acc: 0.9767\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.1009 - val_acc: 0.9771\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.1024 - val_acc: 0.9778\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.1032 - val_acc: 0.9769\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1021 - val_acc: 0.9770\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.1014 - val_acc: 0.9772\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1065 - val_acc: 0.9772\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1024 - val_acc: 0.9769\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1065 - val_acc: 0.9760\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 0.1125 - val_acc: 0.9734\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.1128 - val_acc: 0.9745\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.1073 - val_acc: 0.9765\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.1090 - val_acc: 0.9761\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.1116 - val_acc: 0.9761\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0234 - acc: 0.9927 - val_loss: 0.1201 - val_acc: 0.9713\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0468 - acc: 0.9846 - val_loss: 0.2045 - val_acc: 0.9455\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0722 - acc: 0.9751 - val_loss: 0.1292 - val_acc: 0.9622\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0502 - acc: 0.9815 - val_loss: 0.1121 - val_acc: 0.9683\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0339 - acc: 0.9880 - val_loss: 0.1107 - val_acc: 0.9697\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.1038 - val_acc: 0.9749\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.1049 - val_acc: 0.9744\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0105 - acc: 0.9974 - val_loss: 0.1100 - val_acc: 0.9749\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1105 - val_acc: 0.9760\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1111 - val_acc: 0.9766\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.1128 - val_acc: 0.9771\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0032 - acc: 0.9996 - val_loss: 0.1115 - val_acc: 0.9770\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1141 - val_acc: 0.9778\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1139 - val_acc: 0.9770\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.1151 - val_acc: 0.9774\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1162 - val_acc: 0.9769\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.1170 - val_acc: 0.9776\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1183 - val_acc: 0.9767\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1177 - val_acc: 0.9761\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1170 - val_acc: 0.9776\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1179 - val_acc: 0.9769\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1184 - val_acc: 0.9776\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1174 - val_acc: 0.9775\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1174 - val_acc: 0.9775\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1175 - val_acc: 0.9779\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1213 - val_acc: 0.9774\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1196 - val_acc: 0.9783\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1216 - val_acc: 0.9776\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1211 - val_acc: 0.9778\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1223 - val_acc: 0.9777\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1218 - val_acc: 0.9777\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1212 - val_acc: 0.9773\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1226 - val_acc: 0.9776\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1236 - val_acc: 0.9776\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1236 - val_acc: 0.9775\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1248 - val_acc: 0.9752\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1240 - val_acc: 0.9774\n",
      "\n",
      "Accuracy: 97.74%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.3762 - acc: 0.8217 - val_loss: 0.1663 - val_acc: 0.9419\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1443 - acc: 0.9494 - val_loss: 0.1319 - val_acc: 0.9515\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1239 - acc: 0.9540 - val_loss: 0.1233 - val_acc: 0.9531\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1149 - acc: 0.9569 - val_loss: 0.1188 - val_acc: 0.9552\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1076 - acc: 0.9583 - val_loss: 0.1161 - val_acc: 0.9547\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1002 - acc: 0.9614 - val_loss: 0.1107 - val_acc: 0.9560\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0939 - acc: 0.9641 - val_loss: 0.1081 - val_acc: 0.9565\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0861 - acc: 0.9670 - val_loss: 0.1066 - val_acc: 0.9592\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0811 - acc: 0.9685 - val_loss: 0.1055 - val_acc: 0.9591\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0747 - acc: 0.9719 - val_loss: 0.1050 - val_acc: 0.9599\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0706 - acc: 0.9732 - val_loss: 0.0997 - val_acc: 0.9622\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0648 - acc: 0.9753 - val_loss: 0.0997 - val_acc: 0.9638\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0595 - acc: 0.9778 - val_loss: 0.0974 - val_acc: 0.9646\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0528 - acc: 0.9808 - val_loss: 0.0943 - val_acc: 0.9666\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0481 - acc: 0.9831 - val_loss: 0.0929 - val_acc: 0.9686\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0432 - acc: 0.9848 - val_loss: 0.0959 - val_acc: 0.9673\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0417 - acc: 0.9855 - val_loss: 0.1016 - val_acc: 0.9683\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0385 - acc: 0.9864 - val_loss: 0.0996 - val_acc: 0.9689\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0346 - acc: 0.9884 - val_loss: 0.1001 - val_acc: 0.9692\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0291 - acc: 0.9906 - val_loss: 0.0968 - val_acc: 0.9713\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0940 - val_acc: 0.9731\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0231 - acc: 0.9928 - val_loss: 0.1030 - val_acc: 0.9714\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0216 - acc: 0.9934 - val_loss: 0.1015 - val_acc: 0.9736\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.1069 - val_acc: 0.9708\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.1102 - val_acc: 0.9716\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0190 - acc: 0.9943 - val_loss: 0.1076 - val_acc: 0.9737\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0156 - acc: 0.9955 - val_loss: 0.1083 - val_acc: 0.9741\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0125 - acc: 0.9969 - val_loss: 0.1107 - val_acc: 0.9727\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0119 - acc: 0.9968 - val_loss: 0.1101 - val_acc: 0.9739\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0096 - acc: 0.9979 - val_loss: 0.1117 - val_acc: 0.9746\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.1119 - val_acc: 0.9753\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.1148 - val_acc: 0.9739\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0095 - acc: 0.9978 - val_loss: 0.1149 - val_acc: 0.9752\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0084 - acc: 0.9981 - val_loss: 0.1181 - val_acc: 0.9740\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0068 - acc: 0.9986 - val_loss: 0.1176 - val_acc: 0.9752\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1196 - val_acc: 0.9741\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.1185 - val_acc: 0.9751\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0053 - acc: 0.9990 - val_loss: 0.1236 - val_acc: 0.9746\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1219 - val_acc: 0.9760\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0038 - acc: 0.9995 - val_loss: 0.1220 - val_acc: 0.9756\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1279 - val_acc: 0.9754\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1279 - val_acc: 0.9744\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1269 - val_acc: 0.9755\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.1236 - val_acc: 0.9762\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1298 - val_acc: 0.9762\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1328 - val_acc: 0.9755\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1304 - val_acc: 0.9751\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0031 - acc: 0.9995 - val_loss: 0.1353 - val_acc: 0.9746\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1324 - val_acc: 0.9749\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1355 - val_acc: 0.9754\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1353 - val_acc: 0.9758\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1374 - val_acc: 0.9755\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1388 - val_acc: 0.9758\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1384 - val_acc: 0.9756\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1405 - val_acc: 0.9754\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1409 - val_acc: 0.9758\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1399 - val_acc: 0.9756\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1441 - val_acc: 0.9751\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1435 - val_acc: 0.9737\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1439 - val_acc: 0.9728\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.1571 - val_acc: 0.9700\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0222 - acc: 0.9918 - val_loss: 0.1602 - val_acc: 0.9688\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0245 - acc: 0.9908 - val_loss: 0.1419 - val_acc: 0.9707\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0231 - acc: 0.9912 - val_loss: 0.1489 - val_acc: 0.9698\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0184 - acc: 0.9933 - val_loss: 0.1380 - val_acc: 0.9722\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0142 - acc: 0.9954 - val_loss: 0.1361 - val_acc: 0.9749\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1354 - val_acc: 0.9756\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.1323 - val_acc: 0.9758\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1389 - val_acc: 0.9763\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1370 - val_acc: 0.9770\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1361 - val_acc: 0.9769\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1368 - val_acc: 0.9772\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1383 - val_acc: 0.9772\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1387 - val_acc: 0.9774\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.1401 - val_acc: 0.9760\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1491 - val_acc: 0.9747\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.1427 - val_acc: 0.9755\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1403 - val_acc: 0.9765\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1436 - val_acc: 0.9756\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1423 - val_acc: 0.9770\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 7.9379e-04 - acc: 0.9999 - val_loss: 0.1429 - val_acc: 0.9767\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 7.0041e-04 - acc: 0.9999 - val_loss: 0.1430 - val_acc: 0.9772\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 6.1754e-04 - acc: 0.9999 - val_loss: 0.1429 - val_acc: 0.9769\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 5.5246e-04 - acc: 0.9999 - val_loss: 0.1442 - val_acc: 0.9768\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 4.6043e-04 - acc: 1.0000 - val_loss: 0.1441 - val_acc: 0.9770\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 8.0913e-04 - acc: 0.9998 - val_loss: 0.1453 - val_acc: 0.9766\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 8.9322e-04 - acc: 0.9998 - val_loss: 0.1469 - val_acc: 0.9767\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 9.4721e-04 - acc: 0.9998 - val_loss: 0.1471 - val_acc: 0.9772\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.1475 - val_acc: 0.9765\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 8.1134e-04 - acc: 0.9998 - val_loss: 0.1467 - val_acc: 0.9762\n",
      "\n",
      "Accuracy: 97.62%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.2421 - acc: 0.9278 - val_loss: 0.1587 - val_acc: 0.9524\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.1455 - acc: 0.9519 - val_loss: 0.1358 - val_acc: 0.9535\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1237 - acc: 0.9557 - val_loss: 0.1203 - val_acc: 0.9551\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1086 - acc: 0.9596 - val_loss: 0.1152 - val_acc: 0.9577\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0980 - acc: 0.9636 - val_loss: 0.1094 - val_acc: 0.9598\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0872 - acc: 0.9672 - val_loss: 0.1046 - val_acc: 0.9618\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0782 - acc: 0.9698 - val_loss: 0.1007 - val_acc: 0.9639\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0720 - acc: 0.9725 - val_loss: 0.1006 - val_acc: 0.9648\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0658 - acc: 0.9747 - val_loss: 0.1049 - val_acc: 0.9645\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0606 - acc: 0.9772 - val_loss: 0.1040 - val_acc: 0.9655\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0554 - acc: 0.9797 - val_loss: 0.1023 - val_acc: 0.9655\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0500 - acc: 0.9809 - val_loss: 0.1069 - val_acc: 0.9672\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.1048 - val_acc: 0.9676\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0415 - acc: 0.9843 - val_loss: 0.1154 - val_acc: 0.9680\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0348 - acc: 0.9871 - val_loss: 0.1075 - val_acc: 0.9699\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0296 - acc: 0.9894 - val_loss: 0.1111 - val_acc: 0.9721\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0275 - acc: 0.9904 - val_loss: 0.1124 - val_acc: 0.9716\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0246 - acc: 0.9908 - val_loss: 0.1208 - val_acc: 0.9728\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0219 - acc: 0.9922 - val_loss: 0.1305 - val_acc: 0.9717\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0212 - acc: 0.9924 - val_loss: 0.1268 - val_acc: 0.9723\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0188 - acc: 0.9935 - val_loss: 0.1295 - val_acc: 0.9732\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 319us/step - loss: 0.0200 - acc: 0.9930 - val_loss: 0.1354 - val_acc: 0.9732\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0165 - acc: 0.9942 - val_loss: 0.1361 - val_acc: 0.9728\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0134 - acc: 0.9954 - val_loss: 0.1346 - val_acc: 0.9751\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.1489 - val_acc: 0.9732\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0079 - acc: 0.9977 - val_loss: 0.1464 - val_acc: 0.9760\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1495 - val_acc: 0.9763\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.1566 - val_acc: 0.9764\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1552 - val_acc: 0.9749\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.1650 - val_acc: 0.9729\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.1501 - val_acc: 0.9751\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.1630 - val_acc: 0.9736\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0145 - acc: 0.9952 - val_loss: 0.1565 - val_acc: 0.9697\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.1592 - val_acc: 0.9728\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.1599 - val_acc: 0.9730\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.1562 - val_acc: 0.9735\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.1602 - val_acc: 0.9740\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1694 - val_acc: 0.9755\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1713 - val_acc: 0.9746\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.1768 - val_acc: 0.9747\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1769 - val_acc: 0.9758\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1813 - val_acc: 0.9755\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1837 - val_acc: 0.9759\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1819 - val_acc: 0.9762\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1804 - val_acc: 0.9763\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 9.7517e-04 - acc: 0.9998 - val_loss: 0.1871 - val_acc: 0.9760\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1868 - val_acc: 0.9763\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1885 - val_acc: 0.9760\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 9.3205e-04 - acc: 0.9997 - val_loss: 0.1902 - val_acc: 0.9762\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1867 - val_acc: 0.9758\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1996 - val_acc: 0.9759\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 9.5420e-04 - acc: 0.9997 - val_loss: 0.1890 - val_acc: 0.9763\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 9.5138e-04 - acc: 0.9997 - val_loss: 0.2003 - val_acc: 0.9762\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.1995 - val_acc: 0.9735\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.1863 - val_acc: 0.9744\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.1899 - val_acc: 0.9728\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0087 - acc: 0.9974 - val_loss: 0.1867 - val_acc: 0.9742\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.1890 - val_acc: 0.9733\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.1765 - val_acc: 0.9737\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.1814 - val_acc: 0.9741\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.1827 - val_acc: 0.9731\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.1804 - val_acc: 0.9741\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1841 - val_acc: 0.9735\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.1877 - val_acc: 0.9760\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.1852 - val_acc: 0.9763\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1925 - val_acc: 0.9756\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1924 - val_acc: 0.9759\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.1879 - val_acc: 0.9751\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.1900 - val_acc: 0.9756\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1832 - val_acc: 0.9762\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1944 - val_acc: 0.9758\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1957 - val_acc: 0.9756\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.2039 - val_acc: 0.9741\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1950 - val_acc: 0.9752\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1988 - val_acc: 0.9751\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.2042 - val_acc: 0.9752\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1989 - val_acc: 0.9756\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 7.5665e-04 - acc: 0.9998 - val_loss: 0.2008 - val_acc: 0.9755\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 7.3899e-04 - acc: 0.9999 - val_loss: 0.2025 - val_acc: 0.9767\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 4.6248e-04 - acc: 0.9998 - val_loss: 0.2015 - val_acc: 0.9759\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 4.3841e-04 - acc: 0.9999 - val_loss: 0.1986 - val_acc: 0.9757\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 4.1573e-04 - acc: 0.9999 - val_loss: 0.1972 - val_acc: 0.9758\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 4.2354e-04 - acc: 0.9999 - val_loss: 0.2009 - val_acc: 0.9759\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 6.3263e-04 - acc: 0.9998 - val_loss: 0.1987 - val_acc: 0.9760\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 3.6984e-04 - acc: 0.9999 - val_loss: 0.2061 - val_acc: 0.9757\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 3.5995e-04 - acc: 0.9999 - val_loss: 0.2032 - val_acc: 0.9762\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 3.0269e-04 - acc: 0.9999 - val_loss: 0.2023 - val_acc: 0.9758\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 1.7996e-04 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.9754\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 1.8189e-04 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.9759\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 1.8966e-04 - acc: 1.0000 - val_loss: 0.2032 - val_acc: 0.9761\n",
      "\n",
      "Accuracy: 97.61%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 255us/step - loss: 0.2459 - acc: 0.9269 - val_loss: 0.1663 - val_acc: 0.9516\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1476 - acc: 0.9526 - val_loss: 0.1397 - val_acc: 0.9519\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1244 - acc: 0.9567 - val_loss: 0.1282 - val_acc: 0.9551\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1094 - acc: 0.9600 - val_loss: 0.1168 - val_acc: 0.9554\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1006 - acc: 0.9617 - val_loss: 0.1162 - val_acc: 0.9590\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0942 - acc: 0.9646 - val_loss: 0.1136 - val_acc: 0.9584\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0858 - acc: 0.9672 - val_loss: 0.1059 - val_acc: 0.9606\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0784 - acc: 0.9705 - val_loss: 0.1075 - val_acc: 0.9606\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0732 - acc: 0.9716 - val_loss: 0.1084 - val_acc: 0.9610\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0692 - acc: 0.9735 - val_loss: 0.1067 - val_acc: 0.9619\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0647 - acc: 0.9755 - val_loss: 0.1050 - val_acc: 0.9625\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0591 - acc: 0.9780 - val_loss: 0.1004 - val_acc: 0.9645\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0546 - acc: 0.9791 - val_loss: 0.1049 - val_acc: 0.9646\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0504 - acc: 0.9812 - val_loss: 0.1133 - val_acc: 0.9634\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0502 - acc: 0.9811 - val_loss: 0.1085 - val_acc: 0.9665\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0454 - acc: 0.9833 - val_loss: 0.1023 - val_acc: 0.9673\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0419 - acc: 0.9849 - val_loss: 0.1068 - val_acc: 0.9675\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0389 - acc: 0.9859 - val_loss: 0.1052 - val_acc: 0.9682\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0370 - acc: 0.9868 - val_loss: 0.1080 - val_acc: 0.9688\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0333 - acc: 0.9880 - val_loss: 0.1095 - val_acc: 0.9684\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0301 - acc: 0.9896 - val_loss: 0.1092 - val_acc: 0.9692\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0284 - acc: 0.9903 - val_loss: 0.1087 - val_acc: 0.9715\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0258 - acc: 0.9916 - val_loss: 0.1107 - val_acc: 0.9712\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.1150 - val_acc: 0.9710\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0226 - acc: 0.9928 - val_loss: 0.1154 - val_acc: 0.9716\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0211 - acc: 0.9932 - val_loss: 0.1211 - val_acc: 0.9719\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0189 - acc: 0.9942 - val_loss: 0.1212 - val_acc: 0.9725\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0180 - acc: 0.9943 - val_loss: 0.1195 - val_acc: 0.9728\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.1250 - val_acc: 0.9740\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.1247 - val_acc: 0.9731\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0161 - acc: 0.9951 - val_loss: 0.1329 - val_acc: 0.9719\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0152 - acc: 0.9952 - val_loss: 0.1313 - val_acc: 0.9733\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.1305 - val_acc: 0.9742\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.1342 - val_acc: 0.9735\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 320us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.1330 - val_acc: 0.9746\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0084 - acc: 0.9980 - val_loss: 0.1397 - val_acc: 0.9745\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 0.1395 - val_acc: 0.9744\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.1392 - val_acc: 0.9736\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0070 - acc: 0.9987 - val_loss: 0.1404 - val_acc: 0.9747\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1418 - val_acc: 0.9748\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1474 - val_acc: 0.9743\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.1478 - val_acc: 0.9742\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1488 - val_acc: 0.9747\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.1518 - val_acc: 0.9739\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.1494 - val_acc: 0.9741\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.1541 - val_acc: 0.9745\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1560 - val_acc: 0.9740\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1604 - val_acc: 0.9735\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1541 - val_acc: 0.9739\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.1571 - val_acc: 0.9741\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.1571 - val_acc: 0.9739\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.1590 - val_acc: 0.9737\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1591 - val_acc: 0.9739\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 0.1623 - val_acc: 0.9745\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1611 - val_acc: 0.9744\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.1656 - val_acc: 0.9744\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1651 - val_acc: 0.9741\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1652 - val_acc: 0.9746\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1675 - val_acc: 0.9747\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1690 - val_acc: 0.9739\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1705 - val_acc: 0.9742\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1652 - val_acc: 0.9747\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.1722 - val_acc: 0.9744\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.1769 - val_acc: 0.9740\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1876 - val_acc: 0.9709\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.1851 - val_acc: 0.9721\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0167 - acc: 0.9948 - val_loss: 0.1733 - val_acc: 0.9737\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.1834 - val_acc: 0.9725\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1756 - val_acc: 0.9749\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1765 - val_acc: 0.9747\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1765 - val_acc: 0.9749\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1746 - val_acc: 0.9753\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1771 - val_acc: 0.9749\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1757 - val_acc: 0.9755\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1764 - val_acc: 0.9753\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1832 - val_acc: 0.9739\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1800 - val_acc: 0.9743\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1816 - val_acc: 0.9740\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1831 - val_acc: 0.9753\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1842 - val_acc: 0.9751\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1824 - val_acc: 0.9749\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1819 - val_acc: 0.9751\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 8.1536e-04 - acc: 0.9999 - val_loss: 0.1808 - val_acc: 0.9752\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 6.8926e-04 - acc: 0.9999 - val_loss: 0.1840 - val_acc: 0.9751\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 9.5876e-04 - acc: 0.9998 - val_loss: 0.1843 - val_acc: 0.9751\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 7.1024e-04 - acc: 0.9999 - val_loss: 0.1835 - val_acc: 0.9753\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 5.4197e-04 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9754\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 4.8575e-04 - acc: 1.0000 - val_loss: 0.1863 - val_acc: 0.9752\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 4.5054e-04 - acc: 1.0000 - val_loss: 0.1863 - val_acc: 0.9753\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 4.4262e-04 - acc: 1.0000 - val_loss: 0.1878 - val_acc: 0.9751\n",
      "\n",
      "Accuracy: 97.51%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 329us/step - loss: 0.2807 - acc: 0.8747 - val_loss: 0.1446 - val_acc: 0.9470\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1300 - acc: 0.9522 - val_loss: 0.1222 - val_acc: 0.9544\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1153 - acc: 0.9562 - val_loss: 0.1139 - val_acc: 0.9543\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1028 - acc: 0.9605 - val_loss: 0.1109 - val_acc: 0.9556\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0959 - acc: 0.9633 - val_loss: 0.1076 - val_acc: 0.9577\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0893 - acc: 0.9656 - val_loss: 0.1023 - val_acc: 0.9600\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0840 - acc: 0.9675 - val_loss: 0.1076 - val_acc: 0.9594\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0764 - acc: 0.9707 - val_loss: 0.1030 - val_acc: 0.9606\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0699 - acc: 0.9733 - val_loss: 0.0982 - val_acc: 0.9643\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0664 - acc: 0.9745 - val_loss: 0.0997 - val_acc: 0.9646\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0622 - acc: 0.9769 - val_loss: 0.0970 - val_acc: 0.9650\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 121us/step - loss: 0.0567 - acc: 0.9790 - val_loss: 0.0948 - val_acc: 0.9666\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0960 - val_acc: 0.9668\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.0979 - val_acc: 0.9679\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0443 - acc: 0.9839 - val_loss: 0.0984 - val_acc: 0.9671\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0402 - acc: 0.9860 - val_loss: 0.0935 - val_acc: 0.9687\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0364 - acc: 0.9871 - val_loss: 0.0980 - val_acc: 0.9698\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0326 - acc: 0.9888 - val_loss: 0.0940 - val_acc: 0.9707\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0298 - acc: 0.9900 - val_loss: 0.0971 - val_acc: 0.9726\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0242 - acc: 0.9923 - val_loss: 0.0952 - val_acc: 0.9738\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0225 - acc: 0.9930 - val_loss: 0.0929 - val_acc: 0.9750\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0194 - acc: 0.9943 - val_loss: 0.0943 - val_acc: 0.9751\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0940 - val_acc: 0.9751\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0949 - val_acc: 0.9747\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.0979 - val_acc: 0.9749\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.1029 - val_acc: 0.9750\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 0.0151 - acc: 0.9957 - val_loss: 0.1013 - val_acc: 0.9755\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0146 - acc: 0.9959 - val_loss: 0.1054 - val_acc: 0.9733\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.0184 - acc: 0.9943 - val_loss: 0.1118 - val_acc: 0.9737\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0168 - acc: 0.9946 - val_loss: 0.0999 - val_acc: 0.9753\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0161 - acc: 0.9951 - val_loss: 0.1036 - val_acc: 0.9751\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0135 - acc: 0.9960 - val_loss: 0.0998 - val_acc: 0.9757\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0998 - val_acc: 0.9762\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.1020 - val_acc: 0.9778\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.1063 - val_acc: 0.9766\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 1s 360us/step - loss: 0.0107 - acc: 0.9972 - val_loss: 0.1081 - val_acc: 0.9754\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.1103 - val_acc: 0.9749\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.1108 - val_acc: 0.9756\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0182 - acc: 0.9948 - val_loss: 0.1433 - val_acc: 0.9665\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0846 - acc: 0.9725 - val_loss: 0.1394 - val_acc: 0.9613\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0706 - acc: 0.9745 - val_loss: 0.1244 - val_acc: 0.9641\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0468 - acc: 0.9829 - val_loss: 0.1064 - val_acc: 0.9716\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0303 - acc: 0.9891 - val_loss: 0.1046 - val_acc: 0.9731\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0209 - acc: 0.9926 - val_loss: 0.1030 - val_acc: 0.9760\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0159 - acc: 0.9953 - val_loss: 0.1083 - val_acc: 0.9743\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 166us/step - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1046 - val_acc: 0.9763\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 0.1043 - val_acc: 0.9760\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.1056 - val_acc: 0.9763\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1078 - val_acc: 0.9766\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.1090 - val_acc: 0.9767\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.1126 - val_acc: 0.9772\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0056 - acc: 0.9986 - val_loss: 0.1119 - val_acc: 0.9760\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.1142 - val_acc: 0.9773\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.1135 - val_acc: 0.9776\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1140 - val_acc: 0.9767\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1145 - val_acc: 0.9779\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.1153 - val_acc: 0.9772\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1177 - val_acc: 0.9779\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1184 - val_acc: 0.9779\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1176 - val_acc: 0.9770\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1209 - val_acc: 0.9773\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1192 - val_acc: 0.9767\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1160 - val_acc: 0.9779\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1157 - val_acc: 0.9775\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.1209 - val_acc: 0.9763\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.1331 - val_acc: 0.9756\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.1287 - val_acc: 0.9740\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.1340 - val_acc: 0.9740\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0169 - acc: 0.9940 - val_loss: 0.1273 - val_acc: 0.9731\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0152 - acc: 0.9946 - val_loss: 0.1300 - val_acc: 0.9740\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.1207 - val_acc: 0.9757\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.1280 - val_acc: 0.9756\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.1233 - val_acc: 0.9760\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.1208 - val_acc: 0.9760\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1257 - val_acc: 0.9767\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.1276 - val_acc: 0.9771\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1253 - val_acc: 0.9776\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1295 - val_acc: 0.9776\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1302 - val_acc: 0.9774\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.1347 - val_acc: 0.9756\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1313 - val_acc: 0.9770\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1306 - val_acc: 0.9759\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.1378 - val_acc: 0.9733\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.1310 - val_acc: 0.9760\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1342 - val_acc: 0.9749\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1301 - val_acc: 0.9770\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1310 - val_acc: 0.9760\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1355 - val_acc: 0.9769\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1330 - val_acc: 0.9780\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.1377 - val_acc: 0.9746\n",
      "\n",
      "Accuracy: 97.46%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 244us/step - loss: 0.2599 - acc: 0.9208 - val_loss: 0.1703 - val_acc: 0.9516\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1508 - acc: 0.9518 - val_loss: 0.1405 - val_acc: 0.9538\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1258 - acc: 0.9556 - val_loss: 0.1290 - val_acc: 0.9554\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1125 - acc: 0.9593 - val_loss: 0.1204 - val_acc: 0.9562\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1021 - acc: 0.9618 - val_loss: 0.1153 - val_acc: 0.9590\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0945 - acc: 0.9642 - val_loss: 0.1111 - val_acc: 0.9602\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0881 - acc: 0.9671 - val_loss: 0.1111 - val_acc: 0.9602\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0797 - acc: 0.9694 - val_loss: 0.1063 - val_acc: 0.9616\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0787 - acc: 0.9699 - val_loss: 0.1051 - val_acc: 0.9623\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0737 - acc: 0.9711 - val_loss: 0.1041 - val_acc: 0.9640\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0659 - acc: 0.9750 - val_loss: 0.0995 - val_acc: 0.9650\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0616 - acc: 0.9762 - val_loss: 0.1038 - val_acc: 0.9662\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0583 - acc: 0.9779 - val_loss: 0.1036 - val_acc: 0.9651\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0547 - acc: 0.9788 - val_loss: 0.1052 - val_acc: 0.9667\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0514 - acc: 0.9805 - val_loss: 0.1091 - val_acc: 0.9664\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0511 - acc: 0.9808 - val_loss: 0.1066 - val_acc: 0.9677\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0492 - acc: 0.9817 - val_loss: 0.1076 - val_acc: 0.9682\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0462 - acc: 0.9827 - val_loss: 0.1061 - val_acc: 0.9679\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0417 - acc: 0.9850 - val_loss: 0.1081 - val_acc: 0.9694\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0391 - acc: 0.9860 - val_loss: 0.1109 - val_acc: 0.9689\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0375 - acc: 0.9863 - val_loss: 0.1127 - val_acc: 0.9693\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0364 - acc: 0.9867 - val_loss: 0.1177 - val_acc: 0.9701\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0358 - acc: 0.9871 - val_loss: 0.1125 - val_acc: 0.9696\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0331 - acc: 0.9881 - val_loss: 0.1142 - val_acc: 0.9721\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0307 - acc: 0.9887 - val_loss: 0.1206 - val_acc: 0.9704\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0296 - acc: 0.9893 - val_loss: 0.1280 - val_acc: 0.9692\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0273 - acc: 0.9904 - val_loss: 0.1224 - val_acc: 0.9718\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0272 - acc: 0.9904 - val_loss: 0.1282 - val_acc: 0.9717\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0244 - acc: 0.9911 - val_loss: 0.1264 - val_acc: 0.9720\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0242 - acc: 0.9913 - val_loss: 0.1229 - val_acc: 0.9733\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0253 - acc: 0.9913 - val_loss: 0.1436 - val_acc: 0.9703\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0297 - acc: 0.9893 - val_loss: 0.1410 - val_acc: 0.9710\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0293 - acc: 0.9896 - val_loss: 0.1448 - val_acc: 0.9719\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0247 - acc: 0.9914 - val_loss: 0.1374 - val_acc: 0.9719\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0220 - acc: 0.9922 - val_loss: 0.1406 - val_acc: 0.9710\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.1495 - val_acc: 0.9708\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0236 - acc: 0.9920 - val_loss: 0.1445 - val_acc: 0.9725\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0197 - acc: 0.9929 - val_loss: 0.1427 - val_acc: 0.9734\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.1388 - val_acc: 0.9740\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.1470 - val_acc: 0.9742\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.1464 - val_acc: 0.9740\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.1419 - val_acc: 0.9748\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0091 - acc: 0.9975 - val_loss: 0.1444 - val_acc: 0.9747\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1495 - val_acc: 0.9742\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1464 - val_acc: 0.9746\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1524 - val_acc: 0.9746\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 0.1568 - val_acc: 0.9751\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.1665 - val_acc: 0.9735\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.1769 - val_acc: 0.9730\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0134 - acc: 0.9953 - val_loss: 0.1605 - val_acc: 0.9726\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.1671 - val_acc: 0.9729\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.1737 - val_acc: 0.9727\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0212 - acc: 0.9934 - val_loss: 0.1706 - val_acc: 0.9714\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0212 - acc: 0.9930 - val_loss: 0.1718 - val_acc: 0.9717\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.1686 - val_acc: 0.9736\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0195 - acc: 0.9931 - val_loss: 0.1753 - val_acc: 0.9707\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.1736 - val_acc: 0.9723\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0171 - acc: 0.9942 - val_loss: 0.1780 - val_acc: 0.9718\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.1748 - val_acc: 0.9723\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.1813 - val_acc: 0.9735\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.1777 - val_acc: 0.9733\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.1775 - val_acc: 0.9739\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0108 - acc: 0.9967 - val_loss: 0.1818 - val_acc: 0.9733\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1847 - val_acc: 0.9737\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1911 - val_acc: 0.9722\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.1822 - val_acc: 0.9733\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.1749 - val_acc: 0.9750\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.1868 - val_acc: 0.9725\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0138 - acc: 0.9954 - val_loss: 0.1955 - val_acc: 0.9716\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.1900 - val_acc: 0.9740\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.1878 - val_acc: 0.9731\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.1887 - val_acc: 0.9745\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.1932 - val_acc: 0.9731\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0154 - acc: 0.9957 - val_loss: 0.1950 - val_acc: 0.9726\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.2160 - val_acc: 0.9692\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.1959 - val_acc: 0.9726\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.2072 - val_acc: 0.9737\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.2000 - val_acc: 0.9741\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.2032 - val_acc: 0.9731\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2078 - val_acc: 0.9742\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.2094 - val_acc: 0.9726\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.2007 - val_acc: 0.9741\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1970 - val_acc: 0.9744\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.2008 - val_acc: 0.9730\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.1967 - val_acc: 0.9733\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.2034 - val_acc: 0.9746\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.2023 - val_acc: 0.9749\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.2043 - val_acc: 0.9742\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2067 - val_acc: 0.9729\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.1967 - val_acc: 0.9755\n",
      "\n",
      "Accuracy: 97.55%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.6063 - acc: 0.7248 - val_loss: 0.4659 - val_acc: 0.9184\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.3849 - acc: 0.9392 - val_loss: 0.3387 - val_acc: 0.9229\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.2840 - acc: 0.9425 - val_loss: 0.2439 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.2307 - acc: 0.9487 - val_loss: 0.2129 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.2077 - acc: 0.9489 - val_loss: 0.1963 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1950 - acc: 0.9489 - val_loss: 0.1866 - val_acc: 0.9499\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1874 - acc: 0.9489 - val_loss: 0.1803 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.1823 - acc: 0.9489 - val_loss: 0.1761 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1789 - acc: 0.9489 - val_loss: 0.1732 - val_acc: 0.9499\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1764 - acc: 0.9489 - val_loss: 0.1710 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1746 - acc: 0.9489 - val_loss: 0.1695 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1733 - acc: 0.9489 - val_loss: 0.1681 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1728 - acc: 0.9484 - val_loss: 0.1671 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1720 - acc: 0.9486 - val_loss: 0.1666 - val_acc: 0.9499\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1715 - acc: 0.9486 - val_loss: 0.1689 - val_acc: 0.9480\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1705 - acc: 0.9489 - val_loss: 0.1652 - val_acc: 0.9499\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1698 - acc: 0.9489 - val_loss: 0.1649 - val_acc: 0.9499\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1694 - acc: 0.9489 - val_loss: 0.1645 - val_acc: 0.9499\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1691 - acc: 0.9489 - val_loss: 0.1642 - val_acc: 0.9499\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1688 - acc: 0.9489 - val_loss: 0.1640 - val_acc: 0.9499\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1686 - acc: 0.9489 - val_loss: 0.1638 - val_acc: 0.9499\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1685 - acc: 0.9489 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1682 - acc: 0.9489 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1681 - acc: 0.9489 - val_loss: 0.1633 - val_acc: 0.9499\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1706 - acc: 0.9473 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1689 - acc: 0.9483 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9498\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1660 - acc: 0.9489 - val_loss: 0.1618 - val_acc: 0.9498\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1655 - acc: 0.9489 - val_loss: 0.1615 - val_acc: 0.9499\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1646 - acc: 0.9489 - val_loss: 0.1603 - val_acc: 0.9499\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1635 - acc: 0.9489 - val_loss: 0.1594 - val_acc: 0.9499\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1623 - acc: 0.9488 - val_loss: 0.1590 - val_acc: 0.9497\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1616 - acc: 0.9489 - val_loss: 0.1571 - val_acc: 0.9497\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1593 - acc: 0.9489 - val_loss: 0.1557 - val_acc: 0.9498\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1577 - acc: 0.9488 - val_loss: 0.1549 - val_acc: 0.9496\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1565 - acc: 0.9488 - val_loss: 0.1542 - val_acc: 0.9490\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1554 - acc: 0.9489 - val_loss: 0.1524 - val_acc: 0.9498\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1542 - acc: 0.9490 - val_loss: 0.1511 - val_acc: 0.9495\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1522 - acc: 0.9492 - val_loss: 0.1495 - val_acc: 0.9501\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1512 - acc: 0.9493 - val_loss: 0.1484 - val_acc: 0.9499\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1503 - acc: 0.9496 - val_loss: 0.1476 - val_acc: 0.9491\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1485 - acc: 0.9500 - val_loss: 0.1465 - val_acc: 0.9503\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1474 - acc: 0.9505 - val_loss: 0.1463 - val_acc: 0.9502\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1465 - acc: 0.9502 - val_loss: 0.1459 - val_acc: 0.9499\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1456 - acc: 0.9504 - val_loss: 0.1449 - val_acc: 0.9503\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1448 - acc: 0.9504 - val_loss: 0.1462 - val_acc: 0.9502\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1444 - acc: 0.9500 - val_loss: 0.1443 - val_acc: 0.9498\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1437 - acc: 0.9503 - val_loss: 0.1437 - val_acc: 0.9506\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1423 - acc: 0.9501 - val_loss: 0.1428 - val_acc: 0.9511\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1422 - acc: 0.9505 - val_loss: 0.1429 - val_acc: 0.9503\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1412 - acc: 0.9507 - val_loss: 0.1419 - val_acc: 0.9506\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1403 - acc: 0.9499 - val_loss: 0.1422 - val_acc: 0.9504\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.1394 - acc: 0.9500 - val_loss: 0.1414 - val_acc: 0.9506\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1388 - acc: 0.9500 - val_loss: 0.1412 - val_acc: 0.9500\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1386 - acc: 0.9500 - val_loss: 0.1407 - val_acc: 0.9508\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1380 - acc: 0.9502 - val_loss: 0.1412 - val_acc: 0.9505\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1375 - acc: 0.9501 - val_loss: 0.1406 - val_acc: 0.9503\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1371 - acc: 0.9502 - val_loss: 0.1406 - val_acc: 0.9503\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1367 - acc: 0.9504 - val_loss: 0.1400 - val_acc: 0.9506\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1366 - acc: 0.9506 - val_loss: 0.1403 - val_acc: 0.9506\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1363 - acc: 0.9500 - val_loss: 0.1406 - val_acc: 0.9504\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1364 - acc: 0.9501 - val_loss: 0.1403 - val_acc: 0.9505\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1358 - acc: 0.9501 - val_loss: 0.1394 - val_acc: 0.9509\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1355 - acc: 0.9500 - val_loss: 0.1393 - val_acc: 0.9505\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1352 - acc: 0.9501 - val_loss: 0.1395 - val_acc: 0.9506\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1345 - acc: 0.9502 - val_loss: 0.1386 - val_acc: 0.9508\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1342 - acc: 0.9502 - val_loss: 0.1392 - val_acc: 0.9506\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1341 - acc: 0.9502 - val_loss: 0.1384 - val_acc: 0.9506\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1338 - acc: 0.9502 - val_loss: 0.1386 - val_acc: 0.9504\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1335 - acc: 0.9502 - val_loss: 0.1386 - val_acc: 0.9508\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1330 - acc: 0.9502 - val_loss: 0.1394 - val_acc: 0.9509\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1330 - acc: 0.9503 - val_loss: 0.1387 - val_acc: 0.9510\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1326 - acc: 0.9503 - val_loss: 0.1384 - val_acc: 0.9510\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1325 - acc: 0.9502 - val_loss: 0.1384 - val_acc: 0.9508\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1327 - acc: 0.9502 - val_loss: 0.1392 - val_acc: 0.9506\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1326 - acc: 0.9502 - val_loss: 0.1384 - val_acc: 0.9508\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1324 - acc: 0.9503 - val_loss: 0.1384 - val_acc: 0.9505\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1326 - acc: 0.9499 - val_loss: 0.1386 - val_acc: 0.9508\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1320 - acc: 0.9504 - val_loss: 0.1379 - val_acc: 0.9510\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1318 - acc: 0.9503 - val_loss: 0.1386 - val_acc: 0.9510\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1316 - acc: 0.9502 - val_loss: 0.1384 - val_acc: 0.9508\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1310 - acc: 0.9503 - val_loss: 0.1384 - val_acc: 0.9507\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1308 - acc: 0.9509 - val_loss: 0.1380 - val_acc: 0.9512\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1306 - acc: 0.9506 - val_loss: 0.1378 - val_acc: 0.9508\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1308 - acc: 0.9504 - val_loss: 0.1384 - val_acc: 0.9512\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1309 - acc: 0.9509 - val_loss: 0.1390 - val_acc: 0.9512\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1309 - acc: 0.9511 - val_loss: 0.1384 - val_acc: 0.9511\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1304 - acc: 0.9510 - val_loss: 0.1382 - val_acc: 0.9508\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1299 - acc: 0.9504 - val_loss: 0.1378 - val_acc: 0.9507\n",
      "\n",
      "Accuracy: 95.07%\n",
      "\n",
      "43571\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 412us/step - loss: 0.3423 - acc: 0.8834 - val_loss: 0.1970 - val_acc: 0.9453\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 167us/step - loss: 0.1814 - acc: 0.9477 - val_loss: 0.1687 - val_acc: 0.9499\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1710 - acc: 0.9489 - val_loss: 0.1648 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 1s 188us/step - loss: 0.1694 - acc: 0.9486 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 163us/step - loss: 0.1680 - acc: 0.9489 - val_loss: 0.1632 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1643 - val_acc: 0.9494\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.1683 - acc: 0.9483 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1695 - acc: 0.9476 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1711 - acc: 0.9468 - val_loss: 0.1632 - val_acc: 0.9498\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 166us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1620 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.1662 - acc: 0.9489 - val_loss: 0.1632 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1671 - acc: 0.9476 - val_loss: 0.1604 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 251us/step - loss: 0.1643 - acc: 0.9482 - val_loss: 0.1566 - val_acc: 0.9499\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 224us/step - loss: 0.1599 - acc: 0.9489 - val_loss: 0.1547 - val_acc: 0.9499\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.1586 - acc: 0.9489 - val_loss: 0.1545 - val_acc: 0.9499\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.1570 - acc: 0.9489 - val_loss: 0.1538 - val_acc: 0.9499\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.1563 - acc: 0.9489 - val_loss: 0.1520 - val_acc: 0.9499\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.1562 - acc: 0.9489 - val_loss: 0.1536 - val_acc: 0.9499\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 186us/step - loss: 0.1556 - acc: 0.9489 - val_loss: 0.1512 - val_acc: 0.9499\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1552 - acc: 0.9489 - val_loss: 0.1515 - val_acc: 0.9499\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.1553 - acc: 0.9489 - val_loss: 0.1517 - val_acc: 0.9499\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1549 - acc: 0.9489 - val_loss: 0.1572 - val_acc: 0.9483\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.1544 - acc: 0.9489 - val_loss: 0.1506 - val_acc: 0.9499\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 172us/step - loss: 0.1537 - acc: 0.9489 - val_loss: 0.1501 - val_acc: 0.9498\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 171us/step - loss: 0.1547 - acc: 0.9489 - val_loss: 0.1507 - val_acc: 0.9499\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.1536 - acc: 0.9489 - val_loss: 0.1506 - val_acc: 0.9499\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 179us/step - loss: 0.1533 - acc: 0.9489 - val_loss: 0.1513 - val_acc: 0.9492\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1549 - acc: 0.9481 - val_loss: 0.1508 - val_acc: 0.9498\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.1525 - acc: 0.9489 - val_loss: 0.1497 - val_acc: 0.9499\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 163us/step - loss: 0.1517 - acc: 0.9489 - val_loss: 0.1482 - val_acc: 0.9499\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1514 - acc: 0.9489 - val_loss: 0.1499 - val_acc: 0.9499\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.1509 - acc: 0.9489 - val_loss: 0.1484 - val_acc: 0.9498\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1502 - acc: 0.9489 - val_loss: 0.1486 - val_acc: 0.9499\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 547us/step - loss: 0.1503 - acc: 0.9489 - val_loss: 0.1479 - val_acc: 0.9499\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 1s 248us/step - loss: 0.1507 - acc: 0.9489 - val_loss: 0.1482 - val_acc: 0.9499\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.1496 - acc: 0.9489 - val_loss: 0.1493 - val_acc: 0.9499\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 254us/step - loss: 0.1492 - acc: 0.9489 - val_loss: 0.1475 - val_acc: 0.9499\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 1s 253us/step - loss: 0.1486 - acc: 0.9489 - val_loss: 0.1474 - val_acc: 0.9499\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 1s 206us/step - loss: 0.1492 - acc: 0.9488 - val_loss: 0.1473 - val_acc: 0.9499\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.1501 - acc: 0.9482 - val_loss: 0.1474 - val_acc: 0.9499\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1481 - acc: 0.9489 - val_loss: 0.1459 - val_acc: 0.9499\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1476 - acc: 0.9489 - val_loss: 0.1474 - val_acc: 0.9499\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1486 - acc: 0.9490 - val_loss: 0.1478 - val_acc: 0.9499\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.1476 - acc: 0.9489 - val_loss: 0.1466 - val_acc: 0.9499\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 118us/step - loss: 0.1470 - acc: 0.9489 - val_loss: 0.1465 - val_acc: 0.9499\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 0.1467 - acc: 0.9489 - val_loss: 0.1469 - val_acc: 0.9499\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.1469 - acc: 0.9489 - val_loss: 0.1464 - val_acc: 0.9499\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1462 - acc: 0.9489 - val_loss: 0.1468 - val_acc: 0.9499\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 216us/step - loss: 0.1460 - acc: 0.9489 - val_loss: 0.1462 - val_acc: 0.9499\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 202us/step - loss: 0.1458 - acc: 0.9489 - val_loss: 0.1469 - val_acc: 0.9499\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 349us/step - loss: 0.1462 - acc: 0.9489 - val_loss: 0.1470 - val_acc: 0.9499\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.1464 - acc: 0.9489 - val_loss: 0.1477 - val_acc: 0.9499\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 188us/step - loss: 0.1457 - acc: 0.9489 - val_loss: 0.1473 - val_acc: 0.9499\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.1494 - acc: 0.9489 - val_loss: 0.1517 - val_acc: 0.9478\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 1s 221us/step - loss: 0.1466 - acc: 0.9490 - val_loss: 0.1482 - val_acc: 0.9499\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 1s 300us/step - loss: 0.1455 - acc: 0.9489 - val_loss: 0.1448 - val_acc: 0.9499\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.1449 - acc: 0.9489 - val_loss: 0.1461 - val_acc: 0.9499\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 124us/step - loss: 0.1459 - acc: 0.9489 - val_loss: 0.1464 - val_acc: 0.9499\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1451 - acc: 0.9490 - val_loss: 0.1462 - val_acc: 0.9499\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.1454 - acc: 0.9489 - val_loss: 0.1475 - val_acc: 0.9499\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.1456 - acc: 0.9489 - val_loss: 0.1468 - val_acc: 0.9499\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.1450 - acc: 0.9489 - val_loss: 0.1463 - val_acc: 0.9499\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1447 - acc: 0.9489 - val_loss: 0.1455 - val_acc: 0.9499\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1462 - acc: 0.9487 - val_loss: 0.1472 - val_acc: 0.9499\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.1443 - acc: 0.9488 - val_loss: 0.1457 - val_acc: 0.9499\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.1467 - acc: 0.9487 - val_loss: 0.1469 - val_acc: 0.9499\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 451us/step - loss: 0.1454 - acc: 0.9489 - val_loss: 0.1469 - val_acc: 0.9499\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.1445 - acc: 0.9489 - val_loss: 0.1464 - val_acc: 0.9499\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 177us/step - loss: 0.1443 - acc: 0.9489 - val_loss: 0.1453 - val_acc: 0.9499\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.1441 - acc: 0.9489 - val_loss: 0.1457 - val_acc: 0.9499\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 134us/step - loss: 0.1440 - acc: 0.9489 - val_loss: 0.1450 - val_acc: 0.9499\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 162us/step - loss: 0.1443 - acc: 0.9489 - val_loss: 0.1457 - val_acc: 0.9498\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.1437 - acc: 0.9489 - val_loss: 0.1455 - val_acc: 0.9499\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.1449 - acc: 0.9484 - val_loss: 0.1470 - val_acc: 0.9503\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.1436 - acc: 0.9489 - val_loss: 0.1455 - val_acc: 0.9499\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.1434 - acc: 0.9487 - val_loss: 0.1445 - val_acc: 0.9497\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.1432 - acc: 0.9489 - val_loss: 0.1452 - val_acc: 0.9497\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 135us/step - loss: 0.1431 - acc: 0.9489 - val_loss: 0.1452 - val_acc: 0.9497\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.1431 - acc: 0.9489 - val_loss: 0.1458 - val_acc: 0.9499\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.1430 - acc: 0.9489 - val_loss: 0.1466 - val_acc: 0.9497\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.1430 - acc: 0.9489 - val_loss: 0.1455 - val_acc: 0.9497\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.1430 - acc: 0.9488 - val_loss: 0.1458 - val_acc: 0.9497\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.1432 - acc: 0.9486 - val_loss: 0.1457 - val_acc: 0.9498\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.1434 - acc: 0.9488 - val_loss: 0.1454 - val_acc: 0.9497\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.1433 - acc: 0.9488 - val_loss: 0.1458 - val_acc: 0.9497\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.1433 - acc: 0.9488 - val_loss: 0.1454 - val_acc: 0.9497\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1435 - acc: 0.9488 - val_loss: 0.1453 - val_acc: 0.9498\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1430 - acc: 0.9488 - val_loss: 0.1453 - val_acc: 0.9499\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.1429 - acc: 0.9488 - val_loss: 0.1456 - val_acc: 0.9499\n",
      "\n",
      "Accuracy: 94.99%\n",
      "\n",
      "43571\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.3115 - acc: 0.8547 - val_loss: 0.1420 - val_acc: 0.9514\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1350 - acc: 0.9516 - val_loss: 0.1280 - val_acc: 0.9535\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1204 - acc: 0.9557 - val_loss: 0.1206 - val_acc: 0.9542\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1120 - acc: 0.9572 - val_loss: 0.1167 - val_acc: 0.9563\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1067 - acc: 0.9598 - val_loss: 0.1105 - val_acc: 0.9569\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0977 - acc: 0.9629 - val_loss: 0.1112 - val_acc: 0.9589\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0924 - acc: 0.9653 - val_loss: 0.1096 - val_acc: 0.9589\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0864 - acc: 0.9676 - val_loss: 0.1078 - val_acc: 0.9602\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0815 - acc: 0.9693 - val_loss: 0.1052 - val_acc: 0.9618\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0764 - acc: 0.9708 - val_loss: 0.1039 - val_acc: 0.9629\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0722 - acc: 0.9731 - val_loss: 0.1072 - val_acc: 0.9627\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0697 - acc: 0.9737 - val_loss: 0.1056 - val_acc: 0.9635\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0640 - acc: 0.9764 - val_loss: 0.1034 - val_acc: 0.9653\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0617 - acc: 0.9770 - val_loss: 0.1052 - val_acc: 0.9650\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0568 - acc: 0.9793 - val_loss: 0.1046 - val_acc: 0.9668\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0515 - acc: 0.9815 - val_loss: 0.1042 - val_acc: 0.9675\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0484 - acc: 0.9827 - val_loss: 0.1076 - val_acc: 0.9672\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0456 - acc: 0.9837 - val_loss: 0.1048 - val_acc: 0.9684\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0413 - acc: 0.9857 - val_loss: 0.1034 - val_acc: 0.9698\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0388 - acc: 0.9867 - val_loss: 0.1037 - val_acc: 0.9686\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0359 - acc: 0.9877 - val_loss: 0.1082 - val_acc: 0.9714\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0338 - acc: 0.9888 - val_loss: 0.1058 - val_acc: 0.9724\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0301 - acc: 0.9897 - val_loss: 0.1106 - val_acc: 0.9712\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0283 - acc: 0.9907 - val_loss: 0.1083 - val_acc: 0.9723\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0264 - acc: 0.9915 - val_loss: 0.1129 - val_acc: 0.9712\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0271 - acc: 0.9908 - val_loss: 0.1119 - val_acc: 0.9717\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0249 - acc: 0.9918 - val_loss: 0.1167 - val_acc: 0.9722\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0221 - acc: 0.9933 - val_loss: 0.1118 - val_acc: 0.9731\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0204 - acc: 0.9940 - val_loss: 0.1162 - val_acc: 0.9737\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0197 - acc: 0.9939 - val_loss: 0.1186 - val_acc: 0.9723\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0190 - acc: 0.9937 - val_loss: 0.1155 - val_acc: 0.9742\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.1205 - val_acc: 0.9740\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0151 - acc: 0.9957 - val_loss: 0.1227 - val_acc: 0.9753\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0135 - acc: 0.9964 - val_loss: 0.1196 - val_acc: 0.9756\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.1231 - val_acc: 0.9756\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0110 - acc: 0.9972 - val_loss: 0.1246 - val_acc: 0.9755\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.1283 - val_acc: 0.9751\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.1253 - val_acc: 0.9762\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0093 - acc: 0.9979 - val_loss: 0.1312 - val_acc: 0.9755\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0085 - acc: 0.9981 - val_loss: 0.1287 - val_acc: 0.9764\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 0.1296 - val_acc: 0.9760\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.1343 - val_acc: 0.9756\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0076 - acc: 0.9984 - val_loss: 0.1321 - val_acc: 0.9759\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0068 - acc: 0.9986 - val_loss: 0.1379 - val_acc: 0.9757\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.1372 - val_acc: 0.9761\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0063 - acc: 0.9987 - val_loss: 0.1383 - val_acc: 0.9763\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1399 - val_acc: 0.9758\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.1436 - val_acc: 0.9771\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1393 - val_acc: 0.9763\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.1418 - val_acc: 0.9765\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1426 - val_acc: 0.9767\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1437 - val_acc: 0.9762\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.1464 - val_acc: 0.9765\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 319us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.1487 - val_acc: 0.9754\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.1471 - val_acc: 0.9761\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1509 - val_acc: 0.9757\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.1543 - val_acc: 0.9756\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0039 - acc: 0.9993 - val_loss: 0.1526 - val_acc: 0.9754\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1546 - val_acc: 0.9757\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.1558 - val_acc: 0.9753\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1545 - val_acc: 0.9759\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.1570 - val_acc: 0.9754\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.1556 - val_acc: 0.9763\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1560 - val_acc: 0.9758\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.1582 - val_acc: 0.9755\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 0.1562 - val_acc: 0.9762\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1597 - val_acc: 0.9762\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.1607 - val_acc: 0.9759\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1608 - val_acc: 0.9758\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1610 - val_acc: 0.9750\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1621 - val_acc: 0.9760\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1639 - val_acc: 0.9749\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1612 - val_acc: 0.9754\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.1640 - val_acc: 0.9752\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1654 - val_acc: 0.9755\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.1671 - val_acc: 0.9756\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1666 - val_acc: 0.9758\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1655 - val_acc: 0.9753\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1673 - val_acc: 0.9759\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1669 - val_acc: 0.9757\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1676 - val_acc: 0.9765\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1690 - val_acc: 0.9756\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1688 - val_acc: 0.9756\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1696 - val_acc: 0.9757\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1693 - val_acc: 0.9758\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1686 - val_acc: 0.9760\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1721 - val_acc: 0.9754\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1731 - val_acc: 0.9753\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1740 - val_acc: 0.9754\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1701 - val_acc: 0.9757\n",
      "\n",
      "Accuracy: 97.57%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 546us/step - loss: 0.6312 - acc: 0.6674 - val_loss: 0.5403 - val_acc: 0.7869\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 315us/step - loss: 0.4166 - acc: 0.8752 - val_loss: 0.2763 - val_acc: 0.9355\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.2090 - acc: 0.9453 - val_loss: 0.1710 - val_acc: 0.9481\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 1s 298us/step - loss: 0.1575 - acc: 0.9492 - val_loss: 0.1507 - val_acc: 0.9503\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.1449 - acc: 0.9504 - val_loss: 0.1431 - val_acc: 0.9512\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 1s 308us/step - loss: 0.1392 - acc: 0.9513 - val_loss: 0.1392 - val_acc: 0.9513\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.1355 - acc: 0.9515 - val_loss: 0.1367 - val_acc: 0.9513\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.1326 - acc: 0.9520 - val_loss: 0.1348 - val_acc: 0.9515\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.1303 - acc: 0.9527 - val_loss: 0.1333 - val_acc: 0.9522\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 378us/step - loss: 0.1283 - acc: 0.9531 - val_loss: 0.1319 - val_acc: 0.9525\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 2s 783us/step - loss: 0.1265 - acc: 0.9534 - val_loss: 0.1307 - val_acc: 0.9520\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 420us/step - loss: 0.1248 - acc: 0.9541 - val_loss: 0.1297 - val_acc: 0.9517\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 439us/step - loss: 0.1234 - acc: 0.9543 - val_loss: 0.1286 - val_acc: 0.9519\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 341us/step - loss: 0.1221 - acc: 0.9546 - val_loss: 0.1278 - val_acc: 0.9520\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.1209 - acc: 0.9549 - val_loss: 0.1272 - val_acc: 0.9525\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 312us/step - loss: 0.1197 - acc: 0.9553 - val_loss: 0.1265 - val_acc: 0.9521\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.1187 - acc: 0.9556 - val_loss: 0.1259 - val_acc: 0.9528\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 296us/step - loss: 0.1177 - acc: 0.9557 - val_loss: 0.1254 - val_acc: 0.9526\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.1168 - acc: 0.9562 - val_loss: 0.1246 - val_acc: 0.9527\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.1159 - acc: 0.9562 - val_loss: 0.1241 - val_acc: 0.9533\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 1s 330us/step - loss: 0.1151 - acc: 0.9566 - val_loss: 0.1236 - val_acc: 0.9533\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 309us/step - loss: 0.1143 - acc: 0.9568 - val_loss: 0.1232 - val_acc: 0.9535\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 1s 311us/step - loss: 0.1136 - acc: 0.9568 - val_loss: 0.1230 - val_acc: 0.9535\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 1s 323us/step - loss: 0.1129 - acc: 0.9572 - val_loss: 0.1225 - val_acc: 0.9537\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 1s 305us/step - loss: 0.1122 - acc: 0.9572 - val_loss: 0.1222 - val_acc: 0.9539\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 1s 328us/step - loss: 0.1117 - acc: 0.9577 - val_loss: 0.1217 - val_acc: 0.9537\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.1111 - acc: 0.9577 - val_loss: 0.1214 - val_acc: 0.9542\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 2s 715us/step - loss: 0.1105 - acc: 0.9578 - val_loss: 0.1211 - val_acc: 0.9543\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 1s 310us/step - loss: 0.1099 - acc: 0.9582 - val_loss: 0.1208 - val_acc: 0.9543\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 1s 355us/step - loss: 0.1095 - acc: 0.9582 - val_loss: 0.1206 - val_acc: 0.9545\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 1s 364us/step - loss: 0.1089 - acc: 0.9584 - val_loss: 0.1203 - val_acc: 0.9549\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.1084 - acc: 0.9588 - val_loss: 0.1201 - val_acc: 0.9549\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.1079 - acc: 0.9590 - val_loss: 0.1200 - val_acc: 0.9546\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.1075 - acc: 0.9593 - val_loss: 0.1196 - val_acc: 0.9547\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.1071 - acc: 0.9593 - val_loss: 0.1194 - val_acc: 0.9549\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 1s 369us/step - loss: 0.1066 - acc: 0.9593 - val_loss: 0.1191 - val_acc: 0.9552\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 352us/step - loss: 0.1062 - acc: 0.9595 - val_loss: 0.1190 - val_acc: 0.9551\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 322us/step - loss: 0.1058 - acc: 0.9595 - val_loss: 0.1187 - val_acc: 0.9551\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 1s 364us/step - loss: 0.1054 - acc: 0.9597 - val_loss: 0.1185 - val_acc: 0.9552\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.1050 - acc: 0.9597 - val_loss: 0.1184 - val_acc: 0.9551\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.1047 - acc: 0.9602 - val_loss: 0.1182 - val_acc: 0.9554\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.1043 - acc: 0.9601 - val_loss: 0.1181 - val_acc: 0.9553\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 1s 295us/step - loss: 0.1039 - acc: 0.9603 - val_loss: 0.1178 - val_acc: 0.9552\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.1036 - acc: 0.9603 - val_loss: 0.1176 - val_acc: 0.9551\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 1s 388us/step - loss: 0.1032 - acc: 0.9605 - val_loss: 0.1175 - val_acc: 0.9549\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 2s 583us/step - loss: 0.1029 - acc: 0.9608 - val_loss: 0.1174 - val_acc: 0.9550\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 1s 401us/step - loss: 0.1026 - acc: 0.9608 - val_loss: 0.1171 - val_acc: 0.9550\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 1s 375us/step - loss: 0.1023 - acc: 0.9610 - val_loss: 0.1169 - val_acc: 0.9554\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 357us/step - loss: 0.1020 - acc: 0.9611 - val_loss: 0.1169 - val_acc: 0.9554\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 460us/step - loss: 0.1017 - acc: 0.9613 - val_loss: 0.1168 - val_acc: 0.9553\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 452us/step - loss: 0.1013 - acc: 0.9615 - val_loss: 0.1167 - val_acc: 0.9555\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 308us/step - loss: 0.1011 - acc: 0.9615 - val_loss: 0.1165 - val_acc: 0.9552\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 1s 318us/step - loss: 0.1008 - acc: 0.9617 - val_loss: 0.1163 - val_acc: 0.9556\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 309us/step - loss: 0.1005 - acc: 0.9618 - val_loss: 0.1163 - val_acc: 0.9554\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 447us/step - loss: 0.1003 - acc: 0.9618 - val_loss: 0.1161 - val_acc: 0.9554\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 1s 372us/step - loss: 0.1000 - acc: 0.9620 - val_loss: 0.1160 - val_acc: 0.9557\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 1s 396us/step - loss: 0.0997 - acc: 0.9620 - val_loss: 0.1159 - val_acc: 0.9557\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 1s 460us/step - loss: 0.0995 - acc: 0.9621 - val_loss: 0.1156 - val_acc: 0.9561\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.0992 - acc: 0.9623 - val_loss: 0.1155 - val_acc: 0.9560\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 383us/step - loss: 0.0990 - acc: 0.9624 - val_loss: 0.1155 - val_acc: 0.9558\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 2s 654us/step - loss: 0.0987 - acc: 0.9624 - val_loss: 0.1154 - val_acc: 0.9560\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 429us/step - loss: 0.0985 - acc: 0.9625 - val_loss: 0.1152 - val_acc: 0.9560\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 2s 646us/step - loss: 0.0982 - acc: 0.9628 - val_loss: 0.1151 - val_acc: 0.9563\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 323us/step - loss: 0.0980 - acc: 0.9625 - val_loss: 0.1151 - val_acc: 0.9563\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 319us/step - loss: 0.0978 - acc: 0.9628 - val_loss: 0.1149 - val_acc: 0.9561\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.0976 - acc: 0.9629 - val_loss: 0.1148 - val_acc: 0.9565\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.0973 - acc: 0.9630 - val_loss: 0.1148 - val_acc: 0.9567\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 314us/step - loss: 0.0971 - acc: 0.9631 - val_loss: 0.1147 - val_acc: 0.9566\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 0.0969 - acc: 0.9632 - val_loss: 0.1146 - val_acc: 0.9565\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0967 - acc: 0.9634 - val_loss: 0.1145 - val_acc: 0.9567\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0965 - acc: 0.9635 - val_loss: 0.1144 - val_acc: 0.9567\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 1s 314us/step - loss: 0.0963 - acc: 0.9636 - val_loss: 0.1143 - val_acc: 0.9567\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0961 - acc: 0.9636 - val_loss: 0.1142 - val_acc: 0.9566\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 1s 323us/step - loss: 0.0959 - acc: 0.9638 - val_loss: 0.1142 - val_acc: 0.9568\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0957 - acc: 0.9638 - val_loss: 0.1140 - val_acc: 0.9571\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 1s 311us/step - loss: 0.0955 - acc: 0.9641 - val_loss: 0.1139 - val_acc: 0.9568\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 1s 439us/step - loss: 0.0953 - acc: 0.9640 - val_loss: 0.1138 - val_acc: 0.9570\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 2s 587us/step - loss: 0.0951 - acc: 0.9642 - val_loss: 0.1138 - val_acc: 0.9570\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 1s 358us/step - loss: 0.0949 - acc: 0.9643 - val_loss: 0.1137 - val_acc: 0.9569\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 1s 395us/step - loss: 0.0948 - acc: 0.9643 - val_loss: 0.1136 - val_acc: 0.9566\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 1s 330us/step - loss: 0.0946 - acc: 0.9643 - val_loss: 0.1136 - val_acc: 0.9570\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 1s 348us/step - loss: 0.0944 - acc: 0.9646 - val_loss: 0.1134 - val_acc: 0.9571\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 1s 321us/step - loss: 0.0942 - acc: 0.9645 - val_loss: 0.1133 - val_acc: 0.9571\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 1s 506us/step - loss: 0.0940 - acc: 0.9646 - val_loss: 0.1133 - val_acc: 0.9570\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0939 - acc: 0.9647 - val_loss: 0.1132 - val_acc: 0.9571\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 1s 314us/step - loss: 0.0937 - acc: 0.9648 - val_loss: 0.1131 - val_acc: 0.9569\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 326us/step - loss: 0.0935 - acc: 0.9649 - val_loss: 0.1130 - val_acc: 0.9570\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 1s 308us/step - loss: 0.0934 - acc: 0.9650 - val_loss: 0.1130 - val_acc: 0.9569\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 0.0932 - acc: 0.9649 - val_loss: 0.1129 - val_acc: 0.9572\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 1s 309us/step - loss: 0.0930 - acc: 0.9650 - val_loss: 0.1128 - val_acc: 0.9573\n",
      "\n",
      "Accuracy: 95.73%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 225us/step - loss: 0.4686 - acc: 0.7749 - val_loss: 0.1818 - val_acc: 0.9428\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1584 - acc: 0.9484 - val_loss: 0.1434 - val_acc: 0.9495\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1357 - acc: 0.9511 - val_loss: 0.1298 - val_acc: 0.9532\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1237 - acc: 0.9545 - val_loss: 0.1239 - val_acc: 0.9543\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.1158 - acc: 0.9570 - val_loss: 0.1193 - val_acc: 0.9551\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1096 - acc: 0.9589 - val_loss: 0.1163 - val_acc: 0.9565\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.1041 - acc: 0.9609 - val_loss: 0.1132 - val_acc: 0.9572\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0995 - acc: 0.9628 - val_loss: 0.1106 - val_acc: 0.9575\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0954 - acc: 0.9643 - val_loss: 0.1096 - val_acc: 0.9576\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0915 - acc: 0.9653 - val_loss: 0.1079 - val_acc: 0.9579\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0876 - acc: 0.9677 - val_loss: 0.1057 - val_acc: 0.9588\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0836 - acc: 0.9694 - val_loss: 0.1044 - val_acc: 0.9601\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0800 - acc: 0.9704 - val_loss: 0.1006 - val_acc: 0.9614\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0766 - acc: 0.9721 - val_loss: 0.0985 - val_acc: 0.9630\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0736 - acc: 0.9734 - val_loss: 0.0989 - val_acc: 0.9629\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0698 - acc: 0.9755 - val_loss: 0.0965 - val_acc: 0.9629\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0667 - acc: 0.9766 - val_loss: 0.1009 - val_acc: 0.9616\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0670 - acc: 0.9763 - val_loss: 0.0976 - val_acc: 0.9646\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0645 - acc: 0.9772 - val_loss: 0.0980 - val_acc: 0.9641\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0604 - acc: 0.9792 - val_loss: 0.0959 - val_acc: 0.9654\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0575 - acc: 0.9806 - val_loss: 0.0958 - val_acc: 0.9662\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0546 - acc: 0.9815 - val_loss: 0.0928 - val_acc: 0.9684\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0522 - acc: 0.9826 - val_loss: 0.0926 - val_acc: 0.9684\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0500 - acc: 0.9835 - val_loss: 0.0930 - val_acc: 0.9677\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0486 - acc: 0.9836 - val_loss: 0.0939 - val_acc: 0.9676\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0466 - acc: 0.9848 - val_loss: 0.0936 - val_acc: 0.9700\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0445 - acc: 0.9857 - val_loss: 0.0939 - val_acc: 0.9685\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0436 - acc: 0.9857 - val_loss: 0.0946 - val_acc: 0.9687\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0418 - acc: 0.9866 - val_loss: 0.0931 - val_acc: 0.9693\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.0391 - acc: 0.9877 - val_loss: 0.0937 - val_acc: 0.9692\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0377 - acc: 0.9881 - val_loss: 0.0940 - val_acc: 0.9708\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0363 - acc: 0.9886 - val_loss: 0.0920 - val_acc: 0.9716\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0344 - acc: 0.9893 - val_loss: 0.0950 - val_acc: 0.9702\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0328 - acc: 0.9902 - val_loss: 0.0948 - val_acc: 0.9707\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0976 - val_acc: 0.9699\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0317 - acc: 0.9905 - val_loss: 0.0939 - val_acc: 0.9716\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0306 - acc: 0.9908 - val_loss: 0.0937 - val_acc: 0.9708\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0282 - acc: 0.9918 - val_loss: 0.0947 - val_acc: 0.9720\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0271 - acc: 0.9926 - val_loss: 0.0938 - val_acc: 0.9719\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0257 - acc: 0.9928 - val_loss: 0.0958 - val_acc: 0.9723\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0246 - acc: 0.9935 - val_loss: 0.0938 - val_acc: 0.9731\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0236 - acc: 0.9941 - val_loss: 0.0957 - val_acc: 0.9726\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0227 - acc: 0.9942 - val_loss: 0.0968 - val_acc: 0.9719\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.0976 - val_acc: 0.9721\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0215 - acc: 0.9946 - val_loss: 0.1000 - val_acc: 0.9723\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0981 - val_acc: 0.9726\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0206 - acc: 0.9948 - val_loss: 0.1002 - val_acc: 0.9723\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0195 - acc: 0.9956 - val_loss: 0.1025 - val_acc: 0.9725\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0190 - acc: 0.9958 - val_loss: 0.1014 - val_acc: 0.9731\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.1009 - val_acc: 0.9726\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0997 - val_acc: 0.9735\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0169 - acc: 0.9964 - val_loss: 0.1053 - val_acc: 0.9726\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0162 - acc: 0.9967 - val_loss: 0.1041 - val_acc: 0.9726\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0153 - acc: 0.9971 - val_loss: 0.1056 - val_acc: 0.9735\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0153 - acc: 0.9969 - val_loss: 0.1066 - val_acc: 0.9733\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0155 - acc: 0.9968 - val_loss: 0.1052 - val_acc: 0.9737\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0144 - acc: 0.9973 - val_loss: 0.1055 - val_acc: 0.9728\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0139 - acc: 0.9973 - val_loss: 0.1060 - val_acc: 0.9740\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0130 - acc: 0.9978 - val_loss: 0.1093 - val_acc: 0.9726\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.1072 - val_acc: 0.9743\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0128 - acc: 0.9975 - val_loss: 0.1079 - val_acc: 0.9735\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0122 - acc: 0.9979 - val_loss: 0.1086 - val_acc: 0.9736\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.0113 - acc: 0.9982 - val_loss: 0.1090 - val_acc: 0.9736\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0109 - acc: 0.9983 - val_loss: 0.1111 - val_acc: 0.9735\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0109 - acc: 0.9983 - val_loss: 0.1115 - val_acc: 0.9728\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0115 - acc: 0.9979 - val_loss: 0.1140 - val_acc: 0.9728\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0113 - acc: 0.9981 - val_loss: 0.1133 - val_acc: 0.9730\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0106 - acc: 0.9982 - val_loss: 0.1131 - val_acc: 0.9730\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0107 - acc: 0.9982 - val_loss: 0.1132 - val_acc: 0.9734\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.0098 - acc: 0.9984 - val_loss: 0.1119 - val_acc: 0.9739\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0095 - acc: 0.9986 - val_loss: 0.1146 - val_acc: 0.9733\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0090 - acc: 0.9987 - val_loss: 0.1146 - val_acc: 0.9737\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0085 - acc: 0.9988 - val_loss: 0.1144 - val_acc: 0.9735\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.0082 - acc: 0.9989 - val_loss: 0.1160 - val_acc: 0.9733\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.1171 - val_acc: 0.9737\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0077 - acc: 0.9991 - val_loss: 0.1174 - val_acc: 0.9729\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.1172 - val_acc: 0.9731\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0075 - acc: 0.9991 - val_loss: 0.1185 - val_acc: 0.9731\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.1195 - val_acc: 0.9739\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0075 - acc: 0.9990 - val_loss: 0.1199 - val_acc: 0.9733\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.1207 - val_acc: 0.9728\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.1204 - val_acc: 0.9733\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.1217 - val_acc: 0.9728\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0062 - acc: 0.9993 - val_loss: 0.1219 - val_acc: 0.9731\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.1241 - val_acc: 0.9724\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.1245 - val_acc: 0.9727\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 19us/step - loss: 0.0063 - acc: 0.9991 - val_loss: 0.1250 - val_acc: 0.9722\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.1249 - val_acc: 0.9728\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0057 - acc: 0.9994 - val_loss: 0.1250 - val_acc: 0.9730\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.0056 - acc: 0.9993 - val_loss: 0.1273 - val_acc: 0.9731\n",
      "\n",
      "Accuracy: 97.31%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 316us/step - loss: 0.2642 - acc: 0.9231 - val_loss: 0.1677 - val_acc: 0.9515\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1582 - acc: 0.9501 - val_loss: 0.1417 - val_acc: 0.9526\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1354 - acc: 0.9523 - val_loss: 0.1286 - val_acc: 0.9543\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1231 - acc: 0.9547 - val_loss: 0.1217 - val_acc: 0.9566\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1127 - acc: 0.9582 - val_loss: 0.1185 - val_acc: 0.9578\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1055 - acc: 0.9604 - val_loss: 0.1159 - val_acc: 0.9572\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0975 - acc: 0.9629 - val_loss: 0.1091 - val_acc: 0.9600\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0920 - acc: 0.9648 - val_loss: 0.1084 - val_acc: 0.9604\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0850 - acc: 0.9668 - val_loss: 0.1137 - val_acc: 0.9607\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0791 - acc: 0.9696 - val_loss: 0.1074 - val_acc: 0.9618\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0736 - acc: 0.9717 - val_loss: 0.1093 - val_acc: 0.9627\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0695 - acc: 0.9729 - val_loss: 0.1086 - val_acc: 0.9638\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0622 - acc: 0.9754 - val_loss: 0.1064 - val_acc: 0.9640\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0586 - acc: 0.9774 - val_loss: 0.1058 - val_acc: 0.9657\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0546 - acc: 0.9791 - val_loss: 0.1099 - val_acc: 0.9658\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0514 - acc: 0.9804 - val_loss: 0.1108 - val_acc: 0.9672\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0483 - acc: 0.9818 - val_loss: 0.1132 - val_acc: 0.9675\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0437 - acc: 0.9836 - val_loss: 0.1113 - val_acc: 0.9691\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0398 - acc: 0.9850 - val_loss: 0.1169 - val_acc: 0.9668\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0387 - acc: 0.9858 - val_loss: 0.1164 - val_acc: 0.9705\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0358 - acc: 0.9867 - val_loss: 0.1131 - val_acc: 0.9698\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0354 - acc: 0.9865 - val_loss: 0.1140 - val_acc: 0.9695\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0307 - acc: 0.9890 - val_loss: 0.1178 - val_acc: 0.9730\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0281 - acc: 0.9899 - val_loss: 0.1214 - val_acc: 0.9718\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0253 - acc: 0.9911 - val_loss: 0.1229 - val_acc: 0.9723\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0229 - acc: 0.9919 - val_loss: 0.1249 - val_acc: 0.9722\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0212 - acc: 0.9930 - val_loss: 0.1232 - val_acc: 0.9737\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.1272 - val_acc: 0.9739\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0186 - acc: 0.9936 - val_loss: 0.1341 - val_acc: 0.9735\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0188 - acc: 0.9935 - val_loss: 0.1322 - val_acc: 0.9742\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0173 - acc: 0.9942 - val_loss: 0.1399 - val_acc: 0.9746\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.1372 - val_acc: 0.9756\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.1357 - val_acc: 0.9758\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.1387 - val_acc: 0.9757\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0120 - acc: 0.9966 - val_loss: 0.1436 - val_acc: 0.9758\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0133 - acc: 0.9958 - val_loss: 0.1437 - val_acc: 0.9751\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.1438 - val_acc: 0.9753\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0114 - acc: 0.9966 - val_loss: 0.1481 - val_acc: 0.9761\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.1488 - val_acc: 0.9769\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.1500 - val_acc: 0.9767\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0076 - acc: 0.9981 - val_loss: 0.1515 - val_acc: 0.9779\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.1578 - val_acc: 0.9767\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.1566 - val_acc: 0.9765\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.1597 - val_acc: 0.9767\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1603 - val_acc: 0.9774\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.1608 - val_acc: 0.9770\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.1600 - val_acc: 0.9774\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1671 - val_acc: 0.9776\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.1636 - val_acc: 0.9776\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 0.1632 - val_acc: 0.9778\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1632 - val_acc: 0.9783\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1667 - val_acc: 0.9778\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.1673 - val_acc: 0.9780\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.1675 - val_acc: 0.9783\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.1690 - val_acc: 0.9784\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1701 - val_acc: 0.9784\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.1713 - val_acc: 0.9785\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1740 - val_acc: 0.9782\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1745 - val_acc: 0.9781\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1789 - val_acc: 0.9786\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1776 - val_acc: 0.9781\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1766 - val_acc: 0.9781\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1769 - val_acc: 0.9785\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1771 - val_acc: 0.9778\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1782 - val_acc: 0.9779\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1808 - val_acc: 0.9780\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1803 - val_acc: 0.9784\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1825 - val_acc: 0.9783\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1815 - val_acc: 0.9778\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1822 - val_acc: 0.9780\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1854 - val_acc: 0.9787\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1840 - val_acc: 0.9782\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1887 - val_acc: 0.9780\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1857 - val_acc: 0.9783\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1851 - val_acc: 0.9782\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1861 - val_acc: 0.9778\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1881 - val_acc: 0.9786\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1885 - val_acc: 0.9783\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1886 - val_acc: 0.9781\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1908 - val_acc: 0.9784\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 9.0840e-04 - acc: 0.9999 - val_loss: 0.1891 - val_acc: 0.9778\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 8.7459e-04 - acc: 0.9999 - val_loss: 0.1899 - val_acc: 0.9778\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 8.4099e-04 - acc: 0.9999 - val_loss: 0.1903 - val_acc: 0.9779\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 7.8405e-04 - acc: 0.9999 - val_loss: 0.1912 - val_acc: 0.9778\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 8.0230e-04 - acc: 0.9999 - val_loss: 0.1927 - val_acc: 0.9782\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 7.5041e-04 - acc: 0.9999 - val_loss: 0.1940 - val_acc: 0.9781\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 8.5090e-04 - acc: 0.9999 - val_loss: 0.1933 - val_acc: 0.9779\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 9.0470e-04 - acc: 0.9998 - val_loss: 0.1926 - val_acc: 0.9780\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1931 - val_acc: 0.9776\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1963 - val_acc: 0.9777\n",
      "\n",
      "Accuracy: 97.77%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.2337 - acc: 0.9300 - val_loss: 0.1553 - val_acc: 0.9515\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1433 - acc: 0.9521 - val_loss: 0.1336 - val_acc: 0.9533\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1218 - acc: 0.9554 - val_loss: 0.1194 - val_acc: 0.9566\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1077 - acc: 0.9592 - val_loss: 0.1131 - val_acc: 0.9579\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0992 - acc: 0.9625 - val_loss: 0.1058 - val_acc: 0.9602\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0885 - acc: 0.9655 - val_loss: 0.1063 - val_acc: 0.9611\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0806 - acc: 0.9684 - val_loss: 0.1014 - val_acc: 0.9636\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0715 - acc: 0.9727 - val_loss: 0.0974 - val_acc: 0.9657\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0655 - acc: 0.9745 - val_loss: 0.0956 - val_acc: 0.9661\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0588 - acc: 0.9771 - val_loss: 0.1051 - val_acc: 0.9647\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0532 - acc: 0.9797 - val_loss: 0.0990 - val_acc: 0.9694\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.0989 - val_acc: 0.9694\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0418 - acc: 0.9842 - val_loss: 0.1126 - val_acc: 0.9676\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0430 - acc: 0.9837 - val_loss: 0.1049 - val_acc: 0.9701\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0417 - acc: 0.9840 - val_loss: 0.1009 - val_acc: 0.9698\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0324 - acc: 0.9882 - val_loss: 0.1064 - val_acc: 0.9716\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0313 - acc: 0.9887 - val_loss: 0.1105 - val_acc: 0.9719\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0272 - acc: 0.9905 - val_loss: 0.1132 - val_acc: 0.9731\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0238 - acc: 0.9919 - val_loss: 0.1150 - val_acc: 0.9730\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0282 - acc: 0.9896 - val_loss: 0.1192 - val_acc: 0.9717\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0217 - acc: 0.9921 - val_loss: 0.1112 - val_acc: 0.9733\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0149 - acc: 0.9952 - val_loss: 0.1153 - val_acc: 0.9760\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.1234 - val_acc: 0.9770\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.1247 - val_acc: 0.9769\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0091 - acc: 0.9975 - val_loss: 0.1276 - val_acc: 0.9765\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.1333 - val_acc: 0.9760\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 0.1279 - val_acc: 0.9771\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.1347 - val_acc: 0.9765\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.1391 - val_acc: 0.9777\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1408 - val_acc: 0.9767\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1424 - val_acc: 0.9775\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1455 - val_acc: 0.9770\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1484 - val_acc: 0.9774\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1486 - val_acc: 0.9774\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1477 - val_acc: 0.9778\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1525 - val_acc: 0.9778\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1577 - val_acc: 0.9777\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1547 - val_acc: 0.9776\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1581 - val_acc: 0.9773\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1617 - val_acc: 0.9776\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1634 - val_acc: 0.9780\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1635 - val_acc: 0.9778\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1631 - val_acc: 0.9782\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1669 - val_acc: 0.9781\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1662 - val_acc: 0.9776\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1703 - val_acc: 0.9776\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 8.7965e-04 - acc: 0.9999 - val_loss: 0.1684 - val_acc: 0.9774\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 8.4830e-04 - acc: 0.9999 - val_loss: 0.1699 - val_acc: 0.9778\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1710 - val_acc: 0.9774\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 8.4980e-04 - acc: 0.9998 - val_loss: 0.1710 - val_acc: 0.9770\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 6.6965e-04 - acc: 0.9999 - val_loss: 0.1732 - val_acc: 0.9777\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 5.8616e-04 - acc: 1.0000 - val_loss: 0.1735 - val_acc: 0.9778\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 5.5699e-04 - acc: 0.9999 - val_loss: 0.1752 - val_acc: 0.9778\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 5.2655e-04 - acc: 1.0000 - val_loss: 0.1751 - val_acc: 0.9779\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 5.5774e-04 - acc: 0.9999 - val_loss: 0.1789 - val_acc: 0.9774\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.1733 - val_acc: 0.9776\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1750 - val_acc: 0.9772\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 8.0379e-04 - acc: 0.9999 - val_loss: 0.1748 - val_acc: 0.9774\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 7.4669e-04 - acc: 0.9999 - val_loss: 0.1788 - val_acc: 0.9774\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 6.7265e-04 - acc: 0.9999 - val_loss: 0.1812 - val_acc: 0.9772\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 5.0660e-04 - acc: 0.9999 - val_loss: 0.1801 - val_acc: 0.9774\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 4.4288e-04 - acc: 1.0000 - val_loss: 0.1803 - val_acc: 0.9778\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 6.0536e-04 - acc: 0.9999 - val_loss: 0.1805 - val_acc: 0.9776\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 4.2489e-04 - acc: 1.0000 - val_loss: 0.1822 - val_acc: 0.9777\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 4.1174e-04 - acc: 0.9999 - val_loss: 0.1824 - val_acc: 0.9778\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1842 - val_acc: 0.9769\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 9.2449e-04 - acc: 0.9998 - val_loss: 0.1850 - val_acc: 0.9774\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 6.9221e-04 - acc: 0.9999 - val_loss: 0.1857 - val_acc: 0.9774\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 5.1384e-04 - acc: 0.9999 - val_loss: 0.1867 - val_acc: 0.9774\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 3.2121e-04 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9775\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 2.8204e-04 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9775\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 2.4415e-04 - acc: 1.0000 - val_loss: 0.1867 - val_acc: 0.9774\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 2.4951e-04 - acc: 1.0000 - val_loss: 0.1870 - val_acc: 0.9775\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 2.3627e-04 - acc: 1.0000 - val_loss: 0.1868 - val_acc: 0.9776\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 2.7806e-04 - acc: 0.9999 - val_loss: 0.1881 - val_acc: 0.9774\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 2.1768e-04 - acc: 1.0000 - val_loss: 0.1892 - val_acc: 0.9776\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 1.9934e-04 - acc: 1.0000 - val_loss: 0.1891 - val_acc: 0.9776\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 1.8864e-04 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.9776\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 2.0070e-04 - acc: 1.0000 - val_loss: 0.1894 - val_acc: 0.9777\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 1.9905e-04 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.9775\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 1.6898e-04 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.9778\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 1.6007e-04 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9778\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 1.5530e-04 - acc: 1.0000 - val_loss: 0.1910 - val_acc: 0.9778\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 1.4873e-04 - acc: 1.0000 - val_loss: 0.1917 - val_acc: 0.9776\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 1.4638e-04 - acc: 1.0000 - val_loss: 0.1925 - val_acc: 0.9776\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 1.5555e-04 - acc: 1.0000 - val_loss: 0.1924 - val_acc: 0.9776\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 1.3698e-04 - acc: 1.0000 - val_loss: 0.1927 - val_acc: 0.9778\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 1s 323us/step - loss: 1.3013e-04 - acc: 1.0000 - val_loss: 0.1923 - val_acc: 0.9777\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 1.2716e-04 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.9776\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 1.2502e-04 - acc: 1.0000 - val_loss: 0.1924 - val_acc: 0.9778\n",
      "\n",
      "Accuracy: 97.78%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 212us/step - loss: 0.7200 - acc: 0.5153 - val_loss: 0.7075 - val_acc: 0.5329\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.6993 - acc: 0.5521 - val_loss: 0.6903 - val_acc: 0.5662\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.6845 - acc: 0.5825 - val_loss: 0.6774 - val_acc: 0.5920\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.6730 - acc: 0.6044 - val_loss: 0.6668 - val_acc: 0.6126\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.6631 - acc: 0.6202 - val_loss: 0.6577 - val_acc: 0.6290\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.6542 - acc: 0.6338 - val_loss: 0.6493 - val_acc: 0.6402\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.6457 - acc: 0.6472 - val_loss: 0.6410 - val_acc: 0.6536\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.6373 - acc: 0.6587 - val_loss: 0.6327 - val_acc: 0.6672\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.6289 - acc: 0.6716 - val_loss: 0.6241 - val_acc: 0.6805\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.6202 - acc: 0.6837 - val_loss: 0.6153 - val_acc: 0.6909\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.6112 - acc: 0.6962 - val_loss: 0.6065 - val_acc: 0.7038\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.6018 - acc: 0.7076 - val_loss: 0.5970 - val_acc: 0.7167\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.5919 - acc: 0.7198 - val_loss: 0.5870 - val_acc: 0.7303\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.5816 - acc: 0.7324 - val_loss: 0.5767 - val_acc: 0.7417\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.5710 - acc: 0.7458 - val_loss: 0.5661 - val_acc: 0.7583\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.5599 - acc: 0.7584 - val_loss: 0.5545 - val_acc: 0.7707\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.5483 - acc: 0.7721 - val_loss: 0.5434 - val_acc: 0.7868\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.5364 - acc: 0.7857 - val_loss: 0.5313 - val_acc: 0.8001\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.5241 - acc: 0.7986 - val_loss: 0.5193 - val_acc: 0.8131\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.5117 - acc: 0.8106 - val_loss: 0.5068 - val_acc: 0.8244\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.4989 - acc: 0.8218 - val_loss: 0.4940 - val_acc: 0.8351\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.4860 - acc: 0.8324 - val_loss: 0.4810 - val_acc: 0.8461\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.4731 - acc: 0.8468 - val_loss: 0.4684 - val_acc: 0.8586\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.4599 - acc: 0.8559 - val_loss: 0.4553 - val_acc: 0.8655\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.4471 - acc: 0.8652 - val_loss: 0.4426 - val_acc: 0.8735\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.4340 - acc: 0.8734 - val_loss: 0.4297 - val_acc: 0.8804\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.4213 - acc: 0.8808 - val_loss: 0.4175 - val_acc: 0.8874\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.4088 - acc: 0.8889 - val_loss: 0.4052 - val_acc: 0.8944\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.3966 - acc: 0.8946 - val_loss: 0.3930 - val_acc: 0.9003\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.3847 - acc: 0.9010 - val_loss: 0.3818 - val_acc: 0.9058\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.3732 - acc: 0.9066 - val_loss: 0.3706 - val_acc: 0.9098\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.3621 - acc: 0.9113 - val_loss: 0.3597 - val_acc: 0.9129\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.3514 - acc: 0.9159 - val_loss: 0.3495 - val_acc: 0.9166\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.3412 - acc: 0.9192 - val_loss: 0.3397 - val_acc: 0.9188\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.3315 - acc: 0.9219 - val_loss: 0.3304 - val_acc: 0.9216\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.3222 - acc: 0.9245 - val_loss: 0.3215 - val_acc: 0.9233\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.3133 - acc: 0.9268 - val_loss: 0.3129 - val_acc: 0.9251\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.3050 - acc: 0.9300 - val_loss: 0.3050 - val_acc: 0.9272\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.2970 - acc: 0.9320 - val_loss: 0.2973 - val_acc: 0.9285\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.2896 - acc: 0.9335 - val_loss: 0.2901 - val_acc: 0.9299\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.2825 - acc: 0.9352 - val_loss: 0.2836 - val_acc: 0.9318\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2758 - acc: 0.9366 - val_loss: 0.2770 - val_acc: 0.9327\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.2695 - acc: 0.9379 - val_loss: 0.2709 - val_acc: 0.9334\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.2636 - acc: 0.9391 - val_loss: 0.2652 - val_acc: 0.9343\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.2580 - acc: 0.9402 - val_loss: 0.2599 - val_acc: 0.9350\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.2527 - acc: 0.9412 - val_loss: 0.2549 - val_acc: 0.9355\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.2478 - acc: 0.9416 - val_loss: 0.2501 - val_acc: 0.9357\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.2431 - acc: 0.9423 - val_loss: 0.2456 - val_acc: 0.9365\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.2387 - acc: 0.9427 - val_loss: 0.2412 - val_acc: 0.9370\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.2346 - acc: 0.9433 - val_loss: 0.2373 - val_acc: 0.9374\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.2307 - acc: 0.9437 - val_loss: 0.2335 - val_acc: 0.9383\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.2270 - acc: 0.9442 - val_loss: 0.2299 - val_acc: 0.9396\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.2235 - acc: 0.9448 - val_loss: 0.2266 - val_acc: 0.9395\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 20us/step - loss: 0.2203 - acc: 0.9453 - val_loss: 0.2234 - val_acc: 0.9402\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.2172 - acc: 0.9455 - val_loss: 0.2205 - val_acc: 0.9402\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.2143 - acc: 0.9458 - val_loss: 0.2175 - val_acc: 0.9408\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.2115 - acc: 0.9462 - val_loss: 0.2148 - val_acc: 0.9411\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.2089 - acc: 0.9465 - val_loss: 0.2123 - val_acc: 0.9420\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2064 - acc: 0.9467 - val_loss: 0.2099 - val_acc: 0.9423\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.2041 - acc: 0.9468 - val_loss: 0.2076 - val_acc: 0.9428\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.2019 - acc: 0.9470 - val_loss: 0.2054 - val_acc: 0.9434\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1998 - acc: 0.9473 - val_loss: 0.2033 - val_acc: 0.9437\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1979 - acc: 0.9474 - val_loss: 0.2013 - val_acc: 0.9446\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.1960 - acc: 0.9476 - val_loss: 0.1995 - val_acc: 0.9449\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1942 - acc: 0.9479 - val_loss: 0.1977 - val_acc: 0.9451\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1925 - acc: 0.9479 - val_loss: 0.1960 - val_acc: 0.9452\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1909 - acc: 0.9480 - val_loss: 0.1944 - val_acc: 0.9456\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1894 - acc: 0.9482 - val_loss: 0.1929 - val_acc: 0.9457\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1879 - acc: 0.9482 - val_loss: 0.1914 - val_acc: 0.9463\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1865 - acc: 0.9483 - val_loss: 0.1900 - val_acc: 0.9464\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1852 - acc: 0.9484 - val_loss: 0.1886 - val_acc: 0.9465\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1839 - acc: 0.9486 - val_loss: 0.1873 - val_acc: 0.9469\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1827 - acc: 0.9486 - val_loss: 0.1861 - val_acc: 0.9470\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 22us/step - loss: 0.1815 - acc: 0.9488 - val_loss: 0.1849 - val_acc: 0.9473\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.1804 - acc: 0.9489 - val_loss: 0.1838 - val_acc: 0.9476\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1793 - acc: 0.9490 - val_loss: 0.1827 - val_acc: 0.9477\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.1783 - acc: 0.9490 - val_loss: 0.1817 - val_acc: 0.9477\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1773 - acc: 0.9491 - val_loss: 0.1807 - val_acc: 0.9478\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1764 - acc: 0.9491 - val_loss: 0.1797 - val_acc: 0.9479\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1755 - acc: 0.9491 - val_loss: 0.1787 - val_acc: 0.9483\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1746 - acc: 0.9492 - val_loss: 0.1779 - val_acc: 0.9483\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 28us/step - loss: 0.1738 - acc: 0.9492 - val_loss: 0.1770 - val_acc: 0.9482\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1730 - acc: 0.9492 - val_loss: 0.1762 - val_acc: 0.9483\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 27us/step - loss: 0.1722 - acc: 0.9493 - val_loss: 0.1754 - val_acc: 0.9485\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1715 - acc: 0.9494 - val_loss: 0.1747 - val_acc: 0.9485\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 25us/step - loss: 0.1707 - acc: 0.9494 - val_loss: 0.1739 - val_acc: 0.9486\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.1700 - acc: 0.9495 - val_loss: 0.1732 - val_acc: 0.9487\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 23us/step - loss: 0.1694 - acc: 0.9495 - val_loss: 0.1725 - val_acc: 0.9486\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 24us/step - loss: 0.1687 - acc: 0.9496 - val_loss: 0.1718 - val_acc: 0.9489\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 21us/step - loss: 0.1681 - acc: 0.9496 - val_loss: 0.1712 - val_acc: 0.9491\n",
      "\n",
      "Accuracy: 94.91%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 317us/step - loss: 0.2245 - acc: 0.9062 - val_loss: 0.1296 - val_acc: 0.9526\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.1223 - acc: 0.9545 - val_loss: 0.1158 - val_acc: 0.9558\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 124us/step - loss: 0.1074 - acc: 0.9587 - val_loss: 0.1089 - val_acc: 0.9581\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 143us/step - loss: 0.0958 - acc: 0.9631 - val_loss: 0.1043 - val_acc: 0.9587\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0852 - acc: 0.9674 - val_loss: 0.1019 - val_acc: 0.9612\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0755 - acc: 0.9718 - val_loss: 0.0966 - val_acc: 0.9633\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0673 - acc: 0.9743 - val_loss: 0.0921 - val_acc: 0.9652\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0603 - acc: 0.9778 - val_loss: 0.0893 - val_acc: 0.9683\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0537 - acc: 0.9807 - val_loss: 0.0880 - val_acc: 0.9690\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0475 - acc: 0.9834 - val_loss: 0.0870 - val_acc: 0.9698\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 175us/step - loss: 0.0430 - acc: 0.9856 - val_loss: 0.0860 - val_acc: 0.9708\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.0391 - acc: 0.9871 - val_loss: 0.0867 - val_acc: 0.9723\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 247us/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.0817 - val_acc: 0.9737\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 234us/step - loss: 0.0308 - acc: 0.9908 - val_loss: 0.0832 - val_acc: 0.9746\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 447us/step - loss: 0.0281 - acc: 0.9918 - val_loss: 0.0843 - val_acc: 0.9739\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0254 - acc: 0.9927 - val_loss: 0.0817 - val_acc: 0.9758\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0226 - acc: 0.9939 - val_loss: 0.0831 - val_acc: 0.9761\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 134us/step - loss: 0.0205 - acc: 0.9948 - val_loss: 0.0841 - val_acc: 0.9765\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0186 - acc: 0.9957 - val_loss: 0.0845 - val_acc: 0.9756\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0171 - acc: 0.9962 - val_loss: 0.0855 - val_acc: 0.9763\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0158 - acc: 0.9964 - val_loss: 0.0892 - val_acc: 0.9750\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 140us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0871 - val_acc: 0.9769\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0133 - acc: 0.9975 - val_loss: 0.0892 - val_acc: 0.9763\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0125 - acc: 0.9977 - val_loss: 0.0898 - val_acc: 0.9769\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0112 - acc: 0.9981 - val_loss: 0.0894 - val_acc: 0.9765\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0102 - acc: 0.9984 - val_loss: 0.0899 - val_acc: 0.9777\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0091 - acc: 0.9987 - val_loss: 0.0906 - val_acc: 0.9774\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0090 - acc: 0.9987 - val_loss: 0.0912 - val_acc: 0.9771\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0082 - acc: 0.9989 - val_loss: 0.0924 - val_acc: 0.9774\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.0943 - val_acc: 0.9772\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0070 - acc: 0.9993 - val_loss: 0.0937 - val_acc: 0.9773\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.0966 - val_acc: 0.9769\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0073 - acc: 0.9989 - val_loss: 0.0965 - val_acc: 0.9778\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.0065 - acc: 0.9991 - val_loss: 0.0982 - val_acc: 0.9768\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0056 - acc: 0.9995 - val_loss: 0.0976 - val_acc: 0.9774\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 0.0985 - val_acc: 0.9774\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0048 - acc: 0.9995 - val_loss: 0.1004 - val_acc: 0.9776\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.1006 - val_acc: 0.9769\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.1010 - val_acc: 0.9778\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 111us/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.1029 - val_acc: 0.9768\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 1s 194us/step - loss: 0.0040 - acc: 0.9997 - val_loss: 0.1026 - val_acc: 0.9771\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 0.1036 - val_acc: 0.9768\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 0.1032 - val_acc: 0.9775\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 156us/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.1071 - val_acc: 0.9767\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 0.1058 - val_acc: 0.9775\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 0.1076 - val_acc: 0.9774\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1088 - val_acc: 0.9770\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1088 - val_acc: 0.9769\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1088 - val_acc: 0.9776\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0024 - acc: 0.9998 - val_loss: 0.1092 - val_acc: 0.9774\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1102 - val_acc: 0.9779\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 357us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1106 - val_acc: 0.9772\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 172us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 0.1135 - val_acc: 0.9770\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.1130 - val_acc: 0.9770\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1129 - val_acc: 0.9773\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1139 - val_acc: 0.9773\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1144 - val_acc: 0.9774\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1146 - val_acc: 0.9778\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 156us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.1159 - val_acc: 0.9775\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1170 - val_acc: 0.9772\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1179 - val_acc: 0.9773\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 225us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1193 - val_acc: 0.9772\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1185 - val_acc: 0.9777\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 396us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1202 - val_acc: 0.9771\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 184us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1217 - val_acc: 0.9763\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1203 - val_acc: 0.9774\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1220 - val_acc: 0.9772\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1230 - val_acc: 0.9773\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1238 - val_acc: 0.9772\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1230 - val_acc: 0.9775\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 173us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1239 - val_acc: 0.9776\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1244 - val_acc: 0.9771\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1263 - val_acc: 0.9775\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1263 - val_acc: 0.9765\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1272 - val_acc: 0.9769\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 8.5211e-04 - acc: 1.0000 - val_loss: 0.1272 - val_acc: 0.9772\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 8.3479e-04 - acc: 1.0000 - val_loss: 0.1282 - val_acc: 0.9770\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 7.8335e-04 - acc: 1.0000 - val_loss: 0.1279 - val_acc: 0.9772\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 8.1550e-04 - acc: 1.0000 - val_loss: 0.1280 - val_acc: 0.9772\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 8.1009e-04 - acc: 1.0000 - val_loss: 0.1293 - val_acc: 0.9774\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 7.4103e-04 - acc: 1.0000 - val_loss: 0.1304 - val_acc: 0.9770\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 7.4857e-04 - acc: 1.0000 - val_loss: 0.1303 - val_acc: 0.9768\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 7.0101e-04 - acc: 1.0000 - val_loss: 0.1305 - val_acc: 0.9770\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 8.3846e-04 - acc: 0.9999 - val_loss: 0.1330 - val_acc: 0.9769\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 8.0852e-04 - acc: 0.9999 - val_loss: 0.1329 - val_acc: 0.9768\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 7.3315e-04 - acc: 1.0000 - val_loss: 0.1322 - val_acc: 0.9770\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 5.9221e-04 - acc: 1.0000 - val_loss: 0.1325 - val_acc: 0.9769\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 5.5552e-04 - acc: 1.0000 - val_loss: 0.1339 - val_acc: 0.9771\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 5.3704e-04 - acc: 1.0000 - val_loss: 0.1355 - val_acc: 0.9769\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 5.8443e-04 - acc: 1.0000 - val_loss: 0.1345 - val_acc: 0.9771\n",
      "\n",
      "Accuracy: 97.71%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 324us/step - loss: 0.2816 - acc: 0.9252 - val_loss: 0.1866 - val_acc: 0.9499\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1761 - acc: 0.9489 - val_loss: 0.1595 - val_acc: 0.9499\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1559 - acc: 0.9489 - val_loss: 0.1475 - val_acc: 0.9498\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1481 - acc: 0.9489 - val_loss: 0.1417 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1412 - acc: 0.9492 - val_loss: 0.1399 - val_acc: 0.9513\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1385 - acc: 0.9495 - val_loss: 0.1371 - val_acc: 0.9507\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1354 - acc: 0.9498 - val_loss: 0.1336 - val_acc: 0.9510\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1328 - acc: 0.9510 - val_loss: 0.1355 - val_acc: 0.9501\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1302 - acc: 0.9518 - val_loss: 0.1306 - val_acc: 0.9528\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1276 - acc: 0.9525 - val_loss: 0.1309 - val_acc: 0.9526\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1246 - acc: 0.9533 - val_loss: 0.1292 - val_acc: 0.9525\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1224 - acc: 0.9542 - val_loss: 0.1288 - val_acc: 0.9533\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1205 - acc: 0.9545 - val_loss: 0.1253 - val_acc: 0.9540\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1173 - acc: 0.9554 - val_loss: 0.1238 - val_acc: 0.9551\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1143 - acc: 0.9566 - val_loss: 0.1217 - val_acc: 0.9557\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1131 - acc: 0.9563 - val_loss: 0.1221 - val_acc: 0.9547\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1133 - acc: 0.9560 - val_loss: 0.1220 - val_acc: 0.9551\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1098 - acc: 0.9567 - val_loss: 0.1200 - val_acc: 0.9562\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1070 - acc: 0.9574 - val_loss: 0.1199 - val_acc: 0.9552\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1058 - acc: 0.9575 - val_loss: 0.1195 - val_acc: 0.9558\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1042 - acc: 0.9582 - val_loss: 0.1198 - val_acc: 0.9554\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1024 - acc: 0.9586 - val_loss: 0.1206 - val_acc: 0.9556\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1012 - acc: 0.9589 - val_loss: 0.1199 - val_acc: 0.9558\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0972 - acc: 0.9603 - val_loss: 0.1162 - val_acc: 0.9562\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0937 - acc: 0.9619 - val_loss: 0.1127 - val_acc: 0.9579\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0908 - acc: 0.9633 - val_loss: 0.1124 - val_acc: 0.9598\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0887 - acc: 0.9647 - val_loss: 0.1120 - val_acc: 0.9589\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0875 - acc: 0.9646 - val_loss: 0.1126 - val_acc: 0.9590\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0857 - acc: 0.9653 - val_loss: 0.1114 - val_acc: 0.9600\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0839 - acc: 0.9664 - val_loss: 0.1108 - val_acc: 0.9613\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0827 - acc: 0.9668 - val_loss: 0.1111 - val_acc: 0.9604\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0815 - acc: 0.9674 - val_loss: 0.1148 - val_acc: 0.9590\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0803 - acc: 0.9680 - val_loss: 0.1147 - val_acc: 0.9603\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0794 - acc: 0.9684 - val_loss: 0.1137 - val_acc: 0.9607\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0780 - acc: 0.9693 - val_loss: 0.1124 - val_acc: 0.9609\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0764 - acc: 0.9697 - val_loss: 0.1132 - val_acc: 0.9609\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0751 - acc: 0.9702 - val_loss: 0.1123 - val_acc: 0.9614\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0742 - acc: 0.9706 - val_loss: 0.1135 - val_acc: 0.9615\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0730 - acc: 0.9711 - val_loss: 0.1153 - val_acc: 0.9617\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.0722 - acc: 0.9718 - val_loss: 0.1145 - val_acc: 0.9620\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0708 - acc: 0.9724 - val_loss: 0.1147 - val_acc: 0.9618\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0702 - acc: 0.9727 - val_loss: 0.1151 - val_acc: 0.9616\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0691 - acc: 0.9732 - val_loss: 0.1146 - val_acc: 0.9624\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0686 - acc: 0.9735 - val_loss: 0.1166 - val_acc: 0.9612\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0679 - acc: 0.9734 - val_loss: 0.1170 - val_acc: 0.9618\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0675 - acc: 0.9738 - val_loss: 0.1186 - val_acc: 0.9618\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0667 - acc: 0.9746 - val_loss: 0.1168 - val_acc: 0.9621\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0660 - acc: 0.9744 - val_loss: 0.1208 - val_acc: 0.9615\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0647 - acc: 0.9749 - val_loss: 0.1191 - val_acc: 0.9629\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0643 - acc: 0.9753 - val_loss: 0.1192 - val_acc: 0.9625\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0635 - acc: 0.9759 - val_loss: 0.1204 - val_acc: 0.9625\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0625 - acc: 0.9764 - val_loss: 0.1202 - val_acc: 0.9631\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0619 - acc: 0.9766 - val_loss: 0.1205 - val_acc: 0.9630\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 160us/step - loss: 0.0611 - acc: 0.9769 - val_loss: 0.1199 - val_acc: 0.9640\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0605 - acc: 0.9776 - val_loss: 0.1222 - val_acc: 0.9622\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0598 - acc: 0.9778 - val_loss: 0.1213 - val_acc: 0.9634\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0595 - acc: 0.9778 - val_loss: 0.1242 - val_acc: 0.9622\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0588 - acc: 0.9780 - val_loss: 0.1225 - val_acc: 0.9632\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0583 - acc: 0.9785 - val_loss: 0.1255 - val_acc: 0.9636\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0578 - acc: 0.9784 - val_loss: 0.1250 - val_acc: 0.9632\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0570 - acc: 0.9791 - val_loss: 0.1251 - val_acc: 0.9631\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0568 - acc: 0.9789 - val_loss: 0.1257 - val_acc: 0.9636\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0561 - acc: 0.9792 - val_loss: 0.1257 - val_acc: 0.9645\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0560 - acc: 0.9793 - val_loss: 0.1267 - val_acc: 0.9637\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0559 - acc: 0.9792 - val_loss: 0.1271 - val_acc: 0.9640\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0553 - acc: 0.9799 - val_loss: 0.1286 - val_acc: 0.9646\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0544 - acc: 0.9803 - val_loss: 0.1288 - val_acc: 0.9648\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0539 - acc: 0.9805 - val_loss: 0.1292 - val_acc: 0.9638\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0537 - acc: 0.9807 - val_loss: 0.1308 - val_acc: 0.9639\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0532 - acc: 0.9808 - val_loss: 0.1300 - val_acc: 0.9649\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0529 - acc: 0.9809 - val_loss: 0.1301 - val_acc: 0.9646\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0526 - acc: 0.9812 - val_loss: 0.1334 - val_acc: 0.9642\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0520 - acc: 0.9809 - val_loss: 0.1313 - val_acc: 0.9648\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0518 - acc: 0.9814 - val_loss: 0.1337 - val_acc: 0.9641\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0514 - acc: 0.9815 - val_loss: 0.1342 - val_acc: 0.9645\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0509 - acc: 0.9820 - val_loss: 0.1348 - val_acc: 0.9641\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0507 - acc: 0.9820 - val_loss: 0.1360 - val_acc: 0.9645\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0501 - acc: 0.9820 - val_loss: 0.1365 - val_acc: 0.9641\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0498 - acc: 0.9824 - val_loss: 0.1355 - val_acc: 0.9647\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0495 - acc: 0.9823 - val_loss: 0.1376 - val_acc: 0.9650\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0493 - acc: 0.9827 - val_loss: 0.1375 - val_acc: 0.9649\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0492 - acc: 0.9821 - val_loss: 0.1369 - val_acc: 0.9650\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0488 - acc: 0.9827 - val_loss: 0.1382 - val_acc: 0.9648\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0483 - acc: 0.9830 - val_loss: 0.1389 - val_acc: 0.9645\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0480 - acc: 0.9831 - val_loss: 0.1398 - val_acc: 0.9648\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0477 - acc: 0.9831 - val_loss: 0.1406 - val_acc: 0.9645\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0473 - acc: 0.9830 - val_loss: 0.1410 - val_acc: 0.9655\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0472 - acc: 0.9833 - val_loss: 0.1426 - val_acc: 0.9646\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0469 - acc: 0.9833 - val_loss: 0.1409 - val_acc: 0.9656\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0465 - acc: 0.9837 - val_loss: 0.1425 - val_acc: 0.9648\n",
      "\n",
      "Accuracy: 96.48%\n",
      "\n",
      "50149\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 511us/step - loss: 0.2644 - acc: 0.9046 - val_loss: 0.1616 - val_acc: 0.9497\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1570 - acc: 0.9491 - val_loss: 0.1479 - val_acc: 0.9503\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 175us/step - loss: 0.1479 - acc: 0.9496 - val_loss: 0.1440 - val_acc: 0.9503\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.1428 - acc: 0.9506 - val_loss: 0.1401 - val_acc: 0.9507\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1394 - acc: 0.9513 - val_loss: 0.1379 - val_acc: 0.9519\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1368 - acc: 0.9523 - val_loss: 0.1367 - val_acc: 0.9525\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 176us/step - loss: 0.1345 - acc: 0.9530 - val_loss: 0.1363 - val_acc: 0.9526\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1328 - acc: 0.9539 - val_loss: 0.1343 - val_acc: 0.9528\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 109us/step - loss: 0.1313 - acc: 0.9541 - val_loss: 0.1344 - val_acc: 0.9537\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1300 - acc: 0.9547 - val_loss: 0.1334 - val_acc: 0.9535\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1289 - acc: 0.9549 - val_loss: 0.1327 - val_acc: 0.9539\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.1280 - acc: 0.9552 - val_loss: 0.1327 - val_acc: 0.9542\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1269 - acc: 0.9556 - val_loss: 0.1326 - val_acc: 0.9547\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 160us/step - loss: 0.1260 - acc: 0.9558 - val_loss: 0.1316 - val_acc: 0.9548\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1252 - acc: 0.9561 - val_loss: 0.1315 - val_acc: 0.9547\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.1246 - acc: 0.9561 - val_loss: 0.1320 - val_acc: 0.9548\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.1240 - acc: 0.9564 - val_loss: 0.1315 - val_acc: 0.9552\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 143us/step - loss: 0.1232 - acc: 0.9564 - val_loss: 0.1305 - val_acc: 0.9551\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1227 - acc: 0.9568 - val_loss: 0.1305 - val_acc: 0.9554\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1220 - acc: 0.9570 - val_loss: 0.1309 - val_acc: 0.9551\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1215 - acc: 0.9570 - val_loss: 0.1303 - val_acc: 0.9555\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1211 - acc: 0.9570 - val_loss: 0.1311 - val_acc: 0.9554\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 173us/step - loss: 0.1206 - acc: 0.9572 - val_loss: 0.1305 - val_acc: 0.9551\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 1s 364us/step - loss: 0.1202 - acc: 0.9574 - val_loss: 0.1307 - val_acc: 0.9552\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 184us/step - loss: 0.1197 - acc: 0.9574 - val_loss: 0.1301 - val_acc: 0.9552\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1194 - acc: 0.9573 - val_loss: 0.1306 - val_acc: 0.9550\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.1190 - acc: 0.9575 - val_loss: 0.1303 - val_acc: 0.9561\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.1186 - acc: 0.9577 - val_loss: 0.1305 - val_acc: 0.9552\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.1183 - acc: 0.9576 - val_loss: 0.1304 - val_acc: 0.9554\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1180 - acc: 0.9576 - val_loss: 0.1297 - val_acc: 0.9557\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 1s 187us/step - loss: 0.1177 - acc: 0.9577 - val_loss: 0.1300 - val_acc: 0.9562\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 165us/step - loss: 0.1176 - acc: 0.9578 - val_loss: 0.1298 - val_acc: 0.9557\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1171 - acc: 0.9578 - val_loss: 0.1302 - val_acc: 0.9561\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 1s 219us/step - loss: 0.1169 - acc: 0.9579 - val_loss: 0.1299 - val_acc: 0.9559\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1165 - acc: 0.9579 - val_loss: 0.1303 - val_acc: 0.9554\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1163 - acc: 0.9580 - val_loss: 0.1298 - val_acc: 0.9563\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1161 - acc: 0.9579 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1158 - acc: 0.9580 - val_loss: 0.1305 - val_acc: 0.9558\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 169us/step - loss: 0.1156 - acc: 0.9580 - val_loss: 0.1306 - val_acc: 0.9557\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.1155 - acc: 0.9580 - val_loss: 0.1307 - val_acc: 0.9551\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 156us/step - loss: 0.1152 - acc: 0.9582 - val_loss: 0.1301 - val_acc: 0.9560\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 169us/step - loss: 0.1150 - acc: 0.9582 - val_loss: 0.1301 - val_acc: 0.9564\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 134us/step - loss: 0.1148 - acc: 0.9583 - val_loss: 0.1303 - val_acc: 0.9556\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.1146 - acc: 0.9582 - val_loss: 0.1302 - val_acc: 0.9560\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.1144 - acc: 0.9584 - val_loss: 0.1301 - val_acc: 0.9558\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 163us/step - loss: 0.1142 - acc: 0.9583 - val_loss: 0.1297 - val_acc: 0.9560\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 1s 259us/step - loss: 0.1140 - acc: 0.9583 - val_loss: 0.1299 - val_acc: 0.9561\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 166us/step - loss: 0.1138 - acc: 0.9584 - val_loss: 0.1301 - val_acc: 0.9561\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 188us/step - loss: 0.1137 - acc: 0.9583 - val_loss: 0.1298 - val_acc: 0.9565\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1135 - acc: 0.9585 - val_loss: 0.1298 - val_acc: 0.9565\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.1134 - acc: 0.9585 - val_loss: 0.1304 - val_acc: 0.9554\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 118us/step - loss: 0.1132 - acc: 0.9584 - val_loss: 0.1298 - val_acc: 0.9563\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.1131 - acc: 0.9586 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 162us/step - loss: 0.1129 - acc: 0.9585 - val_loss: 0.1299 - val_acc: 0.9555\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1128 - acc: 0.9586 - val_loss: 0.1301 - val_acc: 0.9561\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 168us/step - loss: 0.1127 - acc: 0.9587 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.1125 - acc: 0.9587 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 168us/step - loss: 0.1124 - acc: 0.9587 - val_loss: 0.1301 - val_acc: 0.9560\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.1122 - acc: 0.9588 - val_loss: 0.1303 - val_acc: 0.9556\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.1121 - acc: 0.9587 - val_loss: 0.1301 - val_acc: 0.9558\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1120 - acc: 0.9588 - val_loss: 0.1302 - val_acc: 0.9561\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 191us/step - loss: 0.1119 - acc: 0.9589 - val_loss: 0.1303 - val_acc: 0.9558\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 1s 456us/step - loss: 0.1118 - acc: 0.9588 - val_loss: 0.1301 - val_acc: 0.9560\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.1117 - acc: 0.9590 - val_loss: 0.1298 - val_acc: 0.9561\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 214us/step - loss: 0.1116 - acc: 0.9589 - val_loss: 0.1302 - val_acc: 0.9562\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.1114 - acc: 0.9589 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1113 - acc: 0.9591 - val_loss: 0.1300 - val_acc: 0.9561\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.1112 - acc: 0.9591 - val_loss: 0.1303 - val_acc: 0.9561\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 218us/step - loss: 0.1111 - acc: 0.9590 - val_loss: 0.1300 - val_acc: 0.9563\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1110 - acc: 0.9591 - val_loss: 0.1298 - val_acc: 0.9563\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1109 - acc: 0.9591 - val_loss: 0.1305 - val_acc: 0.9558\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1108 - acc: 0.9591 - val_loss: 0.1301 - val_acc: 0.9561\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1107 - acc: 0.9591 - val_loss: 0.1300 - val_acc: 0.9562\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1106 - acc: 0.9592 - val_loss: 0.1301 - val_acc: 0.9561\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.1105 - acc: 0.9592 - val_loss: 0.1305 - val_acc: 0.9563\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 118us/step - loss: 0.1104 - acc: 0.9592 - val_loss: 0.1306 - val_acc: 0.9561\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 139us/step - loss: 0.1103 - acc: 0.9592 - val_loss: 0.1304 - val_acc: 0.9563\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.1103 - acc: 0.9592 - val_loss: 0.1303 - val_acc: 0.9564\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1101 - acc: 0.9591 - val_loss: 0.1303 - val_acc: 0.9564\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.1100 - acc: 0.9592 - val_loss: 0.1301 - val_acc: 0.9566\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1099 - acc: 0.9592 - val_loss: 0.1300 - val_acc: 0.9562\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.1098 - acc: 0.9593 - val_loss: 0.1302 - val_acc: 0.9564\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.1098 - acc: 0.9593 - val_loss: 0.1303 - val_acc: 0.9563\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 182us/step - loss: 0.1097 - acc: 0.9593 - val_loss: 0.1301 - val_acc: 0.9563\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.1096 - acc: 0.9593 - val_loss: 0.1302 - val_acc: 0.9563\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.1095 - acc: 0.9593 - val_loss: 0.1304 - val_acc: 0.9562\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1094 - acc: 0.9592 - val_loss: 0.1303 - val_acc: 0.9563\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 135us/step - loss: 0.1094 - acc: 0.9594 - val_loss: 0.1302 - val_acc: 0.9563\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1093 - acc: 0.9593 - val_loss: 0.1306 - val_acc: 0.9563\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1092 - acc: 0.9593 - val_loss: 0.1307 - val_acc: 0.9562\n",
      "\n",
      "Accuracy: 95.62%\n",
      "\n",
      "43571\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 485us/step - loss: 0.1977 - acc: 0.9218 - val_loss: 0.1341 - val_acc: 0.9515\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.1212 - acc: 0.9545 - val_loss: 0.1225 - val_acc: 0.9539\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 234us/step - loss: 0.1054 - acc: 0.9595 - val_loss: 0.1102 - val_acc: 0.9584\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 178us/step - loss: 0.0909 - acc: 0.9647 - val_loss: 0.1034 - val_acc: 0.9612\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 1s 249us/step - loss: 0.0794 - acc: 0.9700 - val_loss: 0.0999 - val_acc: 0.9630\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 1s 460us/step - loss: 0.0706 - acc: 0.9736 - val_loss: 0.0951 - val_acc: 0.9658\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.0612 - acc: 0.9780 - val_loss: 0.0936 - val_acc: 0.9659\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 1s 220us/step - loss: 0.0551 - acc: 0.9806 - val_loss: 0.0884 - val_acc: 0.9682\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.0489 - acc: 0.9832 - val_loss: 0.0909 - val_acc: 0.9687\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 214us/step - loss: 0.0436 - acc: 0.9853 - val_loss: 0.0893 - val_acc: 0.9700\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.0400 - acc: 0.9868 - val_loss: 0.0874 - val_acc: 0.9709\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.0353 - acc: 0.9889 - val_loss: 0.0876 - val_acc: 0.9714\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 183us/step - loss: 0.0322 - acc: 0.9902 - val_loss: 0.0876 - val_acc: 0.9729\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 179us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.0883 - val_acc: 0.9723\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 171us/step - loss: 0.0262 - acc: 0.9927 - val_loss: 0.0859 - val_acc: 0.9740\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0242 - acc: 0.9932 - val_loss: 0.0871 - val_acc: 0.9734\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 1s 235us/step - loss: 0.0221 - acc: 0.9942 - val_loss: 0.0871 - val_acc: 0.9743\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 357us/step - loss: 0.0203 - acc: 0.9950 - val_loss: 0.0879 - val_acc: 0.9748\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 174us/step - loss: 0.0186 - acc: 0.9955 - val_loss: 0.0878 - val_acc: 0.9747\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 177us/step - loss: 0.0172 - acc: 0.9963 - val_loss: 0.0890 - val_acc: 0.9755\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 179us/step - loss: 0.0158 - acc: 0.9968 - val_loss: 0.0901 - val_acc: 0.9751\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 190us/step - loss: 0.0151 - acc: 0.9967 - val_loss: 0.0905 - val_acc: 0.9758\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.0137 - acc: 0.9973 - val_loss: 0.0908 - val_acc: 0.9758\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.0126 - acc: 0.9978 - val_loss: 0.0927 - val_acc: 0.9756\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 1s 195us/step - loss: 0.0119 - acc: 0.9979 - val_loss: 0.0926 - val_acc: 0.9767\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0110 - acc: 0.9982 - val_loss: 0.0926 - val_acc: 0.9763\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 1s 345us/step - loss: 0.0104 - acc: 0.9984 - val_loss: 0.0943 - val_acc: 0.9761\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 173us/step - loss: 0.0095 - acc: 0.9986 - val_loss: 0.0946 - val_acc: 0.9759\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.0090 - acc: 0.9988 - val_loss: 0.0963 - val_acc: 0.9760\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.0085 - acc: 0.9988 - val_loss: 0.0961 - val_acc: 0.9759\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 159us/step - loss: 0.0080 - acc: 0.9990 - val_loss: 0.0982 - val_acc: 0.9760\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 1s 239us/step - loss: 0.0074 - acc: 0.9992 - val_loss: 0.0986 - val_acc: 0.9767\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 2s 790us/step - loss: 0.0071 - acc: 0.9992 - val_loss: 0.0997 - val_acc: 0.9762\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0067 - acc: 0.9994 - val_loss: 0.1005 - val_acc: 0.9762\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 251us/step - loss: 0.0063 - acc: 0.9994 - val_loss: 0.1011 - val_acc: 0.9761\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 156us/step - loss: 0.0058 - acc: 0.9996 - val_loss: 0.1034 - val_acc: 0.9768\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 268us/step - loss: 0.0056 - acc: 0.9995 - val_loss: 0.1028 - val_acc: 0.9763\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.1040 - val_acc: 0.9767\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 183us/step - loss: 0.0051 - acc: 0.9994 - val_loss: 0.1050 - val_acc: 0.9760\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 1s 228us/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.1048 - val_acc: 0.9763\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 182us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 0.1061 - val_acc: 0.9762\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 160us/step - loss: 0.0042 - acc: 0.9997 - val_loss: 0.1065 - val_acc: 0.9761\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 140us/step - loss: 0.0041 - acc: 0.9997 - val_loss: 0.1079 - val_acc: 0.9763\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.0038 - acc: 0.9997 - val_loss: 0.1086 - val_acc: 0.9770\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.0037 - acc: 0.9997 - val_loss: 0.1084 - val_acc: 0.9767\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 0.1104 - val_acc: 0.9759\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 0.1101 - val_acc: 0.9762\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.1113 - val_acc: 0.9763\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 213us/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.1117 - val_acc: 0.9763\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 204us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.1125 - val_acc: 0.9758\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 201us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.1137 - val_acc: 0.9761\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1157 - val_acc: 0.9760\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.1144 - val_acc: 0.9766\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1156 - val_acc: 0.9759\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.0023 - acc: 0.9999 - val_loss: 0.1159 - val_acc: 0.9761\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0023 - acc: 0.9999 - val_loss: 0.1177 - val_acc: 0.9762\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1183 - val_acc: 0.9756\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1183 - val_acc: 0.9762\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1193 - val_acc: 0.9758\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1198 - val_acc: 0.9758\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 1s 209us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1210 - val_acc: 0.9760\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1211 - val_acc: 0.9762\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 1s 239us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.1216 - val_acc: 0.9758\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 488us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.1228 - val_acc: 0.9756\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1235 - val_acc: 0.9756\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 1s 257us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1236 - val_acc: 0.9759\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 163us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1251 - val_acc: 0.9760\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 238us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1254 - val_acc: 0.9763\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 170us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1254 - val_acc: 0.9760\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 165us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1266 - val_acc: 0.9758\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1276 - val_acc: 0.9758\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 140us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1269 - val_acc: 0.9760\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1284 - val_acc: 0.9757\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1296 - val_acc: 0.9758\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 178us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1311 - val_acc: 0.9753\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1293 - val_acc: 0.9758\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 1s 221us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1316 - val_acc: 0.9755\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 9.4354e-04 - acc: 1.0000 - val_loss: 0.1319 - val_acc: 0.9759\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 9.3620e-04 - acc: 1.0000 - val_loss: 0.1331 - val_acc: 0.9755\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 8.8260e-04 - acc: 1.0000 - val_loss: 0.1333 - val_acc: 0.9758\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 8.4175e-04 - acc: 1.0000 - val_loss: 0.1342 - val_acc: 0.9758\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 8.2295e-04 - acc: 1.0000 - val_loss: 0.1346 - val_acc: 0.9754\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 8.1255e-04 - acc: 1.0000 - val_loss: 0.1348 - val_acc: 0.9760\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 7.8551e-04 - acc: 1.0000 - val_loss: 0.1357 - val_acc: 0.9758\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 160us/step - loss: 7.6248e-04 - acc: 1.0000 - val_loss: 0.1353 - val_acc: 0.9756\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 7.1741e-04 - acc: 1.0000 - val_loss: 0.1372 - val_acc: 0.9754\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 7.1117e-04 - acc: 1.0000 - val_loss: 0.1365 - val_acc: 0.9757\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 1s 251us/step - loss: 6.8566e-04 - acc: 1.0000 - val_loss: 0.1381 - val_acc: 0.9756\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 9.0199e-04 - acc: 0.9999 - val_loss: 0.1388 - val_acc: 0.9756\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 1s 229us/step - loss: 7.2078e-04 - acc: 1.0000 - val_loss: 0.1396 - val_acc: 0.9754\n",
      "\n",
      "Accuracy: 97.54%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 322us/step - loss: 0.2505 - acc: 0.9257 - val_loss: 0.1610 - val_acc: 0.9524\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 323us/step - loss: 0.1463 - acc: 0.9526 - val_loss: 0.1387 - val_acc: 0.9524\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1243 - acc: 0.9556 - val_loss: 0.1240 - val_acc: 0.9541\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1090 - acc: 0.9596 - val_loss: 0.1134 - val_acc: 0.9576\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0988 - acc: 0.9624 - val_loss: 0.1105 - val_acc: 0.9581\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0877 - acc: 0.9663 - val_loss: 0.1075 - val_acc: 0.9585\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0788 - acc: 0.9695 - val_loss: 0.1040 - val_acc: 0.9620\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0720 - acc: 0.9723 - val_loss: 0.0998 - val_acc: 0.9652\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0644 - acc: 0.9749 - val_loss: 0.1134 - val_acc: 0.9614\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0605 - acc: 0.9766 - val_loss: 0.1035 - val_acc: 0.9643\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0554 - acc: 0.9790 - val_loss: 0.0987 - val_acc: 0.9674\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0471 - acc: 0.9821 - val_loss: 0.1001 - val_acc: 0.9694\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0424 - acc: 0.9842 - val_loss: 0.1083 - val_acc: 0.9685\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.9857 - val_loss: 0.1024 - val_acc: 0.9701\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0337 - acc: 0.9880 - val_loss: 0.1096 - val_acc: 0.9719\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0315 - acc: 0.9883 - val_loss: 0.1149 - val_acc: 0.9707\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0294 - acc: 0.9893 - val_loss: 0.1131 - val_acc: 0.9715\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0278 - acc: 0.9902 - val_loss: 0.1113 - val_acc: 0.9733\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0220 - acc: 0.9925 - val_loss: 0.1174 - val_acc: 0.9724\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.1209 - val_acc: 0.9750\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 0.1195 - val_acc: 0.9752\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.1227 - val_acc: 0.9755\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0120 - acc: 0.9964 - val_loss: 0.1337 - val_acc: 0.9734\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0183 - acc: 0.9933 - val_loss: 0.1376 - val_acc: 0.9722\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.1401 - val_acc: 0.9728\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0105 - acc: 0.9969 - val_loss: 0.1444 - val_acc: 0.9755\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.1401 - val_acc: 0.9755\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0057 - acc: 0.9987 - val_loss: 0.1458 - val_acc: 0.9757\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.1490 - val_acc: 0.9767\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1540 - val_acc: 0.9758\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1514 - val_acc: 0.9755\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1523 - val_acc: 0.9756\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1565 - val_acc: 0.9758\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1600 - val_acc: 0.9766\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1625 - val_acc: 0.9757\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1650 - val_acc: 0.9752\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1655 - val_acc: 0.9755\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1646 - val_acc: 0.9762\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1685 - val_acc: 0.9758\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1694 - val_acc: 0.9767\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1777 - val_acc: 0.9765\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1683 - val_acc: 0.9763\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1714 - val_acc: 0.9762\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1761 - val_acc: 0.9767\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1765 - val_acc: 0.9762\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1795 - val_acc: 0.9760\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1774 - val_acc: 0.9761\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 9.9710e-04 - acc: 0.9999 - val_loss: 0.1798 - val_acc: 0.9762\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1835 - val_acc: 0.9760\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1825 - val_acc: 0.9763\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 7.9761e-04 - acc: 0.9999 - val_loss: 0.1842 - val_acc: 0.9762\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 6.0839e-04 - acc: 0.9999 - val_loss: 0.1839 - val_acc: 0.9758\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 6.4779e-04 - acc: 0.9999 - val_loss: 0.1862 - val_acc: 0.9763\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 5.3988e-04 - acc: 1.0000 - val_loss: 0.1872 - val_acc: 0.9763\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 5.6176e-04 - acc: 0.9999 - val_loss: 0.1867 - val_acc: 0.9760\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 5.0861e-04 - acc: 0.9999 - val_loss: 0.1897 - val_acc: 0.9758\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 6.4295e-04 - acc: 0.9999 - val_loss: 0.1883 - val_acc: 0.9760\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 5.0903e-04 - acc: 0.9999 - val_loss: 0.1871 - val_acc: 0.9760\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 4.1113e-04 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.9760\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.0014 - acc: 0.9997    - 0s 48us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1947 - val_acc: 0.9761\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1880 - val_acc: 0.9754\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1904 - val_acc: 0.9753\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 7.2508e-04 - acc: 0.9999 - val_loss: 0.1915 - val_acc: 0.9763\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 5.0230e-04 - acc: 0.9999 - val_loss: 0.1898 - val_acc: 0.9760\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 4.0478e-04 - acc: 1.0000 - val_loss: 0.1908 - val_acc: 0.9760\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 2.8822e-04 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 0.9762\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 2.8659e-04 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9760\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 2.5565e-04 - acc: 1.0000 - val_loss: 0.1925 - val_acc: 0.9762\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 2.8029e-04 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.9761\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 2.4152e-04 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.9763\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 2.3092e-04 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.9760\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 2.2501e-04 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9764\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 2.0794e-04 - acc: 1.0000 - val_loss: 0.1959 - val_acc: 0.9762\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 1.9448e-04 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.9763\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 1.8525e-04 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 0.9763\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 1.7511e-04 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9764\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 1.9659e-04 - acc: 1.0000 - val_loss: 0.1969 - val_acc: 0.9763\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 8.5943e-04 - acc: 0.9998 - val_loss: 0.1946 - val_acc: 0.9752\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.1921 - val_acc: 0.9741\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.1988 - val_acc: 0.9728\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.1918 - val_acc: 0.9751\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1931 - val_acc: 0.9745\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.1953 - val_acc: 0.9740\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1923 - val_acc: 0.9749\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1937 - val_acc: 0.9749\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 9.9931e-04 - acc: 0.9999 - val_loss: 0.1938 - val_acc: 0.9753\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1943 - val_acc: 0.9755\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 8.0793e-04 - acc: 0.9999 - val_loss: 0.1928 - val_acc: 0.9761\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 4.4947e-04 - acc: 0.9999 - val_loss: 0.1935 - val_acc: 0.9756\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 9.1417e-04 - acc: 0.9998 - val_loss: 0.1958 - val_acc: 0.9745\n",
      "\n",
      "Accuracy: 97.45%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 2s 576us/step - loss: 0.1913 - acc: 0.9432 - val_loss: 0.1459 - val_acc: 0.9485\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1261 - acc: 0.9550 - val_loss: 0.1228 - val_acc: 0.9565\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1092 - acc: 0.9593 - val_loss: 0.1136 - val_acc: 0.9579\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0993 - acc: 0.9621 - val_loss: 0.1171 - val_acc: 0.9583\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 175us/step - loss: 0.0898 - acc: 0.9653 - val_loss: 0.1095 - val_acc: 0.9606\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0857 - acc: 0.9668 - val_loss: 0.1125 - val_acc: 0.9587\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0772 - acc: 0.9695 - val_loss: 0.1071 - val_acc: 0.9641\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 132us/step - loss: 0.0690 - acc: 0.9738 - val_loss: 0.1114 - val_acc: 0.9639\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.0637 - acc: 0.9754 - val_loss: 0.1084 - val_acc: 0.9659\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0614 - acc: 0.9768 - val_loss: 0.1092 - val_acc: 0.9622\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0565 - acc: 0.9782 - val_loss: 0.1117 - val_acc: 0.9667\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0512 - acc: 0.9807 - val_loss: 0.1183 - val_acc: 0.9671\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0502 - acc: 0.9813 - val_loss: 0.1185 - val_acc: 0.9662\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0444 - acc: 0.9833 - val_loss: 0.1256 - val_acc: 0.9673\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0399 - acc: 0.9853 - val_loss: 0.1249 - val_acc: 0.9684\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0381 - acc: 0.9858 - val_loss: 0.1215 - val_acc: 0.9698\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0409 - acc: 0.9854 - val_loss: 0.1351 - val_acc: 0.9687\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0380 - acc: 0.9863 - val_loss: 0.1333 - val_acc: 0.9711\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0300 - acc: 0.9894 - val_loss: 0.1305 - val_acc: 0.9724\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 113us/step - loss: 0.0291 - acc: 0.9899 - val_loss: 0.1415 - val_acc: 0.9680\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0286 - acc: 0.9898 - val_loss: 0.1384 - val_acc: 0.9721\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0271 - acc: 0.9905 - val_loss: 0.1368 - val_acc: 0.9735\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0271 - acc: 0.9904 - val_loss: 0.1434 - val_acc: 0.9723\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0241 - acc: 0.9915 - val_loss: 0.1456 - val_acc: 0.9732\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0205 - acc: 0.9930 - val_loss: 0.1540 - val_acc: 0.9725\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0280 - acc: 0.9908 - val_loss: 0.1441 - val_acc: 0.9708\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0252 - acc: 0.9915 - val_loss: 0.1590 - val_acc: 0.9723\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0215 - acc: 0.9927 - val_loss: 0.1532 - val_acc: 0.9716\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0147 - acc: 0.9948 - val_loss: 0.1493 - val_acc: 0.9739\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.1562 - val_acc: 0.9742\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.1638 - val_acc: 0.9746\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.1650 - val_acc: 0.9717\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 122us/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.1724 - val_acc: 0.9725\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0225 - acc: 0.9933 - val_loss: 0.1875 - val_acc: 0.9723\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.1651 - val_acc: 0.9726\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.1761 - val_acc: 0.9717\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.1726 - val_acc: 0.9744\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.1749 - val_acc: 0.9747\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.1699 - val_acc: 0.9742\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.1767 - val_acc: 0.9756\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.1695 - val_acc: 0.9756\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.1738 - val_acc: 0.9758\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.998 - 0s 66us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.1713 - val_acc: 0.9769\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.1752 - val_acc: 0.9772\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.1840 - val_acc: 0.9763\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.1805 - val_acc: 0.9758\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1896 - val_acc: 0.9763\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1906 - val_acc: 0.9752\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1848 - val_acc: 0.9753\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0156 - acc: 0.9955 - val_loss: 0.2035 - val_acc: 0.9714\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0222 - acc: 0.9935 - val_loss: 0.1978 - val_acc: 0.9704\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0338 - acc: 0.9909 - val_loss: 0.2000 - val_acc: 0.9695\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0255 - acc: 0.9927 - val_loss: 0.1962 - val_acc: 0.9724\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.0166 - acc: 0.9948 - val_loss: 0.1783 - val_acc: 0.9765\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1877 - val_acc: 0.9771\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.1903 - val_acc: 0.9753\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0091 - acc: 0.9971 - val_loss: 0.1870 - val_acc: 0.9763\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1942 - val_acc: 0.9762\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 135us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1921 - val_acc: 0.9746\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.1967 - val_acc: 0.9758\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.2073 - val_acc: 0.9742\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.0104 - acc: 0.9971 - val_loss: 0.2069 - val_acc: 0.9745\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.2062 - val_acc: 0.9762\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 0.2032 - val_acc: 0.9748\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 431us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.2017 - val_acc: 0.9763\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.1962 - val_acc: 0.9764\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.2031 - val_acc: 0.9769\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2048 - val_acc: 0.9765\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.2042 - val_acc: 0.9770\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.2061 - val_acc: 0.9772\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.2055 - val_acc: 0.9771\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.2110 - val_acc: 0.9763\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 102us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.2058 - val_acc: 0.9773\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.2130 - val_acc: 0.9769\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.2087 - val_acc: 0.9758\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.2190 - val_acc: 0.9747\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.2183 - val_acc: 0.9742\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0276 - acc: 0.9931 - val_loss: 0.2259 - val_acc: 0.9724\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0221 - acc: 0.9944 - val_loss: 0.2249 - val_acc: 0.9722\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0182 - acc: 0.9955 - val_loss: 0.2258 - val_acc: 0.9743\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.2270 - val_acc: 0.9747\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.2249 - val_acc: 0.9757\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.2241 - val_acc: 0.9744\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.2263 - val_acc: 0.9758\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.2217 - val_acc: 0.9755\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 154us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.2271 - val_acc: 0.9754\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.2282 - val_acc: 0.9756\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.2292 - val_acc: 0.9761\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.2214 - val_acc: 0.9764\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.2312 - val_acc: 0.9768\n",
      "\n",
      "Accuracy: 97.68%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 2s 767us/step - loss: 0.1696 - acc: 0.9395 - val_loss: 0.1453 - val_acc: 0.9503\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 537us/step - loss: 0.1432 - acc: 0.9504 - val_loss: 0.1403 - val_acc: 0.9483\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 441us/step - loss: 0.1363 - acc: 0.9506 - val_loss: 0.1346 - val_acc: 0.9523\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 2s 805us/step - loss: 0.1324 - acc: 0.9513 - val_loss: 0.1329 - val_acc: 0.9522\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 2s 646us/step - loss: 0.1272 - acc: 0.9531 - val_loss: 0.1313 - val_acc: 0.9519\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 2s 660us/step - loss: 0.1235 - acc: 0.9540 - val_loss: 0.1268 - val_acc: 0.9539\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 514us/step - loss: 0.1194 - acc: 0.9552 - val_loss: 0.1259 - val_acc: 0.9550\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 1s 478us/step - loss: 0.1163 - acc: 0.9562 - val_loss: 0.1240 - val_acc: 0.9545\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 1s 470us/step - loss: 0.1141 - acc: 0.9575 - val_loss: 0.1247 - val_acc: 0.9551\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 458us/step - loss: 0.1107 - acc: 0.9588 - val_loss: 0.1220 - val_acc: 0.9563\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 1s 534us/step - loss: 0.1082 - acc: 0.9594 - val_loss: 0.1219 - val_acc: 0.9567\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 552us/step - loss: 0.1059 - acc: 0.9610 - val_loss: 0.1207 - val_acc: 0.9565\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 475us/step - loss: 0.1039 - acc: 0.9612 - val_loss: 0.1203 - val_acc: 0.9571\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 456us/step - loss: 0.1022 - acc: 0.9621 - val_loss: 0.1198 - val_acc: 0.9572\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 2s 884us/step - loss: 0.1004 - acc: 0.9633 - val_loss: 0.1203 - val_acc: 0.9578\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 536us/step - loss: 0.0991 - acc: 0.9636 - val_loss: 0.1211 - val_acc: 0.9575\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 2s 573us/step - loss: 0.0977 - acc: 0.9648 - val_loss: 0.1216 - val_acc: 0.9570\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 496us/step - loss: 0.0961 - acc: 0.9650 - val_loss: 0.1199 - val_acc: 0.9576\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 1s 500us/step - loss: 0.0946 - acc: 0.9654 - val_loss: 0.1204 - val_acc: 0.9592\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 441us/step - loss: 0.0930 - acc: 0.9664 - val_loss: 0.1211 - val_acc: 0.9581\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 1s 468us/step - loss: 0.0917 - acc: 0.9672 - val_loss: 0.1205 - val_acc: 0.9586\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 486us/step - loss: 0.0905 - acc: 0.9679 - val_loss: 0.1197 - val_acc: 0.9591\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 1s 516us/step - loss: 0.0891 - acc: 0.9685 - val_loss: 0.1199 - val_acc: 0.9599\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 1s 477us/step - loss: 0.0880 - acc: 0.9689 - val_loss: 0.1209 - val_acc: 0.9597\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 1s 513us/step - loss: 0.0868 - acc: 0.9695 - val_loss: 0.1202 - val_acc: 0.9602\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 2s 772us/step - loss: 0.0858 - acc: 0.9703 - val_loss: 0.1202 - val_acc: 0.9607\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 2s 640us/step - loss: 0.0847 - acc: 0.9706 - val_loss: 0.1204 - val_acc: 0.9602\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 2s 732us/step - loss: 0.0835 - acc: 0.9712 - val_loss: 0.1212 - val_acc: 0.9600\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 2s 587us/step - loss: 0.0825 - acc: 0.9713 - val_loss: 0.1208 - val_acc: 0.9608\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 2s 690us/step - loss: 0.0814 - acc: 0.9722 - val_loss: 0.1199 - val_acc: 0.9610\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 1s 492us/step - loss: 0.0802 - acc: 0.9725 - val_loss: 0.1217 - val_acc: 0.9617\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 1s 521us/step - loss: 0.0795 - acc: 0.9728 - val_loss: 0.1212 - val_acc: 0.9622\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 2s 597us/step - loss: 0.0786 - acc: 0.9730 - val_loss: 0.1216 - val_acc: 0.9623\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 2s 622us/step - loss: 0.0776 - acc: 0.9736 - val_loss: 0.1219 - val_acc: 0.9618\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 2s 601us/step - loss: 0.0767 - acc: 0.9739 - val_loss: 0.1227 - val_acc: 0.9613\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 2s 886us/step - loss: 0.0760 - acc: 0.9742 - val_loss: 0.1231 - val_acc: 0.9616\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 2s 582us/step - loss: 0.0752 - acc: 0.9743 - val_loss: 0.1241 - val_acc: 0.9620\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 491us/step - loss: 0.0744 - acc: 0.9749 - val_loss: 0.1233 - val_acc: 0.9616\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 2s 640us/step - loss: 0.0738 - acc: 0.9747 - val_loss: 0.1238 - val_acc: 0.9614\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 2s 608us/step - loss: 0.0731 - acc: 0.9751 - val_loss: 0.1228 - val_acc: 0.9620\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 2s 581us/step - loss: 0.0723 - acc: 0.9756 - val_loss: 0.1227 - val_acc: 0.9626\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 2s 572us/step - loss: 0.0714 - acc: 0.9760 - val_loss: 0.1236 - val_acc: 0.9625\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 2s 578us/step - loss: 0.0706 - acc: 0.9759 - val_loss: 0.1228 - val_acc: 0.9618\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 1s 537us/step - loss: 0.0699 - acc: 0.9760 - val_loss: 0.1227 - val_acc: 0.9627\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 1s 500us/step - loss: 0.0692 - acc: 0.9763 - val_loss: 0.1231 - val_acc: 0.9627\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 2s 854us/step - loss: 0.0684 - acc: 0.9763 - val_loss: 0.1234 - val_acc: 0.9621\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 2s 607us/step - loss: 0.0676 - acc: 0.9766 - val_loss: 0.1242 - val_acc: 0.9622\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 1s 527us/step - loss: 0.0671 - acc: 0.9771 - val_loss: 0.1237 - val_acc: 0.9630\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 529us/step - loss: 0.0664 - acc: 0.9772 - val_loss: 0.1238 - val_acc: 0.9626\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 507us/step - loss: 0.0658 - acc: 0.9774 - val_loss: 0.1233 - val_acc: 0.9634\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 2s 635us/step - loss: 0.0651 - acc: 0.9780 - val_loss: 0.1244 - val_acc: 0.9636\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 443us/step - loss: 0.0647 - acc: 0.9782 - val_loss: 0.1248 - val_acc: 0.9634\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 2s 681us/step - loss: 0.0641 - acc: 0.9786 - val_loss: 0.1251 - val_acc: 0.9634\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 2s 630us/step - loss: 0.0634 - acc: 0.9788 - val_loss: 0.1245 - val_acc: 0.9637\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 2s 760us/step - loss: 0.0627 - acc: 0.9793 - val_loss: 0.1238 - val_acc: 0.9648\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 2s 889us/step - loss: 0.0621 - acc: 0.9794 - val_loss: 0.1239 - val_acc: 0.9640\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 2s 828us/step - loss: 0.0614 - acc: 0.9801 - val_loss: 0.1238 - val_acc: 0.9646\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 2s 563us/step - loss: 0.0607 - acc: 0.9801 - val_loss: 0.1243 - val_acc: 0.9650\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 1s 515us/step - loss: 0.0601 - acc: 0.9804 - val_loss: 0.1245 - val_acc: 0.9645\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 482us/step - loss: 0.0595 - acc: 0.9807 - val_loss: 0.1244 - val_acc: 0.9650\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 2s 607us/step - loss: 0.0588 - acc: 0.9808 - val_loss: 0.1247 - val_acc: 0.9649\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 513us/step - loss: 0.0582 - acc: 0.9810 - val_loss: 0.1257 - val_acc: 0.9648\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 2s 698us/step - loss: 0.0577 - acc: 0.9814 - val_loss: 0.1255 - val_acc: 0.9643\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 494us/step - loss: 0.0572 - acc: 0.9816 - val_loss: 0.1254 - val_acc: 0.9644\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 2s 918us/step - loss: 0.0568 - acc: 0.9817 - val_loss: 0.1256 - val_acc: 0.9651\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 2s 637us/step - loss: 0.0563 - acc: 0.9819 - val_loss: 0.1255 - val_acc: 0.9654\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 2s 673us/step - loss: 0.0558 - acc: 0.9819 - val_loss: 0.1263 - val_acc: 0.9651\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 483us/step - loss: 0.0553 - acc: 0.9822 - val_loss: 0.1265 - val_acc: 0.9651\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 2s 650us/step - loss: 0.0549 - acc: 0.9822 - val_loss: 0.1262 - val_acc: 0.9652\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 1s 521us/step - loss: 0.0544 - acc: 0.9826 - val_loss: 0.1275 - val_acc: 0.9650\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 1s 491us/step - loss: 0.0540 - acc: 0.9827 - val_loss: 0.1270 - val_acc: 0.9654\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 1s 466us/step - loss: 0.0537 - acc: 0.9829 - val_loss: 0.1274 - val_acc: 0.9649\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 1s 521us/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.1271 - val_acc: 0.9654\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 1s 541us/step - loss: 0.0529 - acc: 0.9830 - val_loss: 0.1276 - val_acc: 0.9658\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 3s 961us/step - loss: 0.0525 - acc: 0.9837 - val_loss: 0.1276 - val_acc: 0.9657\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 2s 708us/step - loss: 0.0521 - acc: 0.9836 - val_loss: 0.1276 - val_acc: 0.9655\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 2s 639us/step - loss: 0.0517 - acc: 0.9838 - val_loss: 0.1280 - val_acc: 0.9659\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 2s 653us/step - loss: 0.0514 - acc: 0.9840 - val_loss: 0.1286 - val_acc: 0.9657\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 2s 580us/step - loss: 0.0511 - acc: 0.9840 - val_loss: 0.1288 - val_acc: 0.9663\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 1s 519us/step - loss: 0.0506 - acc: 0.9842 - val_loss: 0.1286 - val_acc: 0.9656\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 1s 468us/step - loss: 0.0502 - acc: 0.9841 - val_loss: 0.1291 - val_acc: 0.9657\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 1s 464us/step - loss: 0.0499 - acc: 0.9845 - val_loss: 0.1291 - val_acc: 0.9659\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 2s 560us/step - loss: 0.0496 - acc: 0.9846 - val_loss: 0.1289 - val_acc: 0.9664\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 2s 735us/step - loss: 0.0493 - acc: 0.9847 - val_loss: 0.1306 - val_acc: 0.9661\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0490 - acc: 0.9850 - val_loss: 0.1304 - val_acc: 0.9662\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 2s 626us/step - loss: 0.0486 - acc: 0.9850 - val_loss: 0.1314 - val_acc: 0.9665\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 540us/step - loss: 0.0482 - acc: 0.9852 - val_loss: 0.1310 - val_acc: 0.9660\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 1s 505us/step - loss: 0.0478 - acc: 0.9853 - val_loss: 0.1313 - val_acc: 0.9663\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 2s 914us/step - loss: 0.0474 - acc: 0.9855 - val_loss: 0.1321 - val_acc: 0.9661\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 2s 767us/step - loss: 0.0471 - acc: 0.9857 - val_loss: 0.1321 - val_acc: 0.9664\n",
      "\n",
      "Accuracy: 96.64%\n",
      "\n",
      "50149\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 313us/step - loss: 0.6673 - acc: 0.6345 - val_loss: 0.6363 - val_acc: 0.7348\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.6004 - acc: 0.7897 - val_loss: 0.5553 - val_acc: 0.8496\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.5072 - acc: 0.8941 - val_loss: 0.4515 - val_acc: 0.9315\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.4050 - acc: 0.9432 - val_loss: 0.3550 - val_acc: 0.9495\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.3241 - acc: 0.9489 - val_loss: 0.2893 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.2739 - acc: 0.9489 - val_loss: 0.2530 - val_acc: 0.9499\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.2465 - acc: 0.9489 - val_loss: 0.2327 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.2308 - acc: 0.9489 - val_loss: 0.2204 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.2209 - acc: 0.9489 - val_loss: 0.2124 - val_acc: 0.9499\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.2141 - acc: 0.9489 - val_loss: 0.2068 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.2091 - acc: 0.9489 - val_loss: 0.2026 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.2054 - acc: 0.9489 - val_loss: 0.1994 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.2023 - acc: 0.9489 - val_loss: 0.1968 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1999 - acc: 0.9490 - val_loss: 0.1945 - val_acc: 0.9499\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1977 - acc: 0.9490 - val_loss: 0.1927 - val_acc: 0.9499\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1958 - acc: 0.9490 - val_loss: 0.1911 - val_acc: 0.9501\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.1942 - acc: 0.9490 - val_loss: 0.1896 - val_acc: 0.9501\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.1927 - acc: 0.9491 - val_loss: 0.1883 - val_acc: 0.9502\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1913 - acc: 0.9492 - val_loss: 0.1871 - val_acc: 0.9502\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1901 - acc: 0.9494 - val_loss: 0.1860 - val_acc: 0.9505\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1890 - acc: 0.9495 - val_loss: 0.1850 - val_acc: 0.9506\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1879 - acc: 0.9496 - val_loss: 0.1840 - val_acc: 0.9506\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1869 - acc: 0.9497 - val_loss: 0.1833 - val_acc: 0.9508\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1860 - acc: 0.9499 - val_loss: 0.1824 - val_acc: 0.9509\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1851 - acc: 0.9499 - val_loss: 0.1817 - val_acc: 0.9511\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.1844 - acc: 0.9500 - val_loss: 0.1810 - val_acc: 0.9512\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1836 - acc: 0.9501 - val_loss: 0.1803 - val_acc: 0.9512\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1828 - acc: 0.9503 - val_loss: 0.1797 - val_acc: 0.9510\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1822 - acc: 0.9503 - val_loss: 0.1791 - val_acc: 0.9510\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1815 - acc: 0.9504 - val_loss: 0.1785 - val_acc: 0.9510\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1809 - acc: 0.9504 - val_loss: 0.1780 - val_acc: 0.9509\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1803 - acc: 0.9504 - val_loss: 0.1775 - val_acc: 0.9512\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1797 - acc: 0.9504 - val_loss: 0.1770 - val_acc: 0.9511\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1791 - acc: 0.9505 - val_loss: 0.1766 - val_acc: 0.9510\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1786 - acc: 0.9506 - val_loss: 0.1761 - val_acc: 0.9511\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.1781 - acc: 0.9506 - val_loss: 0.1757 - val_acc: 0.9511\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1776 - acc: 0.9507 - val_loss: 0.1752 - val_acc: 0.9510\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1771 - acc: 0.9507 - val_loss: 0.1748 - val_acc: 0.9510\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1766 - acc: 0.9507 - val_loss: 0.1744 - val_acc: 0.9511\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1761 - acc: 0.9507 - val_loss: 0.1740 - val_acc: 0.9510\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1757 - acc: 0.9508 - val_loss: 0.1736 - val_acc: 0.9512\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1752 - acc: 0.9508 - val_loss: 0.1732 - val_acc: 0.9513\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1748 - acc: 0.9508 - val_loss: 0.1729 - val_acc: 0.9515\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.1744 - acc: 0.9508 - val_loss: 0.1725 - val_acc: 0.9515\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1740 - acc: 0.9508 - val_loss: 0.1722 - val_acc: 0.9516\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1736 - acc: 0.9508 - val_loss: 0.1718 - val_acc: 0.9516\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1732 - acc: 0.9507 - val_loss: 0.1715 - val_acc: 0.9517\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1728 - acc: 0.9508 - val_loss: 0.1712 - val_acc: 0.9517\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.1724 - acc: 0.9508 - val_loss: 0.1709 - val_acc: 0.9517\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.1721 - acc: 0.9508 - val_loss: 0.1707 - val_acc: 0.9517\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1717 - acc: 0.9508 - val_loss: 0.1703 - val_acc: 0.9517\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.1714 - acc: 0.9508 - val_loss: 0.1701 - val_acc: 0.9517\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1711 - acc: 0.9508 - val_loss: 0.1698 - val_acc: 0.9517\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.1707 - acc: 0.9509 - val_loss: 0.1695 - val_acc: 0.9519\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1704 - acc: 0.9509 - val_loss: 0.1692 - val_acc: 0.9518\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1701 - acc: 0.9509 - val_loss: 0.1690 - val_acc: 0.9517\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1698 - acc: 0.9510 - val_loss: 0.1688 - val_acc: 0.9518\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1695 - acc: 0.9510 - val_loss: 0.1685 - val_acc: 0.9517\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1692 - acc: 0.9510 - val_loss: 0.1683 - val_acc: 0.9517\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1689 - acc: 0.9510 - val_loss: 0.1681 - val_acc: 0.9518\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1687 - acc: 0.9511 - val_loss: 0.1678 - val_acc: 0.9517\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1684 - acc: 0.9511 - val_loss: 0.1676 - val_acc: 0.9518\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1681 - acc: 0.9511 - val_loss: 0.1674 - val_acc: 0.9519\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1678 - acc: 0.9511 - val_loss: 0.1672 - val_acc: 0.9519\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1676 - acc: 0.9511 - val_loss: 0.1670 - val_acc: 0.9519\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1673 - acc: 0.9510 - val_loss: 0.1668 - val_acc: 0.9519\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1671 - acc: 0.9510 - val_loss: 0.1666 - val_acc: 0.9519\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1668 - acc: 0.9511 - val_loss: 0.1664 - val_acc: 0.9519\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1666 - acc: 0.9512 - val_loss: 0.1662 - val_acc: 0.9519\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1664 - acc: 0.9511 - val_loss: 0.1660 - val_acc: 0.9520\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1661 - acc: 0.9511 - val_loss: 0.1658 - val_acc: 0.9521\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1659 - acc: 0.9512 - val_loss: 0.1657 - val_acc: 0.9521\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1657 - acc: 0.9512 - val_loss: 0.1655 - val_acc: 0.9521\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1654 - acc: 0.9512 - val_loss: 0.1653 - val_acc: 0.9521\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1652 - acc: 0.9512 - val_loss: 0.1651 - val_acc: 0.9521\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1650 - acc: 0.9512 - val_loss: 0.1650 - val_acc: 0.9522\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1648 - acc: 0.9512 - val_loss: 0.1648 - val_acc: 0.9522\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1646 - acc: 0.9513 - val_loss: 0.1646 - val_acc: 0.9522\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1644 - acc: 0.9512 - val_loss: 0.1645 - val_acc: 0.9522\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1642 - acc: 0.9512 - val_loss: 0.1643 - val_acc: 0.9523\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1640 - acc: 0.9513 - val_loss: 0.1641 - val_acc: 0.9523\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1638 - acc: 0.9513 - val_loss: 0.1640 - val_acc: 0.9523\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1636 - acc: 0.9512 - val_loss: 0.1638 - val_acc: 0.9523\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1634 - acc: 0.9513 - val_loss: 0.1637 - val_acc: 0.9524\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1632 - acc: 0.9512 - val_loss: 0.1635 - val_acc: 0.9524\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1630 - acc: 0.9513 - val_loss: 0.1634 - val_acc: 0.9524\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1628 - acc: 0.9513 - val_loss: 0.1632 - val_acc: 0.9524\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1626 - acc: 0.9513 - val_loss: 0.1631 - val_acc: 0.9524\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1625 - acc: 0.9513 - val_loss: 0.1630 - val_acc: 0.9524\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1623 - acc: 0.9512 - val_loss: 0.1628 - val_acc: 0.9524\n",
      "\n",
      "Accuracy: 95.24%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 2s 761us/step - loss: 0.2043 - acc: 0.9233 - val_loss: 0.1541 - val_acc: 0.9444\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 438us/step - loss: 0.1521 - acc: 0.9466 - val_loss: 0.1394 - val_acc: 0.9500\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 453us/step - loss: 0.1381 - acc: 0.9500 - val_loss: 0.1350 - val_acc: 0.9506\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 3s 934us/step - loss: 0.1394 - acc: 0.9485 - val_loss: 0.1312 - val_acc: 0.9512\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 1s 523us/step - loss: 0.1265 - acc: 0.9522 - val_loss: 0.1345 - val_acc: 0.9482\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 1s 506us/step - loss: 0.1193 - acc: 0.9550 - val_loss: 0.1221 - val_acc: 0.9554\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 425us/step - loss: 0.1153 - acc: 0.9567 - val_loss: 0.1208 - val_acc: 0.9540\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 1s 444us/step - loss: 0.1062 - acc: 0.9596 - val_loss: 0.1229 - val_acc: 0.9554\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 1s 455us/step - loss: 0.1015 - acc: 0.9611 - val_loss: 0.1129 - val_acc: 0.9585\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 436us/step - loss: 0.0946 - acc: 0.9641 - val_loss: 0.1133 - val_acc: 0.9593\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 1s 556us/step - loss: 0.0877 - acc: 0.9671 - val_loss: 0.1135 - val_acc: 0.9582\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 430us/step - loss: 0.0822 - acc: 0.9691 - val_loss: 0.1091 - val_acc: 0.9608\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 438us/step - loss: 0.0751 - acc: 0.9716 - val_loss: 0.1046 - val_acc: 0.9624\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 2s 719us/step - loss: 0.0670 - acc: 0.9746 - val_loss: 0.1030 - val_acc: 0.9661\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 2s 899us/step - loss: 0.0606 - acc: 0.9769 - val_loss: 0.1077 - val_acc: 0.9643\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 479us/step - loss: 0.0592 - acc: 0.9774 - val_loss: 0.1067 - val_acc: 0.9647\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 2s 606us/step - loss: 0.0505 - acc: 0.9812 - val_loss: 0.1040 - val_acc: 0.9684\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 2s 599us/step - loss: 0.0461 - acc: 0.9831 - val_loss: 0.1096 - val_acc: 0.9656\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 2s 620us/step - loss: 0.0420 - acc: 0.9847 - val_loss: 0.1084 - val_acc: 0.9675\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 2s 565us/step - loss: 0.0365 - acc: 0.9869 - val_loss: 0.0977 - val_acc: 0.9714\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 1s 465us/step - loss: 0.0342 - acc: 0.9879 - val_loss: 0.1049 - val_acc: 0.9712\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 472us/step - loss: 0.0308 - acc: 0.9886 - val_loss: 0.1039 - val_acc: 0.9709\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 1s 505us/step - loss: 0.0267 - acc: 0.9906 - val_loss: 0.1012 - val_acc: 0.9737\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 2s 657us/step - loss: 0.0220 - acc: 0.9924 - val_loss: 0.1029 - val_acc: 0.9741\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.1052 - val_acc: 0.9739\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 1s 501us/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.1115 - val_acc: 0.9734\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 2s 586us/step - loss: 0.0166 - acc: 0.9945 - val_loss: 0.1086 - val_acc: 0.9733\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 1s 455us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 0.1091 - val_acc: 0.9747\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 1s 518us/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.1187 - val_acc: 0.9739\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 2s 625us/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.1215 - val_acc: 0.9737\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 1s 546us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1194 - val_acc: 0.9752\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 2s 604us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 0.1210 - val_acc: 0.9741\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 1s 535us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.1206 - val_acc: 0.9755\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 1s 498us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.1215 - val_acc: 0.9752\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 536us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1298 - val_acc: 0.9728\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 2s 881us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.1288 - val_acc: 0.9742\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 513us/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1373 - val_acc: 0.9737\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 442us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1346 - val_acc: 0.9749\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 1s 491us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1331 - val_acc: 0.9752\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 1s 472us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1346 - val_acc: 0.9759\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 2s 701us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1336 - val_acc: 0.9751\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 1s 434us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1352 - val_acc: 0.9747\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 1s 498us/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.1387 - val_acc: 0.9742\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 1s 463us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1366 - val_acc: 0.9749\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 1s 453us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1406 - val_acc: 0.9758\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.1357 - val_acc: 0.9767\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 2s 598us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1414 - val_acc: 0.9767\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 1s 515us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1450 - val_acc: 0.9756\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 428us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1454 - val_acc: 0.9752\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 2s 587us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.1522 - val_acc: 0.9751\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 486us/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.1518 - val_acc: 0.9748\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 2s 658us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1528 - val_acc: 0.9752\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 2s 604us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1496 - val_acc: 0.9753\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 491us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1529 - val_acc: 0.9752\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 490us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1491 - val_acc: 0.9759\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1502 - val_acc: 0.9753\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 1s 517us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.1527 - val_acc: 0.9755\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 2s 599us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.1507 - val_acc: 0.9758\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 1s 450us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1534 - val_acc: 0.9757\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 442us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1533 - val_acc: 0.9758\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 2s 583us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1521 - val_acc: 0.9754\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 2s 639us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1546 - val_acc: 0.9758\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 1s 441us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.1527 - val_acc: 0.9749\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 443us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1578 - val_acc: 0.9755\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 425us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1615 - val_acc: 0.9754\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 1s 433us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1589 - val_acc: 0.9751\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 2s 854us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1653 - val_acc: 0.9749\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 515us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1651 - val_acc: 0.9749\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 536us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1688 - val_acc: 0.9749\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 1s 472us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.1713 - val_acc: 0.9746\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 1s 426us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1663 - val_acc: 0.9746\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 1s 540us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1727 - val_acc: 0.9746\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 1s 430us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1686 - val_acc: 0.9746\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 1s 427us/step - loss: 9.5365e-04 - acc: 0.9997 - val_loss: 0.1723 - val_acc: 0.9747\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 1s 425us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1657 - val_acc: 0.9749\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 1s 419us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1688 - val_acc: 0.9756\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 1s 467us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1745 - val_acc: 0.9752\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 1s 502us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1711 - val_acc: 0.9745\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 2s 895us/step - loss: 8.8422e-04 - acc: 0.9997 - val_loss: 0.1709 - val_acc: 0.9748\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 1s 522us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1705 - val_acc: 0.9752\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 1s 550us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1686 - val_acc: 0.9756\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 1s 479us/step - loss: 6.0628e-04 - acc: 0.9998 - val_loss: 0.1709 - val_acc: 0.9755\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 1s 451us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.1711 - val_acc: 0.9757\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 1s 524us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1711 - val_acc: 0.9751\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 1s 428us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1703 - val_acc: 0.9748\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 1s 461us/step - loss: 5.3497e-04 - acc: 0.9999 - val_loss: 0.1762 - val_acc: 0.9751\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 452us/step - loss: 5.2057e-04 - acc: 0.9998 - val_loss: 0.1765 - val_acc: 0.9754\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 1s 465us/step - loss: 5.9526e-04 - acc: 0.9999 - val_loss: 0.1775 - val_acc: 0.9744\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 1s 459us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1793 - val_acc: 0.9749\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 1s 528us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.1778 - val_acc: 0.9750\n",
      "\n",
      "Accuracy: 97.50%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 499us/step - loss: 0.4118 - acc: 0.9086 - val_loss: 0.2689 - val_acc: 0.9499\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.2384 - acc: 0.9489 - val_loss: 0.2141 - val_acc: 0.9499\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 193us/step - loss: 0.2069 - acc: 0.9489 - val_loss: 0.1955 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 166us/step - loss: 0.1942 - acc: 0.9489 - val_loss: 0.1864 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 164us/step - loss: 0.1873 - acc: 0.9489 - val_loss: 0.1810 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 1s 193us/step - loss: 0.1831 - acc: 0.9489 - val_loss: 0.1774 - val_acc: 0.9499\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 195us/step - loss: 0.1802 - acc: 0.9489 - val_loss: 0.1749 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 156us/step - loss: 0.1781 - acc: 0.9489 - val_loss: 0.1730 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 1s 188us/step - loss: 0.1765 - acc: 0.9489 - val_loss: 0.1715 - val_acc: 0.9499\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.1752 - acc: 0.9489 - val_loss: 0.1704 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 1s 203us/step - loss: 0.1742 - acc: 0.9489 - val_loss: 0.1694 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 187us/step - loss: 0.1734 - acc: 0.9489 - val_loss: 0.1686 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 193us/step - loss: 0.1727 - acc: 0.9489 - val_loss: 0.1679 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 319us/step - loss: 0.1721 - acc: 0.9489 - val_loss: 0.1674 - val_acc: 0.9499\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 205us/step - loss: 0.1716 - acc: 0.9489 - val_loss: 0.1669 - val_acc: 0.9499\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 214us/step - loss: 0.1711 - acc: 0.9489 - val_loss: 0.1665 - val_acc: 0.9499\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 1s 238us/step - loss: 0.1707 - acc: 0.9489 - val_loss: 0.1661 - val_acc: 0.9499\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 239us/step - loss: 0.1704 - acc: 0.9489 - val_loss: 0.1658 - val_acc: 0.9499\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 1s 241us/step - loss: 0.1701 - acc: 0.9489 - val_loss: 0.1655 - val_acc: 0.9499\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 232us/step - loss: 0.1699 - acc: 0.9489 - val_loss: 0.1652 - val_acc: 0.9499\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 1s 232us/step - loss: 0.1696 - acc: 0.9489 - val_loss: 0.1650 - val_acc: 0.9499\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 221us/step - loss: 0.1694 - acc: 0.9489 - val_loss: 0.1648 - val_acc: 0.9499\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 1s 214us/step - loss: 0.1692 - acc: 0.9489 - val_loss: 0.1646 - val_acc: 0.9499\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 181us/step - loss: 0.1690 - acc: 0.9489 - val_loss: 0.1644 - val_acc: 0.9499\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.1689 - acc: 0.9489 - val_loss: 0.1643 - val_acc: 0.9499\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 1s 513us/step - loss: 0.1688 - acc: 0.9489 - val_loss: 0.1641 - val_acc: 0.9499\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 1s 232us/step - loss: 0.1686 - acc: 0.9489 - val_loss: 0.1640 - val_acc: 0.9499\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 1s 209us/step - loss: 0.1685 - acc: 0.9489 - val_loss: 0.1639 - val_acc: 0.9499\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 1s 257us/step - loss: 0.1684 - acc: 0.9489 - val_loss: 0.1638 - val_acc: 0.9499\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 1s 193us/step - loss: 0.1683 - acc: 0.9489 - val_loss: 0.1637 - val_acc: 0.9499\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 1s 197us/step - loss: 0.1682 - acc: 0.9489 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.1681 - acc: 0.9489 - val_loss: 0.1635 - val_acc: 0.9499\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 1s 207us/step - loss: 0.1680 - acc: 0.9489 - val_loss: 0.1635 - val_acc: 0.9499\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 1s 256us/step - loss: 0.1680 - acc: 0.9489 - val_loss: 0.1634 - val_acc: 0.9499\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 207us/step - loss: 0.1679 - acc: 0.9489 - val_loss: 0.1633 - val_acc: 0.9499\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 185us/step - loss: 0.1680 - acc: 0.9488 - val_loss: 0.1633 - val_acc: 0.9499\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1678 - acc: 0.9489 - val_loss: 0.1632 - val_acc: 0.9499\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 1s 191us/step - loss: 0.1677 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9499\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 1s 189us/step - loss: 0.1677 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9499\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 1s 196us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9499\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 1s 190us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 1s 198us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1630 - val_acc: 0.9499\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 1s 207us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 1s 197us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 1s 192us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 1s 197us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 184us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 214us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 188us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 1s 523us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 201us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 1s 215us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 1s 191us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 1s 201us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 199us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 1s 199us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9496\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 202us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 202us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 205us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 1s 212us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 185us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 1s 216us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 219us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 1s 204us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 180us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 177us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 182us/step - loss: 0.1667 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 1s 187us/step - loss: 0.1666 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 185us/step - loss: 0.1665 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 1s 215us/step - loss: 0.1664 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 1s 187us/step - loss: 0.1663 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 183us/step - loss: 0.1663 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 1s 246us/step - loss: 0.1661 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 1s 490us/step - loss: 0.1661 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 1s 201us/step - loss: 0.1660 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.1660 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 183us/step - loss: 0.1660 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 1s 254us/step - loss: 0.1659 - acc: 0.9489 - val_loss: 0.1621 - val_acc: 0.9499\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.1659 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 1s 192us/step - loss: 0.1659 - acc: 0.9489 - val_loss: 0.1621 - val_acc: 0.9499\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 1s 190us/step - loss: 0.1659 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 184us/step - loss: 0.1658 - acc: 0.9489 - val_loss: 0.1621 - val_acc: 0.9499\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 186us/step - loss: 0.1658 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 1s 186us/step - loss: 0.1657 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "\n",
      "Accuracy: 94.99%\n",
      "\n",
      "43571\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 322us/step - loss: 0.2525 - acc: 0.9319 - val_loss: 0.1664 - val_acc: 0.9517\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1554 - acc: 0.9502 - val_loss: 0.1438 - val_acc: 0.9505\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1398 - acc: 0.9513 - val_loss: 0.1357 - val_acc: 0.9524\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1315 - acc: 0.9523 - val_loss: 0.1304 - val_acc: 0.9526\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1250 - acc: 0.9538 - val_loss: 0.1259 - val_acc: 0.9538\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1201 - acc: 0.9549 - val_loss: 0.1261 - val_acc: 0.9543\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1168 - acc: 0.9566 - val_loss: 0.1240 - val_acc: 0.9542\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1137 - acc: 0.9573 - val_loss: 0.1231 - val_acc: 0.9561\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1113 - acc: 0.9580 - val_loss: 0.1218 - val_acc: 0.9568\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1077 - acc: 0.9596 - val_loss: 0.1177 - val_acc: 0.9587\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1046 - acc: 0.9610 - val_loss: 0.1190 - val_acc: 0.9585\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1026 - acc: 0.9619 - val_loss: 0.1155 - val_acc: 0.9602\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1001 - acc: 0.9628 - val_loss: 0.1179 - val_acc: 0.9586\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0981 - acc: 0.9635 - val_loss: 0.1143 - val_acc: 0.9602\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0947 - acc: 0.9652 - val_loss: 0.1167 - val_acc: 0.9600\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0935 - acc: 0.9655 - val_loss: 0.1169 - val_acc: 0.9597\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0916 - acc: 0.9658 - val_loss: 0.1160 - val_acc: 0.9608\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0897 - acc: 0.9666 - val_loss: 0.1163 - val_acc: 0.9617\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0883 - acc: 0.9676 - val_loss: 0.1157 - val_acc: 0.9607\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0861 - acc: 0.9680 - val_loss: 0.1152 - val_acc: 0.9614\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0850 - acc: 0.9682 - val_loss: 0.1165 - val_acc: 0.9624\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0833 - acc: 0.9689 - val_loss: 0.1165 - val_acc: 0.9620\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0823 - acc: 0.9693 - val_loss: 0.1170 - val_acc: 0.9622\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0814 - acc: 0.9697 - val_loss: 0.1164 - val_acc: 0.9621\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0804 - acc: 0.9700 - val_loss: 0.1173 - val_acc: 0.9618\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0794 - acc: 0.9700 - val_loss: 0.1160 - val_acc: 0.9629\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0787 - acc: 0.9707 - val_loss: 0.1178 - val_acc: 0.9631\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0777 - acc: 0.9709 - val_loss: 0.1191 - val_acc: 0.9618\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0778 - acc: 0.9709 - val_loss: 0.1186 - val_acc: 0.9629\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0759 - acc: 0.9717 - val_loss: 0.1186 - val_acc: 0.9630\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0748 - acc: 0.9721 - val_loss: 0.1193 - val_acc: 0.9629\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0740 - acc: 0.9726 - val_loss: 0.1195 - val_acc: 0.9638\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0736 - acc: 0.9725 - val_loss: 0.1222 - val_acc: 0.9631\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0726 - acc: 0.9732 - val_loss: 0.1246 - val_acc: 0.9627\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0723 - acc: 0.9731 - val_loss: 0.1188 - val_acc: 0.9646\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0708 - acc: 0.9736 - val_loss: 0.1212 - val_acc: 0.9642\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0705 - acc: 0.9738 - val_loss: 0.1213 - val_acc: 0.9641\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0695 - acc: 0.9741 - val_loss: 0.1220 - val_acc: 0.9645\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0689 - acc: 0.9743 - val_loss: 0.1218 - val_acc: 0.9646\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0686 - acc: 0.9746 - val_loss: 0.1237 - val_acc: 0.9642\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0682 - acc: 0.9749 - val_loss: 0.1224 - val_acc: 0.9645\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0674 - acc: 0.9751 - val_loss: 0.1236 - val_acc: 0.9640\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0673 - acc: 0.9748 - val_loss: 0.1222 - val_acc: 0.9655\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0665 - acc: 0.9753 - val_loss: 0.1248 - val_acc: 0.9648\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0662 - acc: 0.9753 - val_loss: 0.1233 - val_acc: 0.9651\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0655 - acc: 0.9757 - val_loss: 0.1265 - val_acc: 0.9648\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0650 - acc: 0.9760 - val_loss: 0.1262 - val_acc: 0.9652\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0642 - acc: 0.9763 - val_loss: 0.1269 - val_acc: 0.9652\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0638 - acc: 0.9767 - val_loss: 0.1273 - val_acc: 0.9652\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0635 - acc: 0.9765 - val_loss: 0.1257 - val_acc: 0.9662\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0632 - acc: 0.9767 - val_loss: 0.1277 - val_acc: 0.9651\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0627 - acc: 0.9767 - val_loss: 0.1260 - val_acc: 0.9654\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0623 - acc: 0.9770 - val_loss: 0.1279 - val_acc: 0.9655\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0619 - acc: 0.9770 - val_loss: 0.1286 - val_acc: 0.9653\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0620 - acc: 0.9772 - val_loss: 0.1270 - val_acc: 0.9661\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0613 - acc: 0.9774 - val_loss: 0.1298 - val_acc: 0.9655\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0614 - acc: 0.9771 - val_loss: 0.1293 - val_acc: 0.9652\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0604 - acc: 0.9776 - val_loss: 0.1303 - val_acc: 0.9656\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0597 - acc: 0.9781 - val_loss: 0.1297 - val_acc: 0.9659\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0593 - acc: 0.9781 - val_loss: 0.1295 - val_acc: 0.9662\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 1s 227us/step - loss: 0.0590 - acc: 0.9781 - val_loss: 0.1300 - val_acc: 0.9659\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.0590 - acc: 0.9782 - val_loss: 0.1305 - val_acc: 0.9657\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0584 - acc: 0.9784 - val_loss: 0.1312 - val_acc: 0.9658\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0584 - acc: 0.9785 - val_loss: 0.1347 - val_acc: 0.9649\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0580 - acc: 0.9784 - val_loss: 0.1331 - val_acc: 0.9659\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0573 - acc: 0.9790 - val_loss: 0.1347 - val_acc: 0.9658\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0571 - acc: 0.9791 - val_loss: 0.1327 - val_acc: 0.9657\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0570 - acc: 0.9789 - val_loss: 0.1344 - val_acc: 0.9657\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0568 - acc: 0.9790 - val_loss: 0.1335 - val_acc: 0.9657\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0565 - acc: 0.9791 - val_loss: 0.1360 - val_acc: 0.9653\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0560 - acc: 0.9792 - val_loss: 0.1349 - val_acc: 0.9659\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0560 - acc: 0.9793 - val_loss: 0.1347 - val_acc: 0.9664\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0557 - acc: 0.9796 - val_loss: 0.1352 - val_acc: 0.9653\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0558 - acc: 0.9796 - val_loss: 0.1353 - val_acc: 0.9659\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0550 - acc: 0.9797 - val_loss: 0.1385 - val_acc: 0.9657\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0548 - acc: 0.9797 - val_loss: 0.1381 - val_acc: 0.9659\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0547 - acc: 0.9799 - val_loss: 0.1374 - val_acc: 0.9662\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0544 - acc: 0.9798 - val_loss: 0.1373 - val_acc: 0.9665\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0539 - acc: 0.9802 - val_loss: 0.1388 - val_acc: 0.9656\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0538 - acc: 0.9801 - val_loss: 0.1390 - val_acc: 0.9652\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0539 - acc: 0.9801 - val_loss: 0.1383 - val_acc: 0.9655\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0537 - acc: 0.9802 - val_loss: 0.1388 - val_acc: 0.9652\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0533 - acc: 0.9801 - val_loss: 0.1396 - val_acc: 0.9659\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0528 - acc: 0.9807 - val_loss: 0.1391 - val_acc: 0.9662\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0528 - acc: 0.9807 - val_loss: 0.1407 - val_acc: 0.9660\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0522 - acc: 0.9807 - val_loss: 0.1408 - val_acc: 0.9664\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0521 - acc: 0.9810 - val_loss: 0.1416 - val_acc: 0.9659\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0521 - acc: 0.9808 - val_loss: 0.1439 - val_acc: 0.9659\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0518 - acc: 0.9811 - val_loss: 0.1420 - val_acc: 0.9657\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0516 - acc: 0.9810 - val_loss: 0.1421 - val_acc: 0.9662\n",
      "\n",
      "Accuracy: 96.62%\n",
      "\n",
      "50149\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 248us/step - loss: 0.6318 - acc: 0.6936 - val_loss: 0.5272 - val_acc: 0.8870\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.4466 - acc: 0.9290 - val_loss: 0.3600 - val_acc: 0.9491\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.3038 - acc: 0.9486 - val_loss: 0.2501 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.2305 - acc: 0.9491 - val_loss: 0.2088 - val_acc: 0.9508\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.2038 - acc: 0.9498 - val_loss: 0.1927 - val_acc: 0.9513\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1905 - acc: 0.9503 - val_loss: 0.1832 - val_acc: 0.9515\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.1820 - acc: 0.9505 - val_loss: 0.1767 - val_acc: 0.9518\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1754 - acc: 0.9510 - val_loss: 0.1718 - val_acc: 0.9523\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1702 - acc: 0.9513 - val_loss: 0.1679 - val_acc: 0.9526\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1658 - acc: 0.9517 - val_loss: 0.1645 - val_acc: 0.9527\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1619 - acc: 0.9519 - val_loss: 0.1619 - val_acc: 0.9531\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1585 - acc: 0.9523 - val_loss: 0.1594 - val_acc: 0.9528\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1555 - acc: 0.9524 - val_loss: 0.1573 - val_acc: 0.9533\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1528 - acc: 0.9526 - val_loss: 0.1551 - val_acc: 0.9529\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1502 - acc: 0.9529 - val_loss: 0.1533 - val_acc: 0.9533\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1478 - acc: 0.9530 - val_loss: 0.1516 - val_acc: 0.9533\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1455 - acc: 0.9531 - val_loss: 0.1501 - val_acc: 0.9535\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1434 - acc: 0.9534 - val_loss: 0.1485 - val_acc: 0.9540\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1415 - acc: 0.9537 - val_loss: 0.1471 - val_acc: 0.9540\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1397 - acc: 0.9538 - val_loss: 0.1457 - val_acc: 0.9541\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1379 - acc: 0.9540 - val_loss: 0.1445 - val_acc: 0.9542\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1363 - acc: 0.9541 - val_loss: 0.1434 - val_acc: 0.9541\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1347 - acc: 0.9543 - val_loss: 0.1421 - val_acc: 0.9538\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1330 - acc: 0.9547 - val_loss: 0.1409 - val_acc: 0.9542\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1316 - acc: 0.9550 - val_loss: 0.1399 - val_acc: 0.9540\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1303 - acc: 0.9552 - val_loss: 0.1390 - val_acc: 0.9544\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1289 - acc: 0.9554 - val_loss: 0.1381 - val_acc: 0.9544\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1276 - acc: 0.9557 - val_loss: 0.1373 - val_acc: 0.9545\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1265 - acc: 0.9558 - val_loss: 0.1362 - val_acc: 0.9544\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1253 - acc: 0.9564 - val_loss: 0.1354 - val_acc: 0.9545\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1241 - acc: 0.9563 - val_loss: 0.1346 - val_acc: 0.9548\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1230 - acc: 0.9567 - val_loss: 0.1340 - val_acc: 0.9547\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1219 - acc: 0.9569 - val_loss: 0.1331 - val_acc: 0.9551\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1208 - acc: 0.9572 - val_loss: 0.1323 - val_acc: 0.9548\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1198 - acc: 0.9577 - val_loss: 0.1317 - val_acc: 0.9548\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1188 - acc: 0.9576 - val_loss: 0.1311 - val_acc: 0.9551\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1179 - acc: 0.9579 - val_loss: 0.1304 - val_acc: 0.9551\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.1170 - acc: 0.9582 - val_loss: 0.1297 - val_acc: 0.9549\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1161 - acc: 0.9585 - val_loss: 0.1291 - val_acc: 0.9549\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1153 - acc: 0.9588 - val_loss: 0.1286 - val_acc: 0.9552\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1144 - acc: 0.9592 - val_loss: 0.1279 - val_acc: 0.9550\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1136 - acc: 0.9595 - val_loss: 0.1274 - val_acc: 0.9550\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1128 - acc: 0.9596 - val_loss: 0.1267 - val_acc: 0.9549\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1121 - acc: 0.9596 - val_loss: 0.1262 - val_acc: 0.9554\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1113 - acc: 0.9598 - val_loss: 0.1258 - val_acc: 0.9551\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1106 - acc: 0.9598 - val_loss: 0.1252 - val_acc: 0.9552\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1099 - acc: 0.9601 - val_loss: 0.1247 - val_acc: 0.9552\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1093 - acc: 0.9603 - val_loss: 0.1244 - val_acc: 0.9554\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1086 - acc: 0.9604 - val_loss: 0.1241 - val_acc: 0.9551\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1079 - acc: 0.9608 - val_loss: 0.1235 - val_acc: 0.9556\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1073 - acc: 0.9610 - val_loss: 0.1230 - val_acc: 0.9555\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1066 - acc: 0.9611 - val_loss: 0.1228 - val_acc: 0.9554\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.1061 - acc: 0.9611 - val_loss: 0.1224 - val_acc: 0.9557\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1055 - acc: 0.9612 - val_loss: 0.1221 - val_acc: 0.9558\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1049 - acc: 0.9616 - val_loss: 0.1216 - val_acc: 0.9557\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1043 - acc: 0.9617 - val_loss: 0.1214 - val_acc: 0.9560\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1037 - acc: 0.9620 - val_loss: 0.1210 - val_acc: 0.9563\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1033 - acc: 0.9621 - val_loss: 0.1206 - val_acc: 0.9565\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1026 - acc: 0.9624 - val_loss: 0.1202 - val_acc: 0.9564\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1022 - acc: 0.9625 - val_loss: 0.1199 - val_acc: 0.9565\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.1016 - acc: 0.9627 - val_loss: 0.1196 - val_acc: 0.9567\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1012 - acc: 0.9625 - val_loss: 0.1191 - val_acc: 0.9572\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1007 - acc: 0.9632 - val_loss: 0.1191 - val_acc: 0.9567\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1002 - acc: 0.9630 - val_loss: 0.1187 - val_acc: 0.9569\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0998 - acc: 0.9633 - val_loss: 0.1184 - val_acc: 0.9568\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0992 - acc: 0.9633 - val_loss: 0.1184 - val_acc: 0.9572\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0989 - acc: 0.9638 - val_loss: 0.1180 - val_acc: 0.9576\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0983 - acc: 0.9639 - val_loss: 0.1177 - val_acc: 0.9575\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0979 - acc: 0.9639 - val_loss: 0.1175 - val_acc: 0.9571\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0975 - acc: 0.9641 - val_loss: 0.1173 - val_acc: 0.9572\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0971 - acc: 0.9642 - val_loss: 0.1170 - val_acc: 0.9570\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 1s 220us/step - loss: 0.0966 - acc: 0.9644 - val_loss: 0.1168 - val_acc: 0.9570\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.0962 - acc: 0.9645 - val_loss: 0.1166 - val_acc: 0.9568\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0959 - acc: 0.9646 - val_loss: 0.1163 - val_acc: 0.9581\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0953 - acc: 0.9648 - val_loss: 0.1163 - val_acc: 0.9574\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0951 - acc: 0.9648 - val_loss: 0.1159 - val_acc: 0.9571\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0946 - acc: 0.9650 - val_loss: 0.1157 - val_acc: 0.9576\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0943 - acc: 0.9652 - val_loss: 0.1155 - val_acc: 0.9574\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0939 - acc: 0.9653 - val_loss: 0.1153 - val_acc: 0.9578\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0934 - acc: 0.9656 - val_loss: 0.1150 - val_acc: 0.9579\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0931 - acc: 0.9657 - val_loss: 0.1148 - val_acc: 0.9580\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0928 - acc: 0.9657 - val_loss: 0.1147 - val_acc: 0.9581\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0925 - acc: 0.9660 - val_loss: 0.1145 - val_acc: 0.9577\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0920 - acc: 0.9661 - val_loss: 0.1143 - val_acc: 0.9580\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0917 - acc: 0.9660 - val_loss: 0.1141 - val_acc: 0.9580\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0914 - acc: 0.9664 - val_loss: 0.1139 - val_acc: 0.9580\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0911 - acc: 0.9664 - val_loss: 0.1138 - val_acc: 0.9584\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0907 - acc: 0.9665 - val_loss: 0.1136 - val_acc: 0.9581\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.0904 - acc: 0.9667 - val_loss: 0.1134 - val_acc: 0.9581\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0900 - acc: 0.9669 - val_loss: 0.1134 - val_acc: 0.9586\n",
      "\n",
      "Accuracy: 95.86%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 229us/step - loss: 0.3170 - acc: 0.9073 - val_loss: 0.1931 - val_acc: 0.9492\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1770 - acc: 0.9505 - val_loss: 0.1634 - val_acc: 0.9529\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1516 - acc: 0.9520 - val_loss: 0.1487 - val_acc: 0.9523\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1369 - acc: 0.9540 - val_loss: 0.1391 - val_acc: 0.9535\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1265 - acc: 0.9559 - val_loss: 0.1329 - val_acc: 0.9536\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1189 - acc: 0.9576 - val_loss: 0.1283 - val_acc: 0.9537\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1128 - acc: 0.9589 - val_loss: 0.1269 - val_acc: 0.9542\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1081 - acc: 0.9607 - val_loss: 0.1217 - val_acc: 0.9551\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1036 - acc: 0.9621 - val_loss: 0.1196 - val_acc: 0.9555\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0997 - acc: 0.9636 - val_loss: 0.1160 - val_acc: 0.9579\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0956 - acc: 0.9645 - val_loss: 0.1148 - val_acc: 0.9584\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0926 - acc: 0.9659 - val_loss: 0.1131 - val_acc: 0.9583\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0898 - acc: 0.9673 - val_loss: 0.1106 - val_acc: 0.9587\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0873 - acc: 0.9675 - val_loss: 0.1100 - val_acc: 0.9597\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0846 - acc: 0.9692 - val_loss: 0.1091 - val_acc: 0.9607\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0823 - acc: 0.9698 - val_loss: 0.1086 - val_acc: 0.9606\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0802 - acc: 0.9704 - val_loss: 0.1058 - val_acc: 0.9607\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0782 - acc: 0.9714 - val_loss: 0.1049 - val_acc: 0.9609\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0759 - acc: 0.9722 - val_loss: 0.1043 - val_acc: 0.9614\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0741 - acc: 0.9729 - val_loss: 0.1049 - val_acc: 0.9620\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0729 - acc: 0.9734 - val_loss: 0.1032 - val_acc: 0.9624\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0713 - acc: 0.9738 - val_loss: 0.1029 - val_acc: 0.9614\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0691 - acc: 0.9747 - val_loss: 0.1027 - val_acc: 0.9630\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0677 - acc: 0.9753 - val_loss: 0.1009 - val_acc: 0.9627\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0663 - acc: 0.9759 - val_loss: 0.1014 - val_acc: 0.9633\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0652 - acc: 0.9764 - val_loss: 0.1009 - val_acc: 0.9647\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0637 - acc: 0.9767 - val_loss: 0.1003 - val_acc: 0.9641\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0625 - acc: 0.9775 - val_loss: 0.0991 - val_acc: 0.9650\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0612 - acc: 0.9780 - val_loss: 0.0993 - val_acc: 0.9643\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0602 - acc: 0.9788 - val_loss: 0.0989 - val_acc: 0.9648\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0594 - acc: 0.9790 - val_loss: 0.0977 - val_acc: 0.9649\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0584 - acc: 0.9792 - val_loss: 0.0981 - val_acc: 0.9659\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0570 - acc: 0.9800 - val_loss: 0.0980 - val_acc: 0.9660\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0561 - acc: 0.9803 - val_loss: 0.0975 - val_acc: 0.9657\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0554 - acc: 0.9805 - val_loss: 0.0973 - val_acc: 0.9664\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0547 - acc: 0.9810 - val_loss: 0.0962 - val_acc: 0.9668\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0536 - acc: 0.9817 - val_loss: 0.0978 - val_acc: 0.9671\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0529 - acc: 0.9814 - val_loss: 0.0966 - val_acc: 0.9673\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0521 - acc: 0.9822 - val_loss: 0.0967 - val_acc: 0.9677\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0512 - acc: 0.9822 - val_loss: 0.0968 - val_acc: 0.9673\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0504 - acc: 0.9829 - val_loss: 0.0953 - val_acc: 0.9677\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0498 - acc: 0.9828 - val_loss: 0.0965 - val_acc: 0.9678\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0492 - acc: 0.9831 - val_loss: 0.0961 - val_acc: 0.9685\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0484 - acc: 0.9835 - val_loss: 0.0950 - val_acc: 0.9692\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0479 - acc: 0.9840 - val_loss: 0.0949 - val_acc: 0.9684\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0470 - acc: 0.9840 - val_loss: 0.0956 - val_acc: 0.9684\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0463 - acc: 0.9844 - val_loss: 0.0949 - val_acc: 0.9684\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0457 - acc: 0.9845 - val_loss: 0.0945 - val_acc: 0.9692\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0453 - acc: 0.9847 - val_loss: 0.0953 - val_acc: 0.9693\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0447 - acc: 0.9849 - val_loss: 0.0951 - val_acc: 0.9690\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0443 - acc: 0.9851 - val_loss: 0.0946 - val_acc: 0.9699\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0435 - acc: 0.9854 - val_loss: 0.0943 - val_acc: 0.9700\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0432 - acc: 0.9855 - val_loss: 0.0940 - val_acc: 0.9705\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0424 - acc: 0.9861 - val_loss: 0.0943 - val_acc: 0.9695\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0419 - acc: 0.9862 - val_loss: 0.0941 - val_acc: 0.9706\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0414 - acc: 0.9862 - val_loss: 0.0944 - val_acc: 0.9696\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 26us/step - loss: 0.0411 - acc: 0.9864 - val_loss: 0.0943 - val_acc: 0.9701\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0405 - acc: 0.9868 - val_loss: 0.0946 - val_acc: 0.9700\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0400 - acc: 0.9869 - val_loss: 0.0945 - val_acc: 0.9701\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0396 - acc: 0.9870 - val_loss: 0.0952 - val_acc: 0.9699\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0394 - acc: 0.9873 - val_loss: 0.0952 - val_acc: 0.9708\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0387 - acc: 0.9873 - val_loss: 0.0953 - val_acc: 0.9707\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0383 - acc: 0.9876 - val_loss: 0.0947 - val_acc: 0.9708\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0378 - acc: 0.9876 - val_loss: 0.0945 - val_acc: 0.9709\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0373 - acc: 0.9883 - val_loss: 0.0954 - val_acc: 0.9703\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0370 - acc: 0.9883 - val_loss: 0.0942 - val_acc: 0.9704\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0369 - acc: 0.9883 - val_loss: 0.0949 - val_acc: 0.9710\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0362 - acc: 0.9885 - val_loss: 0.0951 - val_acc: 0.9704\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0358 - acc: 0.9885 - val_loss: 0.0948 - val_acc: 0.9706\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0356 - acc: 0.9887 - val_loss: 0.0949 - val_acc: 0.9710\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0352 - acc: 0.9890 - val_loss: 0.0945 - val_acc: 0.9705\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0349 - acc: 0.9890 - val_loss: 0.0945 - val_acc: 0.9712\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.0952 - val_acc: 0.9714\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0340 - acc: 0.9893 - val_loss: 0.0956 - val_acc: 0.9711\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0337 - acc: 0.9896 - val_loss: 0.0955 - val_acc: 0.9714\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0335 - acc: 0.9896 - val_loss: 0.0955 - val_acc: 0.9716\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0331 - acc: 0.9898 - val_loss: 0.0954 - val_acc: 0.9716\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0328 - acc: 0.9898 - val_loss: 0.0954 - val_acc: 0.9712\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.0957 - val_acc: 0.9717\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0323 - acc: 0.9899 - val_loss: 0.0954 - val_acc: 0.9720\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0318 - acc: 0.9904 - val_loss: 0.0951 - val_acc: 0.9716\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0316 - acc: 0.9904 - val_loss: 0.0956 - val_acc: 0.9717\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0965 - val_acc: 0.9714\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0311 - acc: 0.9905 - val_loss: 0.0962 - val_acc: 0.9718\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0308 - acc: 0.9907 - val_loss: 0.0965 - val_acc: 0.9720\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0305 - acc: 0.9907 - val_loss: 0.0967 - val_acc: 0.9718\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0303 - acc: 0.9910 - val_loss: 0.0960 - val_acc: 0.9714\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0300 - acc: 0.9911 - val_loss: 0.0965 - val_acc: 0.9721\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0298 - acc: 0.9910 - val_loss: 0.0967 - val_acc: 0.9726\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0294 - acc: 0.9914 - val_loss: 0.0960 - val_acc: 0.9722\n",
      "\n",
      "Accuracy: 97.22%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 355us/step - loss: 0.3039 - acc: 0.8672 - val_loss: 0.1573 - val_acc: 0.9494\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1516 - acc: 0.9487 - val_loss: 0.1419 - val_acc: 0.9505\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 350us/step - loss: 0.1393 - acc: 0.9502 - val_loss: 0.1369 - val_acc: 0.9504\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1317 - acc: 0.9520 - val_loss: 0.1296 - val_acc: 0.9526\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1252 - acc: 0.9535 - val_loss: 0.1258 - val_acc: 0.9533\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1210 - acc: 0.9538 - val_loss: 0.1225 - val_acc: 0.9542\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1163 - acc: 0.9555 - val_loss: 0.1197 - val_acc: 0.9554\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.1123 - acc: 0.9570 - val_loss: 0.1178 - val_acc: 0.9544\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1082 - acc: 0.9588 - val_loss: 0.1164 - val_acc: 0.9551\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1050 - acc: 0.9601 - val_loss: 0.1129 - val_acc: 0.9572\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1014 - acc: 0.9619 - val_loss: 0.1124 - val_acc: 0.9574\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0983 - acc: 0.9634 - val_loss: 0.1107 - val_acc: 0.9581\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0953 - acc: 0.9645 - val_loss: 0.1086 - val_acc: 0.9594\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 0.0924 - acc: 0.9658 - val_loss: 0.1069 - val_acc: 0.9599\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0896 - acc: 0.9666 - val_loss: 0.1057 - val_acc: 0.9597\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0873 - acc: 0.9676 - val_loss: 0.1058 - val_acc: 0.9604\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0852 - acc: 0.9686 - val_loss: 0.1038 - val_acc: 0.9614\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0827 - acc: 0.9695 - val_loss: 0.1039 - val_acc: 0.9615\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0806 - acc: 0.9706 - val_loss: 0.1021 - val_acc: 0.9630\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0786 - acc: 0.9712 - val_loss: 0.1014 - val_acc: 0.9622\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0770 - acc: 0.9721 - val_loss: 0.0997 - val_acc: 0.9631\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0749 - acc: 0.9730 - val_loss: 0.0991 - val_acc: 0.9639\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0732 - acc: 0.9737 - val_loss: 0.0985 - val_acc: 0.9646\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0713 - acc: 0.9748 - val_loss: 0.0988 - val_acc: 0.9651\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0701 - acc: 0.9755 - val_loss: 0.0963 - val_acc: 0.9659\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0681 - acc: 0.9763 - val_loss: 0.0970 - val_acc: 0.9664\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0663 - acc: 0.9769 - val_loss: 0.0962 - val_acc: 0.9662\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0651 - acc: 0.9775 - val_loss: 0.0958 - val_acc: 0.9669\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0635 - acc: 0.9784 - val_loss: 0.0961 - val_acc: 0.9675\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0628 - acc: 0.9784 - val_loss: 0.0948 - val_acc: 0.9685\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0613 - acc: 0.9792 - val_loss: 0.0948 - val_acc: 0.9681\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0599 - acc: 0.9801 - val_loss: 0.0948 - val_acc: 0.9685\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0586 - acc: 0.9806 - val_loss: 0.0941 - val_acc: 0.9683\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0578 - acc: 0.9811 - val_loss: 0.0944 - val_acc: 0.9680\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0567 - acc: 0.9813 - val_loss: 0.0944 - val_acc: 0.9700\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0554 - acc: 0.9820 - val_loss: 0.0938 - val_acc: 0.9684\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0545 - acc: 0.9824 - val_loss: 0.0940 - val_acc: 0.9688\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0533 - acc: 0.9833 - val_loss: 0.0938 - val_acc: 0.9699\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0525 - acc: 0.9833 - val_loss: 0.0932 - val_acc: 0.9697\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0513 - acc: 0.9840 - val_loss: 0.0936 - val_acc: 0.9700\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0504 - acc: 0.9846 - val_loss: 0.0936 - val_acc: 0.9697\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0495 - acc: 0.9850 - val_loss: 0.0927 - val_acc: 0.9705\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0483 - acc: 0.9851 - val_loss: 0.0941 - val_acc: 0.9701\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0475 - acc: 0.9858 - val_loss: 0.0934 - val_acc: 0.9706\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0467 - acc: 0.9859 - val_loss: 0.0927 - val_acc: 0.9710\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0458 - acc: 0.9865 - val_loss: 0.0930 - val_acc: 0.9701\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0452 - acc: 0.9868 - val_loss: 0.0931 - val_acc: 0.9706\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0446 - acc: 0.9869 - val_loss: 0.0930 - val_acc: 0.9708\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 116us/step - loss: 0.0438 - acc: 0.9872 - val_loss: 0.0932 - val_acc: 0.9710\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.0429 - acc: 0.9878 - val_loss: 0.0936 - val_acc: 0.9708\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0424 - acc: 0.9878 - val_loss: 0.0931 - val_acc: 0.9716\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0416 - acc: 0.9883 - val_loss: 0.0926 - val_acc: 0.9713\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0411 - acc: 0.9881 - val_loss: 0.0938 - val_acc: 0.9706\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0405 - acc: 0.9886 - val_loss: 0.0944 - val_acc: 0.9702\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0398 - acc: 0.9887 - val_loss: 0.0929 - val_acc: 0.9717\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0393 - acc: 0.9888 - val_loss: 0.0931 - val_acc: 0.9714\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0388 - acc: 0.9891 - val_loss: 0.0939 - val_acc: 0.9716\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0380 - acc: 0.9894 - val_loss: 0.0932 - val_acc: 0.9722\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0373 - acc: 0.9899 - val_loss: 0.0936 - val_acc: 0.9720\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0368 - acc: 0.9901 - val_loss: 0.0939 - val_acc: 0.9721\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0364 - acc: 0.9899 - val_loss: 0.0939 - val_acc: 0.9724\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0358 - acc: 0.9904 - val_loss: 0.0940 - val_acc: 0.9723\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0353 - acc: 0.9907 - val_loss: 0.0935 - val_acc: 0.9728\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0349 - acc: 0.9909 - val_loss: 0.0940 - val_acc: 0.9723\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0344 - acc: 0.9910 - val_loss: 0.0938 - val_acc: 0.9728\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0337 - acc: 0.9914 - val_loss: 0.0940 - val_acc: 0.9732\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.0942 - val_acc: 0.9733\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0330 - acc: 0.9918 - val_loss: 0.0944 - val_acc: 0.9729\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0325 - acc: 0.9919 - val_loss: 0.0947 - val_acc: 0.9728\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0322 - acc: 0.9921 - val_loss: 0.0950 - val_acc: 0.9731\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0319 - acc: 0.9922 - val_loss: 0.0944 - val_acc: 0.9729\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0314 - acc: 0.9923 - val_loss: 0.0952 - val_acc: 0.9728\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.0309 - acc: 0.9924 - val_loss: 0.0956 - val_acc: 0.9727\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0305 - acc: 0.9927 - val_loss: 0.0954 - val_acc: 0.9737\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0302 - acc: 0.9927 - val_loss: 0.0957 - val_acc: 0.9738\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0299 - acc: 0.9929 - val_loss: 0.0961 - val_acc: 0.9737\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0295 - acc: 0.9932 - val_loss: 0.0960 - val_acc: 0.9730\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0292 - acc: 0.9931 - val_loss: 0.0955 - val_acc: 0.9736\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0289 - acc: 0.9934 - val_loss: 0.0960 - val_acc: 0.9735\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0287 - acc: 0.9934 - val_loss: 0.0971 - val_acc: 0.9734\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0283 - acc: 0.9934 - val_loss: 0.0964 - val_acc: 0.9736\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.0281 - acc: 0.9935 - val_loss: 0.0964 - val_acc: 0.9742\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 1s 236us/step - loss: 0.0276 - acc: 0.9937 - val_loss: 0.0970 - val_acc: 0.9737\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 1s 245us/step - loss: 0.0273 - acc: 0.9938 - val_loss: 0.0974 - val_acc: 0.9732\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0270 - acc: 0.9940 - val_loss: 0.0967 - val_acc: 0.9740\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0267 - acc: 0.9940 - val_loss: 0.0971 - val_acc: 0.9732\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0976 - val_acc: 0.9740\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0262 - acc: 0.9941 - val_loss: 0.0979 - val_acc: 0.9735\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0260 - acc: 0.9942 - val_loss: 0.0978 - val_acc: 0.9741\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0256 - acc: 0.9944 - val_loss: 0.0982 - val_acc: 0.9733\n",
      "\n",
      "Accuracy: 97.33%\n",
      "\n",
      "43803\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 2s 866us/step - loss: 0.1620 - acc: 0.9482 - val_loss: 0.1374 - val_acc: 0.9538\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 1s 539us/step - loss: 0.1243 - acc: 0.9539 - val_loss: 0.1218 - val_acc: 0.9562\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 1s 545us/step - loss: 0.1094 - acc: 0.9586 - val_loss: 0.1165 - val_acc: 0.9561\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 2s 575us/step - loss: 0.1002 - acc: 0.9612 - val_loss: 0.1170 - val_acc: 0.9578\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 1s 543us/step - loss: 0.0919 - acc: 0.9644 - val_loss: 0.1137 - val_acc: 0.9600\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 1s 550us/step - loss: 0.0842 - acc: 0.9673 - val_loss: 0.1163 - val_acc: 0.9590\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 1s 551us/step - loss: 0.0723 - acc: 0.9717 - val_loss: 0.1044 - val_acc: 0.9643\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0685 - acc: 0.9735 - val_loss: 0.1096 - val_acc: 0.9646\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 2s 717us/step - loss: 0.0587 - acc: 0.9774 - val_loss: 0.1075 - val_acc: 0.9655\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 2s 590us/step - loss: 0.0522 - acc: 0.9802 - val_loss: 0.1112 - val_acc: 0.9684\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 1s 542us/step - loss: 0.0489 - acc: 0.9817 - val_loss: 0.1133 - val_acc: 0.9682\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 1s 555us/step - loss: 0.0417 - acc: 0.9839 - val_loss: 0.1267 - val_acc: 0.9670\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 2s 570us/step - loss: 0.0396 - acc: 0.9851 - val_loss: 0.1200 - val_acc: 0.9696\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 545us/step - loss: 0.0334 - acc: 0.9877 - val_loss: 0.1225 - val_acc: 0.9708\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 533us/step - loss: 0.0336 - acc: 0.9876 - val_loss: 0.1184 - val_acc: 0.9691\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 554us/step - loss: 0.0268 - acc: 0.9903 - val_loss: 0.1176 - val_acc: 0.9727\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 2s 593us/step - loss: 0.0245 - acc: 0.9913 - val_loss: 0.1371 - val_acc: 0.9710\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 2s 907us/step - loss: 0.0269 - acc: 0.9905 - val_loss: 0.1321 - val_acc: 0.9721\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 2s 680us/step - loss: 0.0208 - acc: 0.9924 - val_loss: 0.1328 - val_acc: 0.9721\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 2s 563us/step - loss: 0.0252 - acc: 0.9915 - val_loss: 0.1401 - val_acc: 0.9716\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 2s 595us/step - loss: 0.0195 - acc: 0.9929 - val_loss: 0.1496 - val_acc: 0.9723\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 2s 596us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.1464 - val_acc: 0.9717\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 2s 567us/step - loss: 0.0149 - acc: 0.9949 - val_loss: 0.1517 - val_acc: 0.9744\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 2s 570us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.1699 - val_acc: 0.9701\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 1s 539us/step - loss: 0.0197 - acc: 0.9938 - val_loss: 0.1658 - val_acc: 0.9719\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 2s 572us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.1564 - val_acc: 0.9716\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0127 - acc: 0.9955 - val_loss: 0.1481 - val_acc: 0.9752\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 2s 637us/step - loss: 0.0095 - acc: 0.9966 - val_loss: 0.1626 - val_acc: 0.9746\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 2s 605us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.1667 - val_acc: 0.9767\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 2s 612us/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.1588 - val_acc: 0.9765\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 2s 565us/step - loss: 0.0208 - acc: 0.9944 - val_loss: 0.1862 - val_acc: 0.9680\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 2s 574us/step - loss: 0.0252 - acc: 0.9922 - val_loss: 0.1565 - val_acc: 0.9713\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 2s 611us/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.1627 - val_acc: 0.9739\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 2s 639us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.1582 - val_acc: 0.9749\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 1s 551us/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.1636 - val_acc: 0.9730\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 2s 707us/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.1744 - val_acc: 0.9741\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 3s 953us/step - loss: 0.0136 - acc: 0.9963 - val_loss: 0.1610 - val_acc: 0.9727\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 2s 639us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.1767 - val_acc: 0.9747\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 1s 542us/step - loss: 0.0118 - acc: 0.9966 - val_loss: 0.1730 - val_acc: 0.9743\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 2s 560us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.1625 - val_acc: 0.9747\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 1s 527us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.1672 - val_acc: 0.9747\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 1s 520us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.1665 - val_acc: 0.9758\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 1s 524us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.1779 - val_acc: 0.9731\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 1s 521us/step - loss: 0.0149 - acc: 0.9957 - val_loss: 0.1737 - val_acc: 0.9742\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 1s 517us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.1821 - val_acc: 0.9733\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 1s 536us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.2063 - val_acc: 0.9730\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 2s 877us/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.1850 - val_acc: 0.9755\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 2s 643us/step - loss: 0.0115 - acc: 0.9966 - val_loss: 0.1897 - val_acc: 0.9740\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 1s 535us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.1811 - val_acc: 0.9742\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 1s 531us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.1983 - val_acc: 0.9746\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 527us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.1852 - val_acc: 0.9758\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 529us/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1888 - val_acc: 0.9758\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 2s 571us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.1917 - val_acc: 0.9744\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 1s 528us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1883 - val_acc: 0.9754\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 547us/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.2007 - val_acc: 0.9761\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 1s 550us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.2050 - val_acc: 0.9756\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 2s 928us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.2146 - val_acc: 0.9721\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 2s 634us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.1944 - val_acc: 0.9721\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 2s 581us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1898 - val_acc: 0.9741\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 1s 546us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1986 - val_acc: 0.9740\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 1s 535us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.1953 - val_acc: 0.9743\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 1s 553us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.2046 - val_acc: 0.9755\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 1s 547us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1967 - val_acc: 0.9751\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 1s 548us/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.2151 - val_acc: 0.9734\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 1s 546us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 0.2007 - val_acc: 0.9757\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 2s 625us/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.2095 - val_acc: 0.9730\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.2160 - val_acc: 0.9744\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 2s 603us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.2215 - val_acc: 0.9721\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 1s 552us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2048 - val_acc: 0.9747\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 2s 595us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.2108 - val_acc: 0.9740\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 2s 620us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.2100 - val_acc: 0.9756\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 1s 547us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.2153 - val_acc: 0.9758\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 1s 551us/step - loss: 9.4198e-04 - acc: 0.9996 - val_loss: 0.2176 - val_acc: 0.9746\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 2s 560us/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.2212 - val_acc: 0.9748\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 1s 523us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.2359 - val_acc: 0.9723\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 3s 948us/step - loss: 0.0079 - acc: 0.9979 - val_loss: 0.2383 - val_acc: 0.9746\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 2s 643us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.2239 - val_acc: 0.9747\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 2s 623us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.2465 - val_acc: 0.9746\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 1s 550us/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.2399 - val_acc: 0.9740\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 2s 585us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.2360 - val_acc: 0.9740\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 2s 561us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.2276 - val_acc: 0.9753\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 2s 576us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.2286 - val_acc: 0.9749\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 2s 654us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 0.2225 - val_acc: 0.9756\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 2s 620us/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.2261 - val_acc: 0.9751\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 2s 737us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.2191 - val_acc: 0.9749\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2283 - val_acc: 0.9758\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 2s 615us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.2368 - val_acc: 0.9754\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 2s 580us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.2346 - val_acc: 0.9758\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 2s 589us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.2370 - val_acc: 0.9751\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 2s 571us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.2450 - val_acc: 0.9753\n",
      "\n",
      "Accuracy: 97.53%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 238us/step - loss: 0.6365 - acc: 0.6532 - val_loss: 0.5357 - val_acc: 0.8057\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.4188 - acc: 0.8564 - val_loss: 0.2730 - val_acc: 0.9189\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1976 - acc: 0.9419 - val_loss: 0.1595 - val_acc: 0.9464\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1460 - acc: 0.9505 - val_loss: 0.1441 - val_acc: 0.9506\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1367 - acc: 0.9518 - val_loss: 0.1402 - val_acc: 0.9508\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1322 - acc: 0.9522 - val_loss: 0.1371 - val_acc: 0.9513\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1287 - acc: 0.9533 - val_loss: 0.1344 - val_acc: 0.9520\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.1260 - acc: 0.953 - 0s 37us/step - loss: 0.1261 - acc: 0.9535 - val_loss: 0.1333 - val_acc: 0.9515\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1239 - acc: 0.9543 - val_loss: 0.1319 - val_acc: 0.9508\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1221 - acc: 0.9543 - val_loss: 0.1306 - val_acc: 0.9521\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1205 - acc: 0.9549 - val_loss: 0.1292 - val_acc: 0.9524\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1191 - acc: 0.9552 - val_loss: 0.1283 - val_acc: 0.9524\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 29us/step - loss: 0.1178 - acc: 0.9557 - val_loss: 0.1278 - val_acc: 0.9522\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1167 - acc: 0.9560 - val_loss: 0.1271 - val_acc: 0.9526\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1156 - acc: 0.9562 - val_loss: 0.1261 - val_acc: 0.9529\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1146 - acc: 0.9566 - val_loss: 0.1256 - val_acc: 0.9528\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1137 - acc: 0.9570 - val_loss: 0.1249 - val_acc: 0.9530\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1127 - acc: 0.9572 - val_loss: 0.1244 - val_acc: 0.9529\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1119 - acc: 0.9576 - val_loss: 0.1241 - val_acc: 0.9531\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1112 - acc: 0.9576 - val_loss: 0.1232 - val_acc: 0.9536\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1104 - acc: 0.9581 - val_loss: 0.1229 - val_acc: 0.9532\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1097 - acc: 0.9583 - val_loss: 0.1226 - val_acc: 0.9533\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1091 - acc: 0.9584 - val_loss: 0.1222 - val_acc: 0.9537\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1084 - acc: 0.9587 - val_loss: 0.1216 - val_acc: 0.9535\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1078 - acc: 0.9591 - val_loss: 0.1214 - val_acc: 0.9544\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1072 - acc: 0.9593 - val_loss: 0.1210 - val_acc: 0.9543\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1066 - acc: 0.9597 - val_loss: 0.1206 - val_acc: 0.9540\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1061 - acc: 0.9599 - val_loss: 0.1204 - val_acc: 0.9544\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1055 - acc: 0.9600 - val_loss: 0.1199 - val_acc: 0.9543\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1050 - acc: 0.9601 - val_loss: 0.1198 - val_acc: 0.9545\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1045 - acc: 0.9603 - val_loss: 0.1194 - val_acc: 0.9543\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1040 - acc: 0.9606 - val_loss: 0.1192 - val_acc: 0.9546\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1036 - acc: 0.9610 - val_loss: 0.1190 - val_acc: 0.9551\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1031 - acc: 0.9609 - val_loss: 0.1185 - val_acc: 0.9547\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1027 - acc: 0.9611 - val_loss: 0.1185 - val_acc: 0.9552\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1023 - acc: 0.9613 - val_loss: 0.1180 - val_acc: 0.9552\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.1019 - acc: 0.9616 - val_loss: 0.1181 - val_acc: 0.9552\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1014 - acc: 0.9614 - val_loss: 0.1176 - val_acc: 0.9551\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1010 - acc: 0.9619 - val_loss: 0.1174 - val_acc: 0.9553\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.1006 - acc: 0.9619 - val_loss: 0.1173 - val_acc: 0.9552\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1003 - acc: 0.9622 - val_loss: 0.1170 - val_acc: 0.9550\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1000 - acc: 0.9623 - val_loss: 0.1167 - val_acc: 0.9553\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0995 - acc: 0.9624 - val_loss: 0.1167 - val_acc: 0.9552\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0992 - acc: 0.9624 - val_loss: 0.1164 - val_acc: 0.9552\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0989 - acc: 0.9629 - val_loss: 0.1164 - val_acc: 0.9552\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0985 - acc: 0.9626 - val_loss: 0.1161 - val_acc: 0.9553\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0982 - acc: 0.9629 - val_loss: 0.1159 - val_acc: 0.9558\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.0979 - acc: 0.9632 - val_loss: 0.1157 - val_acc: 0.9552\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0975 - acc: 0.9632 - val_loss: 0.1155 - val_acc: 0.9558\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0972 - acc: 0.9631 - val_loss: 0.1155 - val_acc: 0.9560\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0969 - acc: 0.9633 - val_loss: 0.1153 - val_acc: 0.9560\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0966 - acc: 0.9635 - val_loss: 0.1150 - val_acc: 0.9565\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0964 - acc: 0.9636 - val_loss: 0.1149 - val_acc: 0.9564\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0961 - acc: 0.9637 - val_loss: 0.1149 - val_acc: 0.9565\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 206us/step - loss: 0.0958 - acc: 0.9640 - val_loss: 0.1146 - val_acc: 0.9564\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 0.0955 - acc: 0.9639 - val_loss: 0.1143 - val_acc: 0.9567\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0952 - acc: 0.9642 - val_loss: 0.1142 - val_acc: 0.9570\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0949 - acc: 0.9642 - val_loss: 0.1141 - val_acc: 0.9568\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0947 - acc: 0.9642 - val_loss: 0.1141 - val_acc: 0.9574\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0944 - acc: 0.9643 - val_loss: 0.1138 - val_acc: 0.9572\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0942 - acc: 0.9644 - val_loss: 0.1138 - val_acc: 0.9573\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0939 - acc: 0.9644 - val_loss: 0.1135 - val_acc: 0.9574\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0937 - acc: 0.9649 - val_loss: 0.1135 - val_acc: 0.9572\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0934 - acc: 0.9648 - val_loss: 0.1132 - val_acc: 0.9573\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0932 - acc: 0.9648 - val_loss: 0.1133 - val_acc: 0.9574\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0929 - acc: 0.9649 - val_loss: 0.1130 - val_acc: 0.9576\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0927 - acc: 0.9652 - val_loss: 0.1129 - val_acc: 0.9574\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0925 - acc: 0.9652 - val_loss: 0.1128 - val_acc: 0.9577\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0922 - acc: 0.9652 - val_loss: 0.1126 - val_acc: 0.9577\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0920 - acc: 0.9653 - val_loss: 0.1125 - val_acc: 0.9577\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0918 - acc: 0.9652 - val_loss: 0.1125 - val_acc: 0.9580\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0916 - acc: 0.9653 - val_loss: 0.1123 - val_acc: 0.9581\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0914 - acc: 0.9654 - val_loss: 0.1122 - val_acc: 0.9577\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0912 - acc: 0.9657 - val_loss: 0.1121 - val_acc: 0.9579\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0910 - acc: 0.9657 - val_loss: 0.1120 - val_acc: 0.9579\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.0908 - acc: 0.9657 - val_loss: 0.1118 - val_acc: 0.9581\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0905 - acc: 0.9658 - val_loss: 0.1118 - val_acc: 0.9581\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0903 - acc: 0.9659 - val_loss: 0.1116 - val_acc: 0.9580\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0902 - acc: 0.9661 - val_loss: 0.1115 - val_acc: 0.9580\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0900 - acc: 0.9659 - val_loss: 0.1115 - val_acc: 0.9581\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0898 - acc: 0.9662 - val_loss: 0.1114 - val_acc: 0.9582\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0896 - acc: 0.9661 - val_loss: 0.1113 - val_acc: 0.9584\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0894 - acc: 0.9662 - val_loss: 0.1111 - val_acc: 0.9581\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.0892 - acc: 0.9663 - val_loss: 0.1110 - val_acc: 0.9581\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 31us/step - loss: 0.0890 - acc: 0.9665 - val_loss: 0.1110 - val_acc: 0.9586\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0888 - acc: 0.9664 - val_loss: 0.1109 - val_acc: 0.9583\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0886 - acc: 0.9666 - val_loss: 0.1107 - val_acc: 0.9582\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0885 - acc: 0.9665 - val_loss: 0.1107 - val_acc: 0.9581\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 30us/step - loss: 0.0883 - acc: 0.9666 - val_loss: 0.1105 - val_acc: 0.9582\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0881 - acc: 0.9666 - val_loss: 0.1105 - val_acc: 0.9583\n",
      "\n",
      "Accuracy: 95.83%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 408us/step - loss: 0.5414 - acc: 0.9465 - val_loss: 0.3550 - val_acc: 0.9499\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.2683 - acc: 0.9489 - val_loss: 0.2149 - val_acc: 0.9499\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.2004 - acc: 0.9489 - val_loss: 0.1857 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.1838 - acc: 0.9489 - val_loss: 0.1759 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.1775 - acc: 0.9489 - val_loss: 0.1714 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.1743 - acc: 0.9489 - val_loss: 0.1689 - val_acc: 0.9499\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1725 - acc: 0.9489 - val_loss: 0.1674 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.1713 - acc: 0.9489 - val_loss: 0.1664 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 161us/step - loss: 0.1705 - acc: 0.9489 - val_loss: 0.1656 - val_acc: 0.9499\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1699 - acc: 0.9489 - val_loss: 0.1651 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 139us/step - loss: 0.1695 - acc: 0.9489 - val_loss: 0.1647 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.1692 - acc: 0.9489 - val_loss: 0.1644 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 1s 236us/step - loss: 0.1689 - acc: 0.9489 - val_loss: 0.1641 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 1s 256us/step - loss: 0.1687 - acc: 0.9489 - val_loss: 0.1639 - val_acc: 0.9499\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.1685 - acc: 0.9489 - val_loss: 0.1637 - val_acc: 0.9499\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 1s 333us/step - loss: 0.1683 - acc: 0.9489 - val_loss: 0.1636 - val_acc: 0.9499\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.1682 - acc: 0.9489 - val_loss: 0.1635 - val_acc: 0.9499\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.1681 - acc: 0.9489 - val_loss: 0.1634 - val_acc: 0.9499\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 2s 593us/step - loss: 0.1680 - acc: 0.9489 - val_loss: 0.1633 - val_acc: 0.9499\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 1s 335us/step - loss: 0.1679 - acc: 0.9489 - val_loss: 0.1632 - val_acc: 0.9499\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.1678 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9499\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 1s 415us/step - loss: 0.1678 - acc: 0.9489 - val_loss: 0.1631 - val_acc: 0.9499\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 1s 218us/step - loss: 0.1677 - acc: 0.9489 - val_loss: 0.1630 - val_acc: 0.9499\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1630 - val_acc: 0.9499\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.1676 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1629 - val_acc: 0.9499\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.1675 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 1s 189us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 153us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1627 - val_acc: 0.9499\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 168us/step - loss: 0.1673 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 140us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 159us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1672 - acc: 0.9489 - val_loss: 0.1626 - val_acc: 0.9499\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 140us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 121us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 149us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 134us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1671 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 129us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 169us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9499\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 1s 195us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 1s 357us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 150us/step - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 194us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 139us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 1s 223us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9499\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 137us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 127us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 152us/step - loss: 0.1669 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 143us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 185us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 151us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 134us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 128us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 131us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 148us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 141us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 130us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 161us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 146us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 139us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 136us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 159us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 133us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 158us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 142us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 1s 211us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1623 - val_acc: 0.9499\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 1s 404us/step - loss: 0.1668 - acc: 0.9489 - val_loss: 0.1622 - val_acc: 0.9499\n",
      "\n",
      "Accuracy: 94.99%\n",
      "\n",
      "50149\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 306us/step - loss: 0.2874 - acc: 0.8767 - val_loss: 0.1361 - val_acc: 0.9509\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.1272 - acc: 0.9534 - val_loss: 0.1249 - val_acc: 0.9524\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1133 - acc: 0.9573 - val_loss: 0.1150 - val_acc: 0.9539\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.1025 - acc: 0.9608 - val_loss: 0.1079 - val_acc: 0.9588\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0912 - acc: 0.9654 - val_loss: 0.1061 - val_acc: 0.9588\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 106us/step - loss: 0.0855 - acc: 0.9667 - val_loss: 0.1016 - val_acc: 0.9594\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0774 - acc: 0.9701 - val_loss: 0.0978 - val_acc: 0.9628\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0710 - acc: 0.9724 - val_loss: 0.1014 - val_acc: 0.9612\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0642 - acc: 0.9760 - val_loss: 0.0887 - val_acc: 0.9665\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0584 - acc: 0.9778 - val_loss: 0.0928 - val_acc: 0.9677\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0510 - acc: 0.9814 - val_loss: 0.0914 - val_acc: 0.9679\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0440 - acc: 0.9845 - val_loss: 0.0951 - val_acc: 0.9678\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0392 - acc: 0.9860 - val_loss: 0.0907 - val_acc: 0.9703\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0337 - acc: 0.9883 - val_loss: 0.0890 - val_acc: 0.9708\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0345 - acc: 0.9884 - val_loss: 0.0923 - val_acc: 0.9709\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0297 - acc: 0.9899 - val_loss: 0.0924 - val_acc: 0.9719\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0299 - acc: 0.9902 - val_loss: 0.0898 - val_acc: 0.9724\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0916 - val_acc: 0.9726\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0203 - acc: 0.9938 - val_loss: 0.0943 - val_acc: 0.9730\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0179 - acc: 0.9950 - val_loss: 0.0966 - val_acc: 0.9733\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.1000 - val_acc: 0.9735\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.1000 - val_acc: 0.9733\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0171 - acc: 0.9945 - val_loss: 0.1030 - val_acc: 0.9742\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.1023 - val_acc: 0.9740\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0151 - acc: 0.9955 - val_loss: 0.1028 - val_acc: 0.9747\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.1036 - val_acc: 0.9736\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0140 - acc: 0.9959 - val_loss: 0.1078 - val_acc: 0.9735\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.1052 - val_acc: 0.9745\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.1125 - val_acc: 0.9740\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.1154 - val_acc: 0.9737\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.2146 - val_acc: 0.9515\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1076 - acc: 0.9641 - val_loss: 0.1361 - val_acc: 0.9595\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0709 - acc: 0.9744 - val_loss: 0.1213 - val_acc: 0.9648\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0474 - acc: 0.9823 - val_loss: 0.1108 - val_acc: 0.9689\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0348 - acc: 0.9871 - val_loss: 0.1108 - val_acc: 0.9687\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0269 - acc: 0.9905 - val_loss: 0.1104 - val_acc: 0.9708\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0219 - acc: 0.9927 - val_loss: 0.1028 - val_acc: 0.9728\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.0163 - acc: 0.9947 - val_loss: 0.1027 - val_acc: 0.9740\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0136 - acc: 0.9959 - val_loss: 0.0997 - val_acc: 0.9754\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.1084 - val_acc: 0.9742\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.1091 - val_acc: 0.9753\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0068 - acc: 0.9983 - val_loss: 0.1039 - val_acc: 0.9751\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.1099 - val_acc: 0.9753\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1076 - val_acc: 0.9752\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1095 - val_acc: 0.9753\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.1131 - val_acc: 0.9760\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1142 - val_acc: 0.9753\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.1160 - val_acc: 0.9749\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.0073 - acc: 0.9981 - val_loss: 0.1136 - val_acc: 0.9754\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.1188 - val_acc: 0.9755\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1146 - val_acc: 0.9756\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1240 - val_acc: 0.9740\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.1150 - val_acc: 0.9751\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.1225 - val_acc: 0.9733\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 1s 424us/step - loss: 0.0117 - acc: 0.9960 - val_loss: 0.1273 - val_acc: 0.9725\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.1274 - val_acc: 0.9727\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.1268 - val_acc: 0.9729\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.1265 - val_acc: 0.9719\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 155us/step - loss: 0.0355 - acc: 0.9880 - val_loss: 0.1361 - val_acc: 0.9680\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0347 - acc: 0.9876 - val_loss: 0.1379 - val_acc: 0.9689\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0290 - acc: 0.9895 - val_loss: 0.1328 - val_acc: 0.9703\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0212 - acc: 0.9926 - val_loss: 0.1338 - val_acc: 0.9709\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0199 - acc: 0.9929 - val_loss: 0.1294 - val_acc: 0.9717\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0167 - acc: 0.9941 - val_loss: 0.1369 - val_acc: 0.9728\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.1291 - val_acc: 0.9733\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.1307 - val_acc: 0.9738\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.1333 - val_acc: 0.9739\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.1355 - val_acc: 0.9735\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1342 - val_acc: 0.9758\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1398 - val_acc: 0.9753\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.1397 - val_acc: 0.9740\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.1339 - val_acc: 0.9739\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.1367 - val_acc: 0.9754\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1413 - val_acc: 0.9749\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.1357 - val_acc: 0.9747\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.1370 - val_acc: 0.9748\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1359 - val_acc: 0.9753\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1471 - val_acc: 0.9742\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 105us/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.1496 - val_acc: 0.9738\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1415 - val_acc: 0.9747\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1412 - val_acc: 0.9749\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.1431 - val_acc: 0.9757\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.1435 - val_acc: 0.9753\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0098 - acc: 0.9966 - val_loss: 0.1486 - val_acc: 0.9733\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.1508 - val_acc: 0.9728\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0151 - acc: 0.9947 - val_loss: 0.1533 - val_acc: 0.9722\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 108us/step - loss: 0.0158 - acc: 0.9944 - val_loss: 0.1503 - val_acc: 0.9735\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 145us/step - loss: 0.0162 - acc: 0.9943 - val_loss: 0.1545 - val_acc: 0.9723\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0132 - acc: 0.9952 - val_loss: 0.1497 - val_acc: 0.9734\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.1477 - val_acc: 0.9734\n",
      "\n",
      "Accuracy: 97.34%\n",
      "\n",
      "44035\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.3373 - acc: 0.8668 - val_loss: 0.1800 - val_acc: 0.9499\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.1783 - acc: 0.9489 - val_loss: 0.1667 - val_acc: 0.9499\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1694 - acc: 0.9489 - val_loss: 0.1628 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1666 - acc: 0.9489 - val_loss: 0.1607 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1636 - acc: 0.9489 - val_loss: 0.1604 - val_acc: 0.9499\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1630 - acc: 0.9489 - val_loss: 0.1587 - val_acc: 0.9499\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1612 - acc: 0.9489 - val_loss: 0.1656 - val_acc: 0.9499\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1643 - acc: 0.9489 - val_loss: 0.1591 - val_acc: 0.9499\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1597 - acc: 0.9489 - val_loss: 0.1555 - val_acc: 0.9499\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1583 - acc: 0.9489 - val_loss: 0.1540 - val_acc: 0.9499\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1557 - acc: 0.9489 - val_loss: 0.1528 - val_acc: 0.9499\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1542 - acc: 0.9498 - val_loss: 0.1506 - val_acc: 0.9499\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1512 - acc: 0.9500 - val_loss: 0.1497 - val_acc: 0.9499\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1484 - acc: 0.9507 - val_loss: 0.1510 - val_acc: 0.9503\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1459 - acc: 0.9510 - val_loss: 0.1459 - val_acc: 0.9519\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1433 - acc: 0.9514 - val_loss: 0.1489 - val_acc: 0.9512\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1423 - acc: 0.9514 - val_loss: 0.1433 - val_acc: 0.9520\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1409 - acc: 0.9515 - val_loss: 0.1422 - val_acc: 0.9524\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1389 - acc: 0.9522 - val_loss: 0.1413 - val_acc: 0.9519\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1380 - acc: 0.9521 - val_loss: 0.1405 - val_acc: 0.9524\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1382 - acc: 0.9516 - val_loss: 0.1442 - val_acc: 0.9510\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1393 - acc: 0.9514 - val_loss: 0.1417 - val_acc: 0.9517\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1349 - acc: 0.9522 - val_loss: 0.1407 - val_acc: 0.9523\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1328 - acc: 0.9529 - val_loss: 0.1398 - val_acc: 0.9522\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1321 - acc: 0.9524 - val_loss: 0.1403 - val_acc: 0.9520\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1315 - acc: 0.9526 - val_loss: 0.1376 - val_acc: 0.9527\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1292 - acc: 0.9530 - val_loss: 0.1436 - val_acc: 0.9518\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1358 - acc: 0.9515 - val_loss: 0.1382 - val_acc: 0.9529\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1303 - acc: 0.9525 - val_loss: 0.1400 - val_acc: 0.9526\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.1277 - acc: 0.9528 - val_loss: 0.1374 - val_acc: 0.9526\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.1263 - acc: 0.9531 - val_loss: 0.1341 - val_acc: 0.9531\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.1244 - acc: 0.9534 - val_loss: 0.1342 - val_acc: 0.9526\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1244 - acc: 0.9534 - val_loss: 0.1350 - val_acc: 0.9544\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.1241 - acc: 0.9538 - val_loss: 0.1401 - val_acc: 0.9528\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1226 - acc: 0.9554 - val_loss: 0.1385 - val_acc: 0.9524\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1280 - acc: 0.9539 - val_loss: 0.1397 - val_acc: 0.9539\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 0.1233 - acc: 0.9548 - val_loss: 0.1354 - val_acc: 0.9536\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1217 - acc: 0.9558 - val_loss: 0.1359 - val_acc: 0.9548\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1191 - acc: 0.9563 - val_loss: 0.1349 - val_acc: 0.9552\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1189 - acc: 0.9570 - val_loss: 0.1346 - val_acc: 0.9549\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1189 - acc: 0.9573 - val_loss: 0.1402 - val_acc: 0.9545\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1175 - acc: 0.9580 - val_loss: 0.1365 - val_acc: 0.9548\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1162 - acc: 0.9585 - val_loss: 0.1331 - val_acc: 0.9568\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1187 - acc: 0.9574 - val_loss: 0.1376 - val_acc: 0.9560\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1155 - acc: 0.9587 - val_loss: 0.1352 - val_acc: 0.9561\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.1159 - acc: 0.9582 - val_loss: 0.1398 - val_acc: 0.9555\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1173 - acc: 0.9583 - val_loss: 0.1367 - val_acc: 0.9561\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1145 - acc: 0.9585 - val_loss: 0.1385 - val_acc: 0.9568\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1136 - acc: 0.9591 - val_loss: 0.1413 - val_acc: 0.9568\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1120 - acc: 0.9594 - val_loss: 0.1369 - val_acc: 0.9568\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1149 - acc: 0.9585 - val_loss: 0.1433 - val_acc: 0.9563\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1138 - acc: 0.9591 - val_loss: 0.1396 - val_acc: 0.9570\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.1122 - acc: 0.9597 - val_loss: 0.1381 - val_acc: 0.9573\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1092 - acc: 0.9604 - val_loss: 0.1375 - val_acc: 0.9576\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1125 - acc: 0.9592 - val_loss: 0.1442 - val_acc: 0.9565\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.1100 - acc: 0.960 - 0s 60us/step - loss: 0.1108 - acc: 0.9598 - val_loss: 0.1453 - val_acc: 0.9568\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 47us/step - loss: 0.1097 - acc: 0.9600 - val_loss: 0.1407 - val_acc: 0.9577\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1117 - acc: 0.9598 - val_loss: 0.1415 - val_acc: 0.9571\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1080 - acc: 0.9604 - val_loss: 0.1394 - val_acc: 0.9583\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1066 - acc: 0.9609 - val_loss: 0.1411 - val_acc: 0.9581\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1061 - acc: 0.9609 - val_loss: 0.1394 - val_acc: 0.9580\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1046 - acc: 0.9609 - val_loss: 0.1432 - val_acc: 0.9586\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.1044 - acc: 0.9611 - val_loss: 0.1448 - val_acc: 0.9583\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1087 - acc: 0.9599 - val_loss: 0.1406 - val_acc: 0.9579\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1059 - acc: 0.9607 - val_loss: 0.1404 - val_acc: 0.9590\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1042 - acc: 0.9609 - val_loss: 0.1429 - val_acc: 0.9590\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1035 - acc: 0.9611 - val_loss: 0.1417 - val_acc: 0.9588\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1019 - acc: 0.9615 - val_loss: 0.1493 - val_acc: 0.9583\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1018 - acc: 0.9615 - val_loss: 0.1451 - val_acc: 0.9586\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1025 - acc: 0.9614 - val_loss: 0.1449 - val_acc: 0.9586\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1019 - acc: 0.9613 - val_loss: 0.1528 - val_acc: 0.9575\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1020 - acc: 0.9612 - val_loss: 0.1442 - val_acc: 0.9588\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1001 - acc: 0.9615 - val_loss: 0.1476 - val_acc: 0.9588\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0996 - acc: 0.9614 - val_loss: 0.1466 - val_acc: 0.9588\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0991 - acc: 0.9619 - val_loss: 0.1462 - val_acc: 0.9585\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1005 - acc: 0.9612 - val_loss: 0.1448 - val_acc: 0.9586\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0981 - acc: 0.9617 - val_loss: 0.1479 - val_acc: 0.9591\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0984 - acc: 0.9613 - val_loss: 0.1475 - val_acc: 0.9570\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0984 - acc: 0.9616 - val_loss: 0.1479 - val_acc: 0.9588\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0985 - acc: 0.9612 - val_loss: 0.1493 - val_acc: 0.9572\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0972 - acc: 0.9614 - val_loss: 0.1458 - val_acc: 0.9584\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0978 - acc: 0.9612 - val_loss: 0.1536 - val_acc: 0.9568\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0977 - acc: 0.9610 - val_loss: 0.1509 - val_acc: 0.9577\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0977 - acc: 0.9608 - val_loss: 0.1478 - val_acc: 0.9581\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0967 - acc: 0.9614 - val_loss: 0.1489 - val_acc: 0.9589\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0957 - acc: 0.9616 - val_loss: 0.1479 - val_acc: 0.9586\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0950 - acc: 0.9616 - val_loss: 0.1474 - val_acc: 0.9584\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0940 - acc: 0.9618 - val_loss: 0.1482 - val_acc: 0.9588\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0944 - acc: 0.9618 - val_loss: 0.1475 - val_acc: 0.9588\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0939 - acc: 0.9619 - val_loss: 0.1491 - val_acc: 0.9586\n",
      "\n",
      "Accuracy: 95.86%\n",
      "\n",
      "49917\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 418us/step - loss: 0.6033 - acc: 0.7747 - val_loss: 0.5037 - val_acc: 0.9126\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 117us/step - loss: 0.3962 - acc: 0.9366 - val_loss: 0.2926 - val_acc: 0.9498\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.2474 - acc: 0.9489 - val_loss: 0.2151 - val_acc: 0.9499\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.2095 - acc: 0.9489 - val_loss: 0.1975 - val_acc: 0.9499\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1969 - acc: 0.9489 - val_loss: 0.1889 - val_acc: 0.9500\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.1891 - acc: 0.9493 - val_loss: 0.1828 - val_acc: 0.9505\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.1827 - acc: 0.9498 - val_loss: 0.1776 - val_acc: 0.9510\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1774 - acc: 0.9503 - val_loss: 0.1732 - val_acc: 0.9513\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1726 - acc: 0.9507 - val_loss: 0.1695 - val_acc: 0.9513\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1687 - acc: 0.9511 - val_loss: 0.1665 - val_acc: 0.9518\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1651 - acc: 0.9516 - val_loss: 0.1636 - val_acc: 0.9522\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1619 - acc: 0.9519 - val_loss: 0.1614 - val_acc: 0.9525\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1591 - acc: 0.9523 - val_loss: 0.1591 - val_acc: 0.9527\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.1565 - acc: 0.9523 - val_loss: 0.1573 - val_acc: 0.9531\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1540 - acc: 0.9526 - val_loss: 0.1550 - val_acc: 0.9534\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.1517 - acc: 0.9529 - val_loss: 0.1535 - val_acc: 0.9533\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1497 - acc: 0.9530 - val_loss: 0.1517 - val_acc: 0.9534\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1477 - acc: 0.9534 - val_loss: 0.1505 - val_acc: 0.9531\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1458 - acc: 0.9535 - val_loss: 0.1488 - val_acc: 0.9534\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1441 - acc: 0.9535 - val_loss: 0.1477 - val_acc: 0.9529\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1425 - acc: 0.9537 - val_loss: 0.1462 - val_acc: 0.9538\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1410 - acc: 0.9541 - val_loss: 0.1451 - val_acc: 0.9529\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.1395 - acc: 0.9542 - val_loss: 0.1440 - val_acc: 0.9533\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1381 - acc: 0.9543 - val_loss: 0.1430 - val_acc: 0.9538\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1368 - acc: 0.9547 - val_loss: 0.1421 - val_acc: 0.9542\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 0.1355 - acc: 0.9547 - val_loss: 0.1412 - val_acc: 0.9540\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.1343 - acc: 0.9551 - val_loss: 0.1400 - val_acc: 0.9544\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1332 - acc: 0.9551 - val_loss: 0.1394 - val_acc: 0.9540\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1320 - acc: 0.9555 - val_loss: 0.1385 - val_acc: 0.9542\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1309 - acc: 0.9557 - val_loss: 0.1376 - val_acc: 0.9541\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.1299 - acc: 0.9559 - val_loss: 0.1369 - val_acc: 0.9546\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.1288 - acc: 0.9561 - val_loss: 0.1362 - val_acc: 0.9551\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 1s 366us/step - loss: 0.1278 - acc: 0.9567 - val_loss: 0.1355 - val_acc: 0.9545\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1269 - acc: 0.9567 - val_loss: 0.1348 - val_acc: 0.9548\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1260 - acc: 0.9569 - val_loss: 0.1341 - val_acc: 0.9553\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1251 - acc: 0.9573 - val_loss: 0.1335 - val_acc: 0.9549\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.1243 - acc: 0.9573 - val_loss: 0.1329 - val_acc: 0.9554\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.1234 - acc: 0.9576 - val_loss: 0.1323 - val_acc: 0.9558\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.1228 - acc: 0.9578 - val_loss: 0.1320 - val_acc: 0.9553\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1220 - acc: 0.9578 - val_loss: 0.1314 - val_acc: 0.9555\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 0.1308 - val_acc: 0.9554\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 126us/step - loss: 0.1205 - acc: 0.9580 - val_loss: 0.1302 - val_acc: 0.9554\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1199 - acc: 0.9582 - val_loss: 0.1298 - val_acc: 0.9556\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1193 - acc: 0.9586 - val_loss: 0.1294 - val_acc: 0.9552\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.1187 - acc: 0.9587 - val_loss: 0.1287 - val_acc: 0.9559\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1181 - acc: 0.9584 - val_loss: 0.1285 - val_acc: 0.9553\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1175 - acc: 0.9589 - val_loss: 0.1279 - val_acc: 0.9558\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1169 - acc: 0.9587 - val_loss: 0.1277 - val_acc: 0.9557\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1164 - acc: 0.9591 - val_loss: 0.1271 - val_acc: 0.9558\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1158 - acc: 0.9590 - val_loss: 0.1268 - val_acc: 0.9556\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1154 - acc: 0.9594 - val_loss: 0.1263 - val_acc: 0.9554\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1148 - acc: 0.9591 - val_loss: 0.1260 - val_acc: 0.9559\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1144 - acc: 0.9596 - val_loss: 0.1257 - val_acc: 0.9557\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 0.1253 - val_acc: 0.9561\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1133 - acc: 0.9600 - val_loss: 0.1250 - val_acc: 0.9560\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1129 - acc: 0.9600 - val_loss: 0.1247 - val_acc: 0.9560\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1126 - acc: 0.9599 - val_loss: 0.1243 - val_acc: 0.9567\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.1121 - acc: 0.9602 - val_loss: 0.1240 - val_acc: 0.9558\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1117 - acc: 0.9603 - val_loss: 0.1237 - val_acc: 0.9565\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1113 - acc: 0.9604 - val_loss: 0.1234 - val_acc: 0.9563\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 0.1109 - acc: 0.9604 - val_loss: 0.1232 - val_acc: 0.9559\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.1106 - acc: 0.9605 - val_loss: 0.1230 - val_acc: 0.9560\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.1101 - acc: 0.9605 - val_loss: 0.1226 - val_acc: 0.9563\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1098 - acc: 0.9606 - val_loss: 0.1223 - val_acc: 0.9561\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.1094 - acc: 0.9609 - val_loss: 0.1221 - val_acc: 0.9567\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1090 - acc: 0.9609 - val_loss: 0.1217 - val_acc: 0.9562\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1087 - acc: 0.9610 - val_loss: 0.1215 - val_acc: 0.9565\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1084 - acc: 0.9610 - val_loss: 0.1212 - val_acc: 0.9563\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1080 - acc: 0.9612 - val_loss: 0.1211 - val_acc: 0.9565\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1077 - acc: 0.9612 - val_loss: 0.1206 - val_acc: 0.9565\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.1073 - acc: 0.9614 - val_loss: 0.1206 - val_acc: 0.9563\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1070 - acc: 0.9616 - val_loss: 0.1203 - val_acc: 0.9565\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1067 - acc: 0.9616 - val_loss: 0.1201 - val_acc: 0.9564\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1064 - acc: 0.9617 - val_loss: 0.1200 - val_acc: 0.9563\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1062 - acc: 0.9617 - val_loss: 0.1197 - val_acc: 0.9560\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.1058 - acc: 0.9618 - val_loss: 0.1195 - val_acc: 0.9565\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.1056 - acc: 0.9619 - val_loss: 0.1193 - val_acc: 0.9566\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1053 - acc: 0.9622 - val_loss: 0.1191 - val_acc: 0.9567\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.1050 - acc: 0.9620 - val_loss: 0.1188 - val_acc: 0.9570\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.1048 - acc: 0.9621 - val_loss: 0.1187 - val_acc: 0.9566\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.1045 - acc: 0.9620 - val_loss: 0.1185 - val_acc: 0.9567\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.1042 - acc: 0.9623 - val_loss: 0.1182 - val_acc: 0.9568\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.1039 - acc: 0.9624 - val_loss: 0.1181 - val_acc: 0.9568\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1037 - acc: 0.9624 - val_loss: 0.1180 - val_acc: 0.9570\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1035 - acc: 0.9624 - val_loss: 0.1177 - val_acc: 0.9567\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1032 - acc: 0.9626 - val_loss: 0.1176 - val_acc: 0.9570\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.1030 - acc: 0.9628 - val_loss: 0.1174 - val_acc: 0.9572\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1028 - acc: 0.9628 - val_loss: 0.1173 - val_acc: 0.9572\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.1025 - acc: 0.9629 - val_loss: 0.1172 - val_acc: 0.9572\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.1022 - acc: 0.9630 - val_loss: 0.1169 - val_acc: 0.9575\n",
      "\n",
      "Accuracy: 95.75%\n",
      "\n",
      "50613\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 240us/step - loss: 0.3719 - acc: 0.8302 - val_loss: 0.1562 - val_acc: 0.9447\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1402 - acc: 0.9496 - val_loss: 0.1309 - val_acc: 0.9514\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.1207 - acc: 0.9539 - val_loss: 0.1214 - val_acc: 0.9529\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1104 - acc: 0.9577 - val_loss: 0.1164 - val_acc: 0.9541\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1034 - acc: 0.9613 - val_loss: 0.1154 - val_acc: 0.9554\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0958 - acc: 0.9633 - val_loss: 0.1075 - val_acc: 0.9567\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0899 - acc: 0.9654 - val_loss: 0.1047 - val_acc: 0.9595\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0855 - acc: 0.9681 - val_loss: 0.1027 - val_acc: 0.9603\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0791 - acc: 0.9703 - val_loss: 0.0995 - val_acc: 0.9620\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0744 - acc: 0.9723 - val_loss: 0.0977 - val_acc: 0.9628\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0706 - acc: 0.9740 - val_loss: 0.0981 - val_acc: 0.9622\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 0.0661 - acc: 0.9762 - val_loss: 0.0944 - val_acc: 0.9647\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0626 - acc: 0.9775 - val_loss: 0.0939 - val_acc: 0.9656\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0591 - acc: 0.9791 - val_loss: 0.0919 - val_acc: 0.9668\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0550 - acc: 0.9809 - val_loss: 0.0910 - val_acc: 0.9670\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0528 - acc: 0.9820 - val_loss: 0.0910 - val_acc: 0.9674\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0495 - acc: 0.9832 - val_loss: 0.0917 - val_acc: 0.9678\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0471 - acc: 0.9843 - val_loss: 0.0899 - val_acc: 0.9685\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0444 - acc: 0.9854 - val_loss: 0.0884 - val_acc: 0.9696\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0426 - acc: 0.9863 - val_loss: 0.0895 - val_acc: 0.9697\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0401 - acc: 0.9873 - val_loss: 0.0902 - val_acc: 0.9690\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0381 - acc: 0.9882 - val_loss: 0.0900 - val_acc: 0.9709\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0366 - acc: 0.9888 - val_loss: 0.0887 - val_acc: 0.9707\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.0895 - val_acc: 0.9707\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0331 - acc: 0.9903 - val_loss: 0.0908 - val_acc: 0.9712\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0315 - acc: 0.9911 - val_loss: 0.0886 - val_acc: 0.9721\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0301 - acc: 0.9913 - val_loss: 0.0899 - val_acc: 0.9720\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.0899 - val_acc: 0.9724\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0281 - acc: 0.9925 - val_loss: 0.0919 - val_acc: 0.9723\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0267 - acc: 0.9931 - val_loss: 0.0899 - val_acc: 0.9719\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0257 - acc: 0.9932 - val_loss: 0.0905 - val_acc: 0.9733\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0246 - acc: 0.9939 - val_loss: 0.0911 - val_acc: 0.9726\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0235 - acc: 0.9942 - val_loss: 0.0918 - val_acc: 0.9728\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0230 - acc: 0.9943 - val_loss: 0.0924 - val_acc: 0.9731\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0219 - acc: 0.9950 - val_loss: 0.0907 - val_acc: 0.9739\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0212 - acc: 0.9951 - val_loss: 0.0920 - val_acc: 0.9738\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0929 - val_acc: 0.9736\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0197 - acc: 0.9956 - val_loss: 0.0931 - val_acc: 0.9739\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0190 - acc: 0.9957 - val_loss: 0.0940 - val_acc: 0.9740\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0184 - acc: 0.9960 - val_loss: 0.0936 - val_acc: 0.9739\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0179 - acc: 0.9962 - val_loss: 0.0945 - val_acc: 0.9737\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0170 - acc: 0.9964 - val_loss: 0.0951 - val_acc: 0.9736\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0162 - acc: 0.9968 - val_loss: 0.0946 - val_acc: 0.9740\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0159 - acc: 0.9970 - val_loss: 0.0949 - val_acc: 0.9740\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0155 - acc: 0.9972 - val_loss: 0.0946 - val_acc: 0.9744\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 0.0954 - val_acc: 0.9740\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0144 - acc: 0.9973 - val_loss: 0.0956 - val_acc: 0.9742\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0140 - acc: 0.9976 - val_loss: 0.0962 - val_acc: 0.9746\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0134 - acc: 0.9977 - val_loss: 0.0978 - val_acc: 0.9746\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0130 - acc: 0.9980 - val_loss: 0.0970 - val_acc: 0.9744\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0126 - acc: 0.9980 - val_loss: 0.0976 - val_acc: 0.9744\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0123 - acc: 0.9981 - val_loss: 0.0983 - val_acc: 0.9745\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0119 - acc: 0.9983 - val_loss: 0.1001 - val_acc: 0.9744\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0117 - acc: 0.9983 - val_loss: 0.0994 - val_acc: 0.9751\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0114 - acc: 0.9985 - val_loss: 0.0993 - val_acc: 0.9748\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0112 - acc: 0.9984 - val_loss: 0.1002 - val_acc: 0.9748\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0107 - acc: 0.9986 - val_loss: 0.1000 - val_acc: 0.9749\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0104 - acc: 0.9986 - val_loss: 0.1005 - val_acc: 0.9747\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0103 - acc: 0.9986 - val_loss: 0.1007 - val_acc: 0.9751\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0102 - acc: 0.9985 - val_loss: 0.1010 - val_acc: 0.9752\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0097 - acc: 0.9988 - val_loss: 0.1018 - val_acc: 0.9750\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0094 - acc: 0.9988 - val_loss: 0.1018 - val_acc: 0.9746\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0093 - acc: 0.9989 - val_loss: 0.1020 - val_acc: 0.9753\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0090 - acc: 0.9990 - val_loss: 0.1035 - val_acc: 0.9752\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0086 - acc: 0.9990 - val_loss: 0.1032 - val_acc: 0.9755\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0084 - acc: 0.9991 - val_loss: 0.1034 - val_acc: 0.9757\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0082 - acc: 0.9992 - val_loss: 0.1032 - val_acc: 0.9751\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0080 - acc: 0.9991 - val_loss: 0.1041 - val_acc: 0.9753\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0079 - acc: 0.9992 - val_loss: 0.1046 - val_acc: 0.9754\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0077 - acc: 0.9992 - val_loss: 0.1046 - val_acc: 0.9748\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0077 - acc: 0.9993 - val_loss: 0.1050 - val_acc: 0.9751\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0073 - acc: 0.9994 - val_loss: 0.1061 - val_acc: 0.9752\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.1066 - val_acc: 0.9745\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0071 - acc: 0.9994 - val_loss: 0.1070 - val_acc: 0.9754\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0069 - acc: 0.9994 - val_loss: 0.1061 - val_acc: 0.9755\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0067 - acc: 0.9995 - val_loss: 0.1072 - val_acc: 0.9753\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0066 - acc: 0.9995 - val_loss: 0.1070 - val_acc: 0.9755\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0065 - acc: 0.9995 - val_loss: 0.1077 - val_acc: 0.9751\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0064 - acc: 0.9995 - val_loss: 0.1080 - val_acc: 0.9757\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0062 - acc: 0.9995 - val_loss: 0.1087 - val_acc: 0.9756\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0061 - acc: 0.9996 - val_loss: 0.1088 - val_acc: 0.9754\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.0059 - acc: 0.9996 - val_loss: 0.1093 - val_acc: 0.9753\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0059 - acc: 0.9996 - val_loss: 0.1087 - val_acc: 0.9757\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0057 - acc: 0.9996 - val_loss: 0.1093 - val_acc: 0.9757\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0056 - acc: 0.9997 - val_loss: 0.1097 - val_acc: 0.9754\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.1099 - val_acc: 0.9757\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0054 - acc: 0.9997 - val_loss: 0.1113 - val_acc: 0.9750\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0053 - acc: 0.9997 - val_loss: 0.1106 - val_acc: 0.9754\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.0052 - acc: 0.9997 - val_loss: 0.1108 - val_acc: 0.9754\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0051 - acc: 0.9998 - val_loss: 0.1116 - val_acc: 0.9753\n",
      "\n",
      "Accuracy: 97.53%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 315us/step - loss: 0.1817 - acc: 0.9442 - val_loss: 0.1347 - val_acc: 0.9532\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 119us/step - loss: 0.1206 - acc: 0.9567 - val_loss: 0.1193 - val_acc: 0.9559\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.1040 - acc: 0.9607 - val_loss: 0.1097 - val_acc: 0.9586\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0921 - acc: 0.9647 - val_loss: 0.1049 - val_acc: 0.9609\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0808 - acc: 0.9690 - val_loss: 0.1004 - val_acc: 0.9618\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0738 - acc: 0.9719 - val_loss: 0.1027 - val_acc: 0.9620\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0683 - acc: 0.9740 - val_loss: 0.0969 - val_acc: 0.9653\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0626 - acc: 0.9762 - val_loss: 0.0973 - val_acc: 0.9662\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0575 - acc: 0.9781 - val_loss: 0.0979 - val_acc: 0.9652\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0551 - acc: 0.9795 - val_loss: 0.0961 - val_acc: 0.9671\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0511 - acc: 0.9810 - val_loss: 0.0940 - val_acc: 0.9687\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0481 - acc: 0.9820 - val_loss: 0.0938 - val_acc: 0.9692\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 84us/step - loss: 0.0452 - acc: 0.9835 - val_loss: 0.0941 - val_acc: 0.9697\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0428 - acc: 0.9846 - val_loss: 0.0947 - val_acc: 0.9696\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0406 - acc: 0.9854 - val_loss: 0.0941 - val_acc: 0.9701\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 104us/step - loss: 0.0389 - acc: 0.9860 - val_loss: 0.0951 - val_acc: 0.9710\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 138us/step - loss: 0.0370 - acc: 0.9871 - val_loss: 0.0948 - val_acc: 0.9721\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 1s 406us/step - loss: 0.0350 - acc: 0.9877 - val_loss: 0.0947 - val_acc: 0.9716\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 115us/step - loss: 0.0340 - acc: 0.9884 - val_loss: 0.0940 - val_acc: 0.9720\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0321 - acc: 0.9892 - val_loss: 0.0938 - val_acc: 0.9731\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 114us/step - loss: 0.0313 - acc: 0.9892 - val_loss: 0.0951 - val_acc: 0.9722\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0297 - acc: 0.9902 - val_loss: 0.0965 - val_acc: 0.9726\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0287 - acc: 0.9904 - val_loss: 0.0981 - val_acc: 0.9728\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0277 - acc: 0.9909 - val_loss: 0.0975 - val_acc: 0.9727\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0974 - val_acc: 0.9734\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 172us/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0985 - val_acc: 0.9737\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 110us/step - loss: 0.0249 - acc: 0.9920 - val_loss: 0.0979 - val_acc: 0.9742\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0240 - acc: 0.9927 - val_loss: 0.1011 - val_acc: 0.9728\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 0.0235 - acc: 0.9929 - val_loss: 0.1006 - val_acc: 0.9737\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0227 - acc: 0.9933 - val_loss: 0.1012 - val_acc: 0.9739\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0219 - acc: 0.9935 - val_loss: 0.1005 - val_acc: 0.9739\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0213 - acc: 0.9939 - val_loss: 0.1035 - val_acc: 0.9739\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0206 - acc: 0.9941 - val_loss: 0.1032 - val_acc: 0.9744\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0200 - acc: 0.9944 - val_loss: 0.1026 - val_acc: 0.9746\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 125us/step - loss: 0.0195 - acc: 0.9946 - val_loss: 0.1030 - val_acc: 0.9749\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.1038 - val_acc: 0.9749\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.1042 - val_acc: 0.9752\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0180 - acc: 0.9951 - val_loss: 0.1047 - val_acc: 0.9754\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0177 - acc: 0.9955 - val_loss: 0.1063 - val_acc: 0.9748\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0172 - acc: 0.9955 - val_loss: 0.1059 - val_acc: 0.9756\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0167 - acc: 0.9958 - val_loss: 0.1073 - val_acc: 0.9751\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0163 - acc: 0.9957 - val_loss: 0.1077 - val_acc: 0.9749\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0160 - acc: 0.9960 - val_loss: 0.1082 - val_acc: 0.9751\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.1086 - val_acc: 0.9757\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0151 - acc: 0.9962 - val_loss: 0.1100 - val_acc: 0.9753\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.1101 - val_acc: 0.9753\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 96us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.1102 - val_acc: 0.9756\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0141 - acc: 0.9967 - val_loss: 0.1108 - val_acc: 0.9754\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.0138 - acc: 0.9968 - val_loss: 0.1128 - val_acc: 0.9755\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.1121 - val_acc: 0.9753\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0131 - acc: 0.9971 - val_loss: 0.1125 - val_acc: 0.9754\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0130 - acc: 0.9971 - val_loss: 0.1134 - val_acc: 0.9755\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0127 - acc: 0.9972 - val_loss: 0.1136 - val_acc: 0.9755\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 0.0124 - acc: 0.9975 - val_loss: 0.1145 - val_acc: 0.9754\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0122 - acc: 0.9973 - val_loss: 0.1147 - val_acc: 0.9755\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0120 - acc: 0.9974 - val_loss: 0.1153 - val_acc: 0.9751\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.1160 - val_acc: 0.9753\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.1172 - val_acc: 0.9753\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0112 - acc: 0.9977 - val_loss: 0.1169 - val_acc: 0.9754\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0110 - acc: 0.9978 - val_loss: 0.1173 - val_acc: 0.9751\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0108 - acc: 0.9979 - val_loss: 0.1184 - val_acc: 0.9755\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 101us/step - loss: 0.0105 - acc: 0.9979 - val_loss: 0.1189 - val_acc: 0.9756\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 103us/step - loss: 0.0104 - acc: 0.9981 - val_loss: 0.1199 - val_acc: 0.9753\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 91us/step - loss: 0.0103 - acc: 0.9979 - val_loss: 0.1196 - val_acc: 0.9756\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0100 - acc: 0.9981 - val_loss: 0.1205 - val_acc: 0.9754\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.0099 - acc: 0.9981 - val_loss: 0.1211 - val_acc: 0.9751\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0097 - acc: 0.9982 - val_loss: 0.1213 - val_acc: 0.9752\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0094 - acc: 0.9982 - val_loss: 0.1227 - val_acc: 0.9755\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0093 - acc: 0.9984 - val_loss: 0.1227 - val_acc: 0.9755\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0091 - acc: 0.9984 - val_loss: 0.1229 - val_acc: 0.9753\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.0089 - acc: 0.9984 - val_loss: 0.1225 - val_acc: 0.9758\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0088 - acc: 0.9984 - val_loss: 0.1236 - val_acc: 0.9756\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0087 - acc: 0.9985 - val_loss: 0.1242 - val_acc: 0.9754\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 120us/step - loss: 0.0085 - acc: 0.9986 - val_loss: 0.1245 - val_acc: 0.9756\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 147us/step - loss: 0.0084 - acc: 0.9986 - val_loss: 0.1253 - val_acc: 0.9757\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 1s 417us/step - loss: 0.0083 - acc: 0.9986 - val_loss: 0.1242 - val_acc: 0.9756\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0082 - acc: 0.9986 - val_loss: 0.1258 - val_acc: 0.9761\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0080 - acc: 0.9986 - val_loss: 0.1268 - val_acc: 0.9757\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.1257 - val_acc: 0.9761\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 166us/step - loss: 0.0077 - acc: 0.9988 - val_loss: 0.1270 - val_acc: 0.9759\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.1271 - val_acc: 0.9758\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 88us/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.1275 - val_acc: 0.9757\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.1286 - val_acc: 0.9759\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 157us/step - loss: 0.0072 - acc: 0.9989 - val_loss: 0.1290 - val_acc: 0.9758\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 94us/step - loss: 0.0071 - acc: 0.9990 - val_loss: 0.1286 - val_acc: 0.9760\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 95us/step - loss: 0.0070 - acc: 0.9990 - val_loss: 0.1292 - val_acc: 0.9760\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 0.0069 - acc: 0.9990 - val_loss: 0.1297 - val_acc: 0.9757\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 93us/step - loss: 0.0068 - acc: 0.9990 - val_loss: 0.1303 - val_acc: 0.9760\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 0.0066 - acc: 0.9990 - val_loss: 0.1309 - val_acc: 0.9757\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 92us/step - loss: 0.0066 - acc: 0.9990 - val_loss: 0.1315 - val_acc: 0.9761\n",
      "\n",
      "Accuracy: 97.61%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 241us/step - loss: 0.6978 - acc: 0.5468 - val_loss: 0.6785 - val_acc: 0.5964\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 0.6693 - acc: 0.6124 - val_loss: 0.6571 - val_acc: 0.6276\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.6523 - acc: 0.6395 - val_loss: 0.6418 - val_acc: 0.6507\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.6368 - acc: 0.6617 - val_loss: 0.6267 - val_acc: 0.6715\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.6208 - acc: 0.6845 - val_loss: 0.6100 - val_acc: 0.6959\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.6029 - acc: 0.7072 - val_loss: 0.5924 - val_acc: 0.7224\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.5827 - acc: 0.7325 - val_loss: 0.5715 - val_acc: 0.7482\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.5604 - acc: 0.7537 - val_loss: 0.5476 - val_acc: 0.7741\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.5353 - acc: 0.7878 - val_loss: 0.5228 - val_acc: 0.8057\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.5080 - acc: 0.8149 - val_loss: 0.4960 - val_acc: 0.8312\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.4794 - acc: 0.8410 - val_loss: 0.4679 - val_acc: 0.8534\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.4498 - acc: 0.8655 - val_loss: 0.4390 - val_acc: 0.8725\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.4202 - acc: 0.8808 - val_loss: 0.4107 - val_acc: 0.8856\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.3916 - acc: 0.8985 - val_loss: 0.3833 - val_acc: 0.8974\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.3643 - acc: 0.9079 - val_loss: 0.3578 - val_acc: 0.9062\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.3390 - acc: 0.9197 - val_loss: 0.3343 - val_acc: 0.9142\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.3162 - acc: 0.9251 - val_loss: 0.3135 - val_acc: 0.9194\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.2959 - acc: 0.9302 - val_loss: 0.2948 - val_acc: 0.9236\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.2782 - acc: 0.9331 - val_loss: 0.2785 - val_acc: 0.9276\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.2624 - acc: 0.9360 - val_loss: 0.2639 - val_acc: 0.9310\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.2488 - acc: 0.9380 - val_loss: 0.2510 - val_acc: 0.9328\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.2373 - acc: 0.9401 - val_loss: 0.2401 - val_acc: 0.9347\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.2270 - acc: 0.9414 - val_loss: 0.2306 - val_acc: 0.9368\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.2180 - acc: 0.9427 - val_loss: 0.2224 - val_acc: 0.9386\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.2104 - acc: 0.9433 - val_loss: 0.2148 - val_acc: 0.9402\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.2037 - acc: 0.9442 - val_loss: 0.2086 - val_acc: 0.9411\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1979 - acc: 0.9448 - val_loss: 0.2027 - val_acc: 0.9422\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1928 - acc: 0.9455 - val_loss: 0.1977 - val_acc: 0.9431\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1884 - acc: 0.9462 - val_loss: 0.1937 - val_acc: 0.9438\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1843 - acc: 0.9467 - val_loss: 0.1895 - val_acc: 0.9444\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1808 - acc: 0.9472 - val_loss: 0.1860 - val_acc: 0.9451\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.1777 - acc: 0.9476 - val_loss: 0.1829 - val_acc: 0.9452\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 34us/step - loss: 0.1749 - acc: 0.9477 - val_loss: 0.1801 - val_acc: 0.9455\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1723 - acc: 0.9483 - val_loss: 0.1776 - val_acc: 0.9459\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1701 - acc: 0.9485 - val_loss: 0.1752 - val_acc: 0.9464\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1680 - acc: 0.9487 - val_loss: 0.1731 - val_acc: 0.9465\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1661 - acc: 0.9487 - val_loss: 0.1712 - val_acc: 0.9465\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1644 - acc: 0.9490 - val_loss: 0.1694 - val_acc: 0.9467\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.1628 - acc: 0.9491 - val_loss: 0.1678 - val_acc: 0.9468\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1614 - acc: 0.9491 - val_loss: 0.1663 - val_acc: 0.9469\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1600 - acc: 0.9492 - val_loss: 0.1650 - val_acc: 0.9471\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.1588 - acc: 0.9494 - val_loss: 0.1636 - val_acc: 0.9473\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1577 - acc: 0.9494 - val_loss: 0.1625 - val_acc: 0.9474\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1566 - acc: 0.9495 - val_loss: 0.1613 - val_acc: 0.9476\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1556 - acc: 0.9497 - val_loss: 0.1602 - val_acc: 0.9477\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1546 - acc: 0.9498 - val_loss: 0.1594 - val_acc: 0.9480\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.1538 - acc: 0.9498 - val_loss: 0.1584 - val_acc: 0.9482\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1530 - acc: 0.9499 - val_loss: 0.1575 - val_acc: 0.9483\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1522 - acc: 0.9501 - val_loss: 0.1568 - val_acc: 0.9483\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1514 - acc: 0.9502 - val_loss: 0.1560 - val_acc: 0.9485\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.1507 - acc: 0.9502 - val_loss: 0.1552 - val_acc: 0.9487\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 33us/step - loss: 0.1501 - acc: 0.9502 - val_loss: 0.1546 - val_acc: 0.9489\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1495 - acc: 0.9503 - val_loss: 0.1540 - val_acc: 0.9487\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1489 - acc: 0.9503 - val_loss: 0.1533 - val_acc: 0.9489\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1483 - acc: 0.9504 - val_loss: 0.1528 - val_acc: 0.9490\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1478 - acc: 0.9503 - val_loss: 0.1522 - val_acc: 0.9490\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1472 - acc: 0.9504 - val_loss: 0.1517 - val_acc: 0.9489\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1468 - acc: 0.9504 - val_loss: 0.1512 - val_acc: 0.9488\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1463 - acc: 0.9504 - val_loss: 0.1507 - val_acc: 0.9488\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1458 - acc: 0.9504 - val_loss: 0.1502 - val_acc: 0.9491\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1454 - acc: 0.9505 - val_loss: 0.1498 - val_acc: 0.9490\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1450 - acc: 0.9505 - val_loss: 0.1494 - val_acc: 0.9491\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1446 - acc: 0.9505 - val_loss: 0.1489 - val_acc: 0.9492\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1442 - acc: 0.9507 - val_loss: 0.1486 - val_acc: 0.9493\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.1438 - acc: 0.9508 - val_loss: 0.1482 - val_acc: 0.9493\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 32us/step - loss: 0.1434 - acc: 0.9506 - val_loss: 0.1478 - val_acc: 0.9494\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1431 - acc: 0.9508 - val_loss: 0.1475 - val_acc: 0.9494\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1428 - acc: 0.9509 - val_loss: 0.1471 - val_acc: 0.9496\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 0.1424 - acc: 0.9508 - val_loss: 0.1468 - val_acc: 0.9497\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.1421 - acc: 0.9509 - val_loss: 0.1465 - val_acc: 0.9497\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1418 - acc: 0.9509 - val_loss: 0.1462 - val_acc: 0.9497\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1415 - acc: 0.9510 - val_loss: 0.1459 - val_acc: 0.9498\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1412 - acc: 0.9509 - val_loss: 0.1456 - val_acc: 0.9498\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1409 - acc: 0.9510 - val_loss: 0.1454 - val_acc: 0.9498\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1407 - acc: 0.9510 - val_loss: 0.1451 - val_acc: 0.9499\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.1404 - acc: 0.9510 - val_loss: 0.1448 - val_acc: 0.9499\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1401 - acc: 0.9511 - val_loss: 0.1446 - val_acc: 0.9499\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.1399 - acc: 0.9511 - val_loss: 0.1443 - val_acc: 0.9501\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.1397 - acc: 0.9511 - val_loss: 0.1441 - val_acc: 0.9502\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1394 - acc: 0.9512 - val_loss: 0.1439 - val_acc: 0.9503\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1392 - acc: 0.9512 - val_loss: 0.1436 - val_acc: 0.9504\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.1389 - acc: 0.9513 - val_loss: 0.1434 - val_acc: 0.9505\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.1387 - acc: 0.9513 - val_loss: 0.1432 - val_acc: 0.9505\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 99us/step - loss: 0.1385 - acc: 0.9513 - val_loss: 0.1430 - val_acc: 0.9505\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.1383 - acc: 0.9513 - val_loss: 0.1428 - val_acc: 0.9505\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - ETA: 0s - loss: 0.1385 - acc: 0.951 - 1s 301us/step - loss: 0.1381 - acc: 0.9513 - val_loss: 0.1426 - val_acc: 0.9505\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 0.1379 - acc: 0.9513 - val_loss: 0.1424 - val_acc: 0.9506\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.1377 - acc: 0.9514 - val_loss: 0.1422 - val_acc: 0.9506\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.1375 - acc: 0.9514 - val_loss: 0.1420 - val_acc: 0.9506\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.1373 - acc: 0.9514 - val_loss: 0.1419 - val_acc: 0.9506\n",
      "\n",
      "Accuracy: 95.06%\n",
      "\n",
      "37457\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/90\n",
      "2682/2682 [==============================] - 1s 308us/step - loss: 0.2145 - acc: 0.9342 - val_loss: 0.1516 - val_acc: 0.9524\n",
      "Epoch 2/90\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.1372 - acc: 0.9533 - val_loss: 0.1337 - val_acc: 0.9538\n",
      "Epoch 3/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.1189 - acc: 0.9575 - val_loss: 0.1223 - val_acc: 0.9568\n",
      "Epoch 4/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.1028 - acc: 0.9623 - val_loss: 0.1156 - val_acc: 0.9578\n",
      "Epoch 5/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0928 - acc: 0.9654 - val_loss: 0.1137 - val_acc: 0.9584\n",
      "Epoch 6/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0872 - acc: 0.9674 - val_loss: 0.1141 - val_acc: 0.9598\n",
      "Epoch 7/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0797 - acc: 0.9700 - val_loss: 0.1052 - val_acc: 0.9615\n",
      "Epoch 8/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0727 - acc: 0.9725 - val_loss: 0.1027 - val_acc: 0.9627\n",
      "Epoch 9/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0680 - acc: 0.9745 - val_loss: 0.1016 - val_acc: 0.9632\n",
      "Epoch 10/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0636 - acc: 0.9763 - val_loss: 0.0986 - val_acc: 0.9639\n",
      "Epoch 11/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0595 - acc: 0.9778 - val_loss: 0.0994 - val_acc: 0.9655\n",
      "Epoch 12/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0569 - acc: 0.9786 - val_loss: 0.0980 - val_acc: 0.9664\n",
      "Epoch 13/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0542 - acc: 0.9800 - val_loss: 0.0978 - val_acc: 0.9675\n",
      "Epoch 14/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0515 - acc: 0.9809 - val_loss: 0.0975 - val_acc: 0.9676\n",
      "Epoch 15/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0979 - val_acc: 0.9678\n",
      "Epoch 16/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0466 - acc: 0.9834 - val_loss: 0.0961 - val_acc: 0.9690\n",
      "Epoch 17/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0446 - acc: 0.9841 - val_loss: 0.0959 - val_acc: 0.9691\n",
      "Epoch 18/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0432 - acc: 0.9847 - val_loss: 0.0961 - val_acc: 0.9693\n",
      "Epoch 19/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0403 - acc: 0.9861 - val_loss: 0.0964 - val_acc: 0.9698\n",
      "Epoch 20/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0385 - acc: 0.9868 - val_loss: 0.0958 - val_acc: 0.9701\n",
      "Epoch 21/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0371 - acc: 0.9875 - val_loss: 0.0951 - val_acc: 0.9707\n",
      "Epoch 22/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0356 - acc: 0.9884 - val_loss: 0.0970 - val_acc: 0.9696\n",
      "Epoch 23/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0348 - acc: 0.9884 - val_loss: 0.0966 - val_acc: 0.9706\n",
      "Epoch 24/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0333 - acc: 0.9889 - val_loss: 0.0951 - val_acc: 0.9714\n",
      "Epoch 25/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0957 - val_acc: 0.9710\n",
      "Epoch 26/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0308 - acc: 0.9900 - val_loss: 0.0965 - val_acc: 0.9721\n",
      "Epoch 27/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0298 - acc: 0.9907 - val_loss: 0.0964 - val_acc: 0.9723\n",
      "Epoch 28/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.0961 - val_acc: 0.9722\n",
      "Epoch 29/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0278 - acc: 0.9914 - val_loss: 0.0966 - val_acc: 0.9726\n",
      "Epoch 30/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0269 - acc: 0.9917 - val_loss: 0.0978 - val_acc: 0.9724\n",
      "Epoch 31/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0261 - acc: 0.9920 - val_loss: 0.0969 - val_acc: 0.9733\n",
      "Epoch 32/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0252 - acc: 0.9926 - val_loss: 0.0966 - val_acc: 0.9727\n",
      "Epoch 33/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0245 - acc: 0.9929 - val_loss: 0.0981 - val_acc: 0.9738\n",
      "Epoch 34/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0973 - val_acc: 0.9733\n",
      "Epoch 35/90\n",
      "2682/2682 [==============================] - 0s 35us/step - loss: 0.0231 - acc: 0.9934 - val_loss: 0.0971 - val_acc: 0.9734\n",
      "Epoch 36/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.0984 - val_acc: 0.9735\n",
      "Epoch 37/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0218 - acc: 0.9939 - val_loss: 0.0989 - val_acc: 0.9737\n",
      "Epoch 38/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0213 - acc: 0.9942 - val_loss: 0.0996 - val_acc: 0.9740\n",
      "Epoch 39/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0208 - acc: 0.9944 - val_loss: 0.1009 - val_acc: 0.9734\n",
      "Epoch 40/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 0.0993 - val_acc: 0.9746\n",
      "Epoch 41/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0196 - acc: 0.9948 - val_loss: 0.1015 - val_acc: 0.9739\n",
      "Epoch 42/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.1020 - val_acc: 0.9747\n",
      "Epoch 43/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0189 - acc: 0.9953 - val_loss: 0.1019 - val_acc: 0.9749\n",
      "Epoch 44/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0181 - acc: 0.9951 - val_loss: 0.1030 - val_acc: 0.9753\n",
      "Epoch 45/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0178 - acc: 0.9955 - val_loss: 0.1027 - val_acc: 0.9747\n",
      "Epoch 46/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0174 - acc: 0.9955 - val_loss: 0.1028 - val_acc: 0.9749\n",
      "Epoch 47/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0170 - acc: 0.9957 - val_loss: 0.1029 - val_acc: 0.9745\n",
      "Epoch 48/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0165 - acc: 0.9960 - val_loss: 0.1044 - val_acc: 0.9742\n",
      "Epoch 49/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.1034 - val_acc: 0.9744\n",
      "Epoch 50/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0159 - acc: 0.9960 - val_loss: 0.1039 - val_acc: 0.9747\n",
      "Epoch 51/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.1051 - val_acc: 0.9744\n",
      "Epoch 52/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0151 - acc: 0.9962 - val_loss: 0.1049 - val_acc: 0.9755\n",
      "Epoch 53/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0148 - acc: 0.9965 - val_loss: 0.1053 - val_acc: 0.9745\n",
      "Epoch 54/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.1056 - val_acc: 0.9755\n",
      "Epoch 55/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.1064 - val_acc: 0.9751\n",
      "Epoch 56/90\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0140 - acc: 0.9966 - val_loss: 0.1066 - val_acc: 0.9751\n",
      "Epoch 57/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0136 - acc: 0.9968 - val_loss: 0.1076 - val_acc: 0.9753\n",
      "Epoch 58/90\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0133 - acc: 0.9971 - val_loss: 0.1072 - val_acc: 0.9749\n",
      "Epoch 59/90\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0131 - acc: 0.9971 - val_loss: 0.1079 - val_acc: 0.9751\n",
      "Epoch 60/90\n",
      "2682/2682 [==============================] - 0s 44us/step - loss: 0.0128 - acc: 0.9971 - val_loss: 0.1080 - val_acc: 0.9753\n",
      "Epoch 61/90\n",
      "2682/2682 [==============================] - 0s 46us/step - loss: 0.0126 - acc: 0.9974 - val_loss: 0.1093 - val_acc: 0.9751\n",
      "Epoch 62/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0125 - acc: 0.9972 - val_loss: 0.1094 - val_acc: 0.9751\n",
      "Epoch 63/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0121 - acc: 0.9973 - val_loss: 0.1104 - val_acc: 0.9756\n",
      "Epoch 64/90\n",
      "2682/2682 [==============================] - 0s 45us/step - loss: 0.0118 - acc: 0.9974 - val_loss: 0.1099 - val_acc: 0.9756\n",
      "Epoch 65/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.1109 - val_acc: 0.9755\n",
      "Epoch 66/90\n",
      "2682/2682 [==============================] - 0s 48us/step - loss: 0.0115 - acc: 0.9975 - val_loss: 0.1097 - val_acc: 0.9754\n",
      "Epoch 67/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0112 - acc: 0.9977 - val_loss: 0.1109 - val_acc: 0.9757\n",
      "Epoch 68/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0111 - acc: 0.9977 - val_loss: 0.1115 - val_acc: 0.9755\n",
      "Epoch 69/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0108 - acc: 0.9978 - val_loss: 0.1115 - val_acc: 0.9758\n",
      "Epoch 70/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0107 - acc: 0.9978 - val_loss: 0.1123 - val_acc: 0.9753\n",
      "Epoch 71/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0105 - acc: 0.9979 - val_loss: 0.1124 - val_acc: 0.9753\n",
      "Epoch 72/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0103 - acc: 0.9981 - val_loss: 0.1126 - val_acc: 0.9764\n",
      "Epoch 73/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0102 - acc: 0.9980 - val_loss: 0.1133 - val_acc: 0.9755\n",
      "Epoch 74/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0100 - acc: 0.9982 - val_loss: 0.1138 - val_acc: 0.9755\n",
      "Epoch 75/90\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 0.0097 - acc: 0.9982 - val_loss: 0.1138 - val_acc: 0.9758\n",
      "Epoch 76/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.1143 - val_acc: 0.9764\n",
      "Epoch 77/90\n",
      "2682/2682 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9984 - val_loss: 0.1145 - val_acc: 0.9763\n",
      "Epoch 78/90\n",
      "2682/2682 [==============================] - 0s 36us/step - loss: 0.0093 - acc: 0.9985 - val_loss: 0.1151 - val_acc: 0.9760\n",
      "Epoch 79/90\n",
      "2682/2682 [==============================] - 0s 37us/step - loss: 0.0092 - acc: 0.9984 - val_loss: 0.1152 - val_acc: 0.9762\n",
      "Epoch 80/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0089 - acc: 0.9985 - val_loss: 0.1163 - val_acc: 0.9760\n",
      "Epoch 81/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0088 - acc: 0.9985 - val_loss: 0.1166 - val_acc: 0.9763\n",
      "Epoch 82/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.1169 - val_acc: 0.9765\n",
      "Epoch 83/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0086 - acc: 0.9987 - val_loss: 0.1171 - val_acc: 0.9762\n",
      "Epoch 84/90\n",
      "2682/2682 [==============================] - 0s 43us/step - loss: 0.0084 - acc: 0.9987 - val_loss: 0.1180 - val_acc: 0.9760\n",
      "Epoch 85/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0083 - acc: 0.9987 - val_loss: 0.1176 - val_acc: 0.9763\n",
      "Epoch 86/90\n",
      "2682/2682 [==============================] - 0s 42us/step - loss: 0.0082 - acc: 0.9987 - val_loss: 0.1180 - val_acc: 0.9763\n",
      "Epoch 87/90\n",
      "2682/2682 [==============================] - 0s 41us/step - loss: 0.0080 - acc: 0.9988 - val_loss: 0.1187 - val_acc: 0.9760\n",
      "Epoch 88/90\n",
      "2682/2682 [==============================] - 0s 40us/step - loss: 0.0080 - acc: 0.9988 - val_loss: 0.1194 - val_acc: 0.9762\n",
      "Epoch 89/90\n",
      "2682/2682 [==============================] - 0s 39us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.1189 - val_acc: 0.9762\n",
      "Epoch 90/90\n",
      "2682/2682 [==============================] - 0s 38us/step - loss: 0.0077 - acc: 0.9989 - val_loss: 0.1188 - val_acc: 0.9762\n",
      "\n",
      "Accuracy: 97.62%\n",
      "\n",
      "37457\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "gp_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_parameters)\n",
    "\"\"\"\n",
    "gbrt_result = gbrt_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=50,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gp_result.x\n",
    "search_result=gbrt_result\n",
    "plot_convergence(search_result)\n",
    "search_result.x\n",
    "#space.point_to_dict(search_result.x)\n",
    "search_result.fun\n",
    "aaaa=sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(search_result.x_iters,columns =['Learning Rate', 'Numero de capas ocultas', 'Neuronas en 1 capa oculta', 'Neuronas en 2 capa oculta', 'Neuronas en 3 capa oculta', 'funcion activacion', 'Tamano de batch ','ADAM decay']) \n",
    "df['acc']=search_result.func_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9778161079068727"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('optimizacion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x000001B8E50F4DD8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Programas\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x000001B8E56C1240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Programas\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84192\n",
      "Train on 2682 samples, validate on 474 samples\n",
      "Epoch 1/150\n",
      "2682/2682 [==============================] - 1s 376us/step - loss: 0.2412 - acc: 0.9284 - val_loss: 0.1627 - val_acc: 0.9514\n",
      "Epoch 2/150\n",
      "2682/2682 [==============================] - 0s 77us/step - loss: 0.1456 - acc: 0.9520 - val_loss: 0.1351 - val_acc: 0.9538\n",
      "Epoch 3/150\n",
      "2682/2682 [==============================] - 0s 81us/step - loss: 0.1218 - acc: 0.9548 - val_loss: 0.1283 - val_acc: 0.9557\n",
      "Epoch 4/150\n",
      "2682/2682 [==============================] - 0s 123us/step - loss: 0.1108 - acc: 0.9585 - val_loss: 0.1198 - val_acc: 0.9561\n",
      "Epoch 5/150\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0974 - acc: 0.9627 - val_loss: 0.1099 - val_acc: 0.9595\n",
      "Epoch 6/150\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 0.0898 - acc: 0.9656 - val_loss: 0.1135 - val_acc: 0.9600\n",
      "Epoch 7/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0807 - acc: 0.9688 - val_loss: 0.1119 - val_acc: 0.9616\n",
      "Epoch 8/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0712 - acc: 0.9728 - val_loss: 0.1054 - val_acc: 0.9643\n",
      "Epoch 9/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0636 - acc: 0.9755 - val_loss: 0.0969 - val_acc: 0.9660\n",
      "Epoch 10/150\n",
      "2682/2682 [==============================] - 0s 144us/step - loss: 0.0562 - acc: 0.9784 - val_loss: 0.1019 - val_acc: 0.9661\n",
      "Epoch 11/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0510 - acc: 0.9807 - val_loss: 0.1060 - val_acc: 0.9660\n",
      "Epoch 12/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0463 - acc: 0.9822 - val_loss: 0.1044 - val_acc: 0.9684\n",
      "Epoch 13/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0414 - acc: 0.9845 - val_loss: 0.1069 - val_acc: 0.9696\n",
      "Epoch 14/150\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 0.0374 - acc: 0.9862 - val_loss: 0.1156 - val_acc: 0.9687\n",
      "Epoch 15/150\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 0.0375 - acc: 0.9857 - val_loss: 0.1099 - val_acc: 0.9705\n",
      "Epoch 16/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0316 - acc: 0.9884 - val_loss: 0.1160 - val_acc: 0.9682\n",
      "Epoch 17/150\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0302 - acc: 0.9890 - val_loss: 0.1137 - val_acc: 0.9714\n",
      "Epoch 18/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0245 - acc: 0.9915 - val_loss: 0.1139 - val_acc: 0.9739\n",
      "Epoch 19/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0197 - acc: 0.9933 - val_loss: 0.1224 - val_acc: 0.9738\n",
      "Epoch 20/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0162 - acc: 0.9945 - val_loss: 0.1240 - val_acc: 0.9746\n",
      "Epoch 21/150\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.1259 - val_acc: 0.9744\n",
      "Epoch 22/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 0.1309 - val_acc: 0.9759\n",
      "Epoch 23/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0109 - acc: 0.9968 - val_loss: 0.1298 - val_acc: 0.9755\n",
      "Epoch 24/150\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.1334 - val_acc: 0.9758\n",
      "Epoch 25/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.1397 - val_acc: 0.9753\n",
      "Epoch 26/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.1401 - val_acc: 0.9760\n",
      "Epoch 27/150\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.1440 - val_acc: 0.9761\n",
      "Epoch 28/150\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1468 - val_acc: 0.9764\n",
      "Epoch 29/150\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1474 - val_acc: 0.9761\n",
      "Epoch 30/150\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1514 - val_acc: 0.9759\n",
      "Epoch 31/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1549 - val_acc: 0.9758\n",
      "Epoch 32/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1551 - val_acc: 0.9765\n",
      "Epoch 33/150\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1587 - val_acc: 0.9763\n",
      "Epoch 34/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1605 - val_acc: 0.9768\n",
      "Epoch 35/150\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1636 - val_acc: 0.9763\n",
      "Epoch 36/150\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1619 - val_acc: 0.9765\n",
      "Epoch 37/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1653 - val_acc: 0.9768\n",
      "Epoch 38/150\n",
      "2682/2682 [==============================] - 0s 80us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1661 - val_acc: 0.9765\n",
      "Epoch 39/150\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1674 - val_acc: 0.9769\n",
      "Epoch 40/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1704 - val_acc: 0.9768\n",
      "Epoch 41/150\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1691 - val_acc: 0.9766\n",
      "Epoch 42/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1722 - val_acc: 0.9766\n",
      "Epoch 43/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1713 - val_acc: 0.9770\n",
      "Epoch 44/150\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 9.4197e-04 - acc: 0.9998 - val_loss: 0.1746 - val_acc: 0.9763\n",
      "Epoch 45/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 9.4978e-04 - acc: 0.9998 - val_loss: 0.1749 - val_acc: 0.9764\n",
      "Epoch 46/150\n",
      "2682/2682 [==============================] - 0s 74us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1753 - val_acc: 0.9769\n",
      "Epoch 47/150\n",
      "2682/2682 [==============================] - 0s 73us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1788 - val_acc: 0.9766\n",
      "Epoch 48/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 7.7249e-04 - acc: 0.9999 - val_loss: 0.1768 - val_acc: 0.9766\n",
      "Epoch 49/150\n",
      "2682/2682 [==============================] - 0s 82us/step - loss: 6.7727e-04 - acc: 0.9999 - val_loss: 0.1780 - val_acc: 0.9758\n",
      "Epoch 50/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 8.9626e-04 - acc: 0.9998 - val_loss: 0.1795 - val_acc: 0.9769\n",
      "Epoch 51/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 8.4586e-04 - acc: 0.9998 - val_loss: 0.1787 - val_acc: 0.9768\n",
      "Epoch 52/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 7.2557e-04 - acc: 0.9999 - val_loss: 0.1791 - val_acc: 0.9767\n",
      "Epoch 53/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 5.7037e-04 - acc: 0.9999 - val_loss: 0.1808 - val_acc: 0.9764\n",
      "Epoch 54/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 4.8209e-04 - acc: 0.9999 - val_loss: 0.1811 - val_acc: 0.9765\n",
      "Epoch 55/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 4.1706e-04 - acc: 1.0000 - val_loss: 0.1827 - val_acc: 0.9763\n",
      "Epoch 56/150\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 4.2622e-04 - acc: 0.9999 - val_loss: 0.1826 - val_acc: 0.9765\n",
      "Epoch 57/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 5.2078e-04 - acc: 0.9999 - val_loss: 0.1834 - val_acc: 0.9765\n",
      "Epoch 58/150\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 7.1846e-04 - acc: 0.9998 - val_loss: 0.1839 - val_acc: 0.9765\n",
      "Epoch 59/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 8.5263e-04 - acc: 0.9999 - val_loss: 0.1873 - val_acc: 0.9760\n",
      "Epoch 60/150\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 7.9490e-04 - acc: 0.9998 - val_loss: 0.1865 - val_acc: 0.9763\n",
      "Epoch 61/150\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 8.6490e-04 - acc: 0.9998 - val_loss: 0.1871 - val_acc: 0.9762\n",
      "Epoch 62/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 5.9524e-04 - acc: 0.9999 - val_loss: 0.1900 - val_acc: 0.9766\n",
      "Epoch 63/150\n",
      "2682/2682 [==============================] - 0s 51us/step - loss: 9.2740e-04 - acc: 0.9998 - val_loss: 0.1849 - val_acc: 0.9765\n",
      "Epoch 64/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 4.8544e-04 - acc: 0.9999 - val_loss: 0.1880 - val_acc: 0.9765\n",
      "Epoch 65/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 4.1103e-04 - acc: 0.9999 - val_loss: 0.1877 - val_acc: 0.9760\n",
      "Epoch 66/150\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 3.1508e-04 - acc: 1.0000 - val_loss: 0.1880 - val_acc: 0.9760\n",
      "Epoch 67/150\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 3.3465e-04 - acc: 0.9999 - val_loss: 0.1888 - val_acc: 0.9761\n",
      "Epoch 68/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 2.8355e-04 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9763\n",
      "Epoch 69/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 2.3648e-04 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 0.9761\n",
      "Epoch 70/150\n",
      "2682/2682 [==============================] - 0s 86us/step - loss: 4.2859e-04 - acc: 0.9999 - val_loss: 0.1907 - val_acc: 0.9762\n",
      "Epoch 71/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 7.3756e-04 - acc: 0.9997 - val_loss: 0.1967 - val_acc: 0.9761\n",
      "Epoch 72/150\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 9.2245e-04 - acc: 0.9998 - val_loss: 0.1948 - val_acc: 0.9761\n",
      "Epoch 73/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 9.2235e-04 - acc: 0.9998 - val_loss: 0.1954 - val_acc: 0.9767\n",
      "Epoch 74/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 4.8871e-04 - acc: 0.9999 - val_loss: 0.1948 - val_acc: 0.9763\n",
      "Epoch 75/150\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 4.3438e-04 - acc: 0.9999 - val_loss: 0.1931 - val_acc: 0.9764\n",
      "Epoch 76/150\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 8.5040e-04 - acc: 0.9999 - val_loss: 0.1910 - val_acc: 0.9765\n",
      "Epoch 77/150\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1948 - val_acc: 0.9762\n",
      "Epoch 78/150\n",
      "2682/2682 [==============================] - 0s 52us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1950 - val_acc: 0.9760\n",
      "Epoch 79/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.1924 - val_acc: 0.9751\n",
      "Epoch 80/150\n",
      "2682/2682 [==============================] - 0s 61us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.1891 - val_acc: 0.9759\n",
      "Epoch 81/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1907 - val_acc: 0.9751\n",
      "Epoch 82/150\n",
      "2682/2682 [==============================] - 0s 100us/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.1863 - val_acc: 0.9743\n",
      "Epoch 83/150\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 0.1939 - val_acc: 0.9742\n",
      "Epoch 84/150\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.1853 - val_acc: 0.9762\n",
      "Epoch 85/150\n",
      "2682/2682 [==============================] - 1s 320us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.1890 - val_acc: 0.9753\n",
      "Epoch 86/150\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1891 - val_acc: 0.9762\n",
      "Epoch 87/150\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 0.0032 - acc: 0.9996 - val_loss: 0.1858 - val_acc: 0.9765\n",
      "Epoch 88/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1880 - val_acc: 0.9765\n",
      "Epoch 89/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1860 - val_acc: 0.9768\n",
      "Epoch 90/150\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1884 - val_acc: 0.9770\n",
      "Epoch 91/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 9.7265e-04 - acc: 0.9999 - val_loss: 0.1890 - val_acc: 0.9769\n",
      "Epoch 92/150\n",
      "2682/2682 [==============================] - 0s 107us/step - loss: 9.4584e-04 - acc: 1.0000 - val_loss: 0.1899 - val_acc: 0.9767\n",
      "Epoch 93/150\n",
      "2682/2682 [==============================] - 0s 89us/step - loss: 9.3170e-04 - acc: 0.9999 - val_loss: 0.1898 - val_acc: 0.9769\n",
      "Epoch 94/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 9.1936e-04 - acc: 1.0000 - val_loss: 0.1915 - val_acc: 0.9769\n",
      "Epoch 95/150\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 9.1961e-04 - acc: 0.9999 - val_loss: 0.1914 - val_acc: 0.9769\n",
      "Epoch 96/150\n",
      "2682/2682 [==============================] - 0s 66us/step - loss: 8.9634e-04 - acc: 1.0000 - val_loss: 0.1927 - val_acc: 0.9768\n",
      "Epoch 97/150\n",
      "2682/2682 [==============================] - 0s 112us/step - loss: 8.8582e-04 - acc: 1.0000 - val_loss: 0.1927 - val_acc: 0.9768\n",
      "Epoch 98/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 8.7800e-04 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9769\n",
      "Epoch 99/150\n",
      "2682/2682 [==============================] - 0s 97us/step - loss: 8.7130e-04 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9769\n",
      "Epoch 100/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 8.6614e-04 - acc: 1.0000 - val_loss: 0.1941 - val_acc: 0.9769\n",
      "Epoch 101/150\n",
      "2682/2682 [==============================] - 0s 87us/step - loss: 8.6516e-04 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.9769\n",
      "Epoch 102/150\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 8.5625e-04 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 0.9768\n",
      "Epoch 103/150\n",
      "2682/2682 [==============================] - 0s 83us/step - loss: 8.5116e-04 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.9769\n",
      "Epoch 104/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 8.4802e-04 - acc: 1.0000 - val_loss: 0.1954 - val_acc: 0.9769\n",
      "Epoch 105/150\n",
      "2682/2682 [==============================] - 0s 90us/step - loss: 8.4520e-04 - acc: 1.0000 - val_loss: 0.1954 - val_acc: 0.9769\n",
      "Epoch 106/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 8.3927e-04 - acc: 1.0000 - val_loss: 0.1960 - val_acc: 0.9769\n",
      "Epoch 107/150\n",
      "2682/2682 [==============================] - 0s 85us/step - loss: 8.4214e-04 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 0.9770\n",
      "Epoch 108/150\n",
      "2682/2682 [==============================] - 0s 55us/step - loss: 8.3996e-04 - acc: 0.9999 - val_loss: 0.1969 - val_acc: 0.9769\n",
      "Epoch 109/150\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 8.3067e-04 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9771\n",
      "Epoch 110/150\n",
      "2682/2682 [==============================] - 0s 78us/step - loss: 8.3198e-04 - acc: 1.0000 - val_loss: 0.1977 - val_acc: 0.9768\n",
      "Epoch 111/150\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 8.3221e-04 - acc: 1.0000 - val_loss: 0.1973 - val_acc: 0.9770\n",
      "Epoch 112/150\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 8.2415e-04 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9769\n",
      "Epoch 113/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 8.2180e-04 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9770\n",
      "Epoch 114/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 8.2061e-04 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 0.9769\n",
      "Epoch 115/150\n",
      "2682/2682 [==============================] - 0s 58us/step - loss: 8.1633e-04 - acc: 1.0000 - val_loss: 0.1985 - val_acc: 0.9769\n",
      "Epoch 116/150\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 8.1635e-04 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9769\n",
      "Epoch 117/150\n",
      "2682/2682 [==============================] - 0s 98us/step - loss: 8.1357e-04 - acc: 1.0000 - val_loss: 0.1991 - val_acc: 0.9769\n",
      "Epoch 118/150\n",
      "2682/2682 [==============================] - 0s 69us/step - loss: 8.1085e-04 - acc: 1.0000 - val_loss: 0.1992 - val_acc: 0.9767\n",
      "Epoch 119/150\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 8.0983e-04 - acc: 1.0000 - val_loss: 0.1996 - val_acc: 0.9769\n",
      "Epoch 120/150\n",
      "2682/2682 [==============================] - 0s 54us/step - loss: 8.0612e-04 - acc: 1.0000 - val_loss: 0.1997 - val_acc: 0.9769\n",
      "Epoch 121/150\n",
      "2682/2682 [==============================] - 0s 50us/step - loss: 8.0716e-04 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 0.9769\n",
      "Epoch 122/150\n",
      "2682/2682 [==============================] - 0s 57us/step - loss: 8.0541e-04 - acc: 1.0000 - val_loss: 0.2000 - val_acc: 0.9769\n",
      "Epoch 123/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 8.0277e-04 - acc: 1.0000 - val_loss: 0.2004 - val_acc: 0.9770\n",
      "Epoch 124/150\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 8.0042e-04 - acc: 1.0000 - val_loss: 0.2008 - val_acc: 0.9769\n",
      "Epoch 125/150\n",
      "2682/2682 [==============================] - 0s 75us/step - loss: 8.0065e-04 - acc: 1.0000 - val_loss: 0.2007 - val_acc: 0.9770\n",
      "Epoch 126/150\n",
      "2682/2682 [==============================] - 0s 72us/step - loss: 7.9826e-04 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9770\n",
      "Epoch 127/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 7.9672e-04 - acc: 1.0000 - val_loss: 0.2012 - val_acc: 0.9771\n",
      "Epoch 128/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 7.9565e-04 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 0.9771\n",
      "Epoch 129/150\n",
      "2682/2682 [==============================] - 0s 71us/step - loss: 7.9606e-04 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 0.9770\n",
      "Epoch 130/150\n",
      "2682/2682 [==============================] - 0s 62us/step - loss: 7.9436e-04 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.9771\n",
      "Epoch 131/150\n",
      "2682/2682 [==============================] - 0s 53us/step - loss: 7.9300e-04 - acc: 1.0000 - val_loss: 0.2016 - val_acc: 0.9770\n",
      "Epoch 132/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 7.9271e-04 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.9770\n",
      "Epoch 133/150\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 7.9071e-04 - acc: 1.0000 - val_loss: 0.2023 - val_acc: 0.9770\n",
      "Epoch 134/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 7.9174e-04 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.9770\n",
      "Epoch 135/150\n",
      "2682/2682 [==============================] - 0s 65us/step - loss: 7.9219e-04 - acc: 1.0000 - val_loss: 0.2030 - val_acc: 0.9771\n",
      "Epoch 136/150\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 7.9501e-04 - acc: 1.0000 - val_loss: 0.2032 - val_acc: 0.9770\n",
      "Epoch 137/150\n",
      "2682/2682 [==============================] - 0s 64us/step - loss: 7.8659e-04 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 0.9770\n",
      "Epoch 138/150\n",
      "2682/2682 [==============================] - 0s 60us/step - loss: 7.8690e-04 - acc: 1.0000 - val_loss: 0.2037 - val_acc: 0.9769\n",
      "Epoch 139/150\n",
      "2682/2682 [==============================] - 0s 67us/step - loss: 7.8582e-04 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.9769\n",
      "Epoch 140/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 7.8374e-04 - acc: 1.0000 - val_loss: 0.2038 - val_acc: 0.9770\n",
      "Epoch 141/150\n",
      "2682/2682 [==============================] - 0s 70us/step - loss: 7.8341e-04 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9770\n",
      "Epoch 142/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 7.8301e-04 - acc: 1.0000 - val_loss: 0.2040 - val_acc: 0.9770\n",
      "Epoch 143/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 7.8266e-04 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.9770\n",
      "Epoch 144/150\n",
      "2682/2682 [==============================] - 0s 76us/step - loss: 7.8121e-04 - acc: 1.0000 - val_loss: 0.2044 - val_acc: 0.9769\n",
      "Epoch 145/150\n",
      "2682/2682 [==============================] - 0s 59us/step - loss: 7.8238e-04 - acc: 1.0000 - val_loss: 0.2046 - val_acc: 0.9770\n",
      "Epoch 146/150\n",
      "2682/2682 [==============================] - 0s 68us/step - loss: 7.7922e-04 - acc: 1.0000 - val_loss: 0.2048 - val_acc: 0.9768\n",
      "Epoch 147/150\n",
      "2682/2682 [==============================] - 0s 79us/step - loss: 7.7869e-04 - acc: 1.0000 - val_loss: 0.2051 - val_acc: 0.9769\n",
      "Epoch 148/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 7.7917e-04 - acc: 1.0000 - val_loss: 0.2051 - val_acc: 0.9767\n",
      "Epoch 149/150\n",
      "2682/2682 [==============================] - 0s 63us/step - loss: 7.7748e-04 - acc: 1.0000 - val_loss: 0.2053 - val_acc: 0.9769\n",
      "Epoch 150/150\n",
      "2682/2682 [==============================] - 0s 56us/step - loss: 7.7713e-04 - acc: 1.0000 - val_loss: 0.2056 - val_acc: 0.9770\n"
     ]
    }
   ],
   "source": [
    "#del model\n",
    "gbrt_result2 = [0.00812947, 2, 134, 299, 39, 'tanh', 94, 7.499189092642762e-05]\n",
    "reset_keras()\n",
    "model = create_model(gbrt_result[0],gbrt_result[1],gbrt_result[2],gbrt_result[3],gbrt_result[4],gbrt_result[5],gbrt_result[7])\n",
    "history=model.fit(X_train,y_train, epochs=150,batch_size=gbrt_result[6], verbose=1,validation_split=0.15)#model.evaluate(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 99)                9900      \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 277)               27700     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 273)               75894     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 33)                9042      \n",
      "=================================================================\n",
      "Total params: 122,536\n",
      "Trainable params: 122,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(Ypred[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=0\n",
    "for i in range (Ypred.shape[0]):\n",
    "    if (np.round(Ypred[i])==Ytest[i]).all():\n",
    "        aux=aux+1\n",
    "    else:\n",
    "        a=(np.round(Ypred[i])==Ytest[i])\n",
    "        unique, counts = np.unique(a, return_counts=True)\n",
    "        if counts[0] <4:\n",
    "            aux=aux+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753 789 0.9543726235741445\n"
     ]
    }
   ],
   "source": [
    "eficiencia=aux/Ypred.shape[0]\n",
    "print(aux,Ypred.shape[0],eficiencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02379526399155535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(Ytest, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 33)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  86.05830164765526\n",
      "Accuracy :  98.2256020278834\n",
      "2\n",
      "3\n",
      "Accuracy :  91.25475285171103\n",
      "5\n",
      "6\n",
      "Accuracy :  79.72116603295311\n",
      "8\n",
      "Accuracy :  93.78960709759188\n",
      "Accuracy :  89.60709759188846\n",
      "Accuracy :  93.02915082382764\n",
      "Accuracy :  91.50823827629911\n",
      "Accuracy :  95.18377693282636\n",
      "Accuracy :  85.93155893536121\n",
      "Accuracy :  95.69074778200253\n",
      "Accuracy :  99.36628643852978\n",
      "Accuracy :  94.55006337135615\n",
      "Accuracy :  99.23954372623574\n",
      "Accuracy :  99.36628643852978\n",
      "Accuracy :  99.11280101394169\n",
      "Accuracy :  96.1977186311787\n",
      "Accuracy :  99.23954372623574\n",
      "Accuracy :  94.93029150823827\n",
      "Accuracy :  99.23954372623574\n",
      "Accuracy :  98.35234474017744\n",
      "Accuracy :  98.98605830164766\n",
      "Accuracy :  93.5361216730038\n",
      "Accuracy :  98.73257287705957\n",
      "Accuracy :  90.8745247148289\n",
      "Accuracy :  98.47908745247149\n",
      "Accuracy :  98.47908745247149\n",
      "Accuracy :  98.73257287705957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programas\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn = np.zeros(33)\n",
    "fp = np.zeros(33)\n",
    "fn = np.zeros(33)\n",
    "tp = np.zeros(33)\n",
    "\n",
    "Accuracy = np.zeros(33)\n",
    "Precision = np.zeros(33)\n",
    "Recall = np.zeros(33)\n",
    "f1 = np.zeros(33)\n",
    "for i in range(33):\n",
    "    try:\n",
    "        tn[i], fp[i], fn[i], tp[i]=confusion_matrix(Ypred[:,i].astype(int),Ytest[:,i].astype(int)).ravel()\n",
    "        Accuracy[i] = (tn[i]+tp[i])*100/(tp[i]+tn[i]+fp[i]+fn[i])\n",
    "        Precision[i] = tp[i]/(tp[i]+fp[i])\n",
    "        Recall[i] = tp[i]/(tp[i]+fn[i])\n",
    "        f1[i] = (2*Precision[i]*Recall[i])/(Precision[i] + Recall[i])\n",
    "        print(\"Accuracy : \",Accuracy[i])\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25342466, 0.06666667, 0.        , 0.        , 0.21176471,\n",
       "       0.        , 0.        , 0.23188406, 0.        , 0.05882353,\n",
       "       0.16666667, 0.01785714, 0.04285714, 0.07317073, 0.04310345,\n",
       "       0.15      , 0.16666667, 0.06818182, 0.        , 0.28571429,\n",
       "       0.22222222, 0.11764706, 0.        , 0.14893617, 0.        ,\n",
       "       0.        , 0.        , 0.30769231, 0.        , 0.2183908 ,\n",
       "       0.        , 0.2       , 0.        ])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy2 = (tn.sum()+tp.sum())*100/(tp.sum()+tn.sum()+fp.sum()+fn.sum())\n",
    "Precision2 = tp.sum()/(tp.sum()+fp.sum())\n",
    "Recall2 = tp.sum()/(tp.sum()+fn.sum())\n",
    "Espesi= tn.sum()/(tn.sum()+fp.sum())\n",
    "sensi = tp.sum()/(tp.sum()+fn.sum())\n",
    "f1_2 = ((1+(8*8))*Precision2*Recall2)/((8*8)*Precision2 + Recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94.9076588810429,\n",
       " 15.43778801843318,\n",
       " 89.33333333333333,\n",
       " 94.96501577719852,\n",
       " 89.33333333333333,\n",
       " 83.20596102407336]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Accuracy2,Precision2*100,Recall2*100,Espesi*100,sensi*100,f1_2*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Ypred[:,0].astype(int))\n",
    "print(Ytest[:,0].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MLANOMALIAS_OP.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbrt_result = [0.004822841807009301, 3, 379, 257, 'sigmoid', 76, 0.009899190596633045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbrt_result =[0.0021363296723156777, 1, 243, 96, 'sigmoid', 35, 0.003991454981799475]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
